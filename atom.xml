<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浅笑の博客</title>
  
  <subtitle>我们的征途是星辰大海</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zhengyujie.cn/"/>
  <updated>2019-08-23T02:31:50.068Z</updated>
  <id>http://zhengyujie.cn/</id>
  
  <author>
    <name>Zheng Yujie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorFlow学习笔记12：CIFAR-10数据集图片分类</title>
    <link href="http://zhengyujie.cn/2019/08/23/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/"/>
    <id>http://zhengyujie.cn/2019/08/23/tf学习笔记12/</id>
    <published>2019-08-23T02:02:38.000Z</published>
    <updated>2019-08-23T02:31:50.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CIFAR-10数据集简介"><a href="#CIFAR-10数据集简介" class="headerlink" title="CIFAR-10数据集简介"></a>CIFAR-10数据集简介</h1><blockquote><p>官网链接：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p></blockquote><p> CIFAR-10是一个更接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10个类别的RGB彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有50000 张训练图片和10000 张测试图片。</p><a id="more"></a><p><img src="http://pwbhioup3.bkt.clouddn.com/CIFAR-10.png" alt="CIFAR-10"></p><h1 id="TensenFlow实现"><a href="#TensenFlow实现" class="headerlink" title="TensenFlow实现"></a>TensenFlow实现</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>首先去官方库下载<code>cifar10.py</code>以及<code>cifar10_input.py</code>文件来下载CIFAR-10数据集二进制文件，以及读取文件内容。</p><blockquote><p>GitHub地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p></blockquote><p>运行下载数据集函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.models.tutorials.image.cifar10 <span class="keyword">import</span> cifar10</span><br><span class="line">cifar10.maybe_download_and_extract()</span><br></pre></td></tr></table></figure></p><p>数据集文件默认下载在<code>./tmp/cifar10_data</code>文件下，可以将其移动到自己的工程文件夹下</p><h2 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h2><p>首先定义一个权重初始化函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_with_weight_loss</span><span class="params">(shape,std,w1)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.truncated_normal(shape,stddev=std),dtype=tf.float32)</span><br><span class="line">    <span class="keyword">if</span> w1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name=<span class="string">"weight_loss"</span>)</span><br><span class="line">        tf.add_to_collection(<span class="string">"losses"</span>,weight_loss)</span><br><span class="line">    <span class="keyword">return</span> var</span><br></pre></td></tr></table></figure></p><p>然后定义一个损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(logits,labels)</span>:</span></span><br><span class="line">    labels = tf.cast(labels,tf.int32)</span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</span><br><span class="line">                           labels=labels,name=<span class="string">"cross_entropy_per_example"</span>)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(tf.reduce_sum(cross_entropy))</span><br><span class="line">    tf.add_to_collection(<span class="string">"losses"</span>,cross_entropy_mean)</span><br><span class="line">    <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">"losses"</span>),name=<span class="string">"total_loss"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置每次训练的数据大小</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment">#下载解压数据</span></span><br><span class="line"><span class="comment"># cifar10.maybe_download_and_extract()</span></span><br><span class="line"><span class="comment"># 设置数据的存放目录</span></span><br><span class="line">cifar10_dir = <span class="string">"cifar10_data/cifar-10-batches-bin"</span></span><br><span class="line"><span class="comment">#获取数据增强后的训练集数据</span></span><br><span class="line">images_train,labels_train = cifar10_input.distorted_inputs(cifar10_dir,batch_size)</span><br><span class="line"><span class="comment">#获取裁剪后的测试数据</span></span><br><span class="line">images_test,labels_test = cifar10_input.inputs(eval_data=<span class="literal">True</span>,data_dir=cifar10_dir,                     </span><br><span class="line">                                               batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#定义模型的输入和输出数据</span></span><br><span class="line">image_holder = tf.placeholder(dtype=tf.float32,shape=[batch_size,<span class="number">24</span>,<span class="number">24</span>,<span class="number">3</span>])</span><br><span class="line">label_holder = tf.placeholder(dtype=tf.int32,shape=[batch_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#设计第一层卷积</span></span><br><span class="line">weight1 = variable_with_weight_loss(shape=[<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">64</span>],std=<span class="number">5e-2</span>,w1=<span class="number">0</span>)</span><br><span class="line">kernel1 = tf.nn.conv2d(image_holder,weight1,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">bais1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">64</span>]))</span><br><span class="line">conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bais1))</span><br><span class="line">pool1 = tf.nn.max_pool(conv1,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">norm1 = tf.nn.lrn(pool1,<span class="number">4</span>,bias=<span class="number">1.0</span>,alpha=<span class="number">0.001</span> / <span class="number">9</span>,beta=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设计第二层卷积</span></span><br><span class="line">weight2 = variable_with_weight_loss(shape=[<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">64</span>],std=<span class="number">5e-2</span>,w1=<span class="number">0</span>)</span><br><span class="line">kernel2 = tf.nn.conv2d(norm1,weight2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">bais2 = tf.Variable(tf.constant(<span class="number">0.1</span>,dtype=tf.float32,shape=[<span class="number">64</span>]))</span><br><span class="line">conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bais2))</span><br><span class="line">norm2 = tf.nn.lrn(conv2,<span class="number">4</span>,bias=<span class="number">1.0</span>,alpha=<span class="number">0.01</span> / <span class="number">9</span>,beta=<span class="number">0.75</span>)</span><br><span class="line">pool2 = tf.nn.max_pool(norm2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一层全连接层</span></span><br><span class="line">reshape = tf.reshape(pool2,[batch_size,<span class="number">-1</span>])</span><br><span class="line">dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line">weight3 = variable_with_weight_loss([dim,<span class="number">384</span>],std=<span class="number">0.04</span>,w1=<span class="number">0.004</span>)</span><br><span class="line">bais3 = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[<span class="number">384</span>],dtype=tf.float32))</span><br><span class="line">local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bais3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二层全连接层</span></span><br><span class="line">weight4 = variable_with_weight_loss([<span class="number">384</span>,<span class="number">192</span>],std=<span class="number">0.04</span>,w1=<span class="number">0.004</span>)</span><br><span class="line">bais4 = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[<span class="number">192</span>],dtype=tf.float32))</span><br><span class="line">local4 = tf.nn.relu(tf.matmul(local3,weight4)+bais4)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后一层</span></span><br><span class="line">weight5 = variable_with_weight_loss([<span class="number">192</span>,<span class="number">10</span>],std=<span class="number">1</span>/<span class="number">192.0</span>,w1=<span class="number">0</span>)</span><br><span class="line">bais5 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>],dtype=tf.float32))</span><br><span class="line">logits = tf.add(tf.matmul(local4,weight5),bais5)</span><br></pre></td></tr></table></figure><h2 id="训练和优化"><a href="#训练和优化" class="headerlink" title="训练和优化"></a>训练和优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置最大迭代次数</span></span><br><span class="line">max_steps = <span class="number">10000</span></span><br><span class="line"><span class="comment">#获取损失函数</span></span><br><span class="line">loss = loss_func(logits,label_holder)</span><br><span class="line"><span class="comment">#设置优化算法使得成本最小</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</span><br><span class="line"><span class="comment">#获取最高类的分类准确率，取top1作为衡量标准</span></span><br><span class="line">top_k_op = tf.nn.in_top_k(logits,label_holder,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(max_steps):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    images_batch,labels_batch = sess.run([images_train,labels_train])</span><br><span class="line">    _,loss_value = sess.run([train_step,loss],feed_dict=&#123;image_holder:images_batch,</span><br><span class="line">                                                            label_holder:labels_batch&#125;)</span><br><span class="line">    <span class="comment">#获取计算时间</span></span><br><span class="line">    duration = time.time() - start_time</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#计算每秒处理多少张图片</span></span><br><span class="line">        per_images_second = batch_size / duration</span><br><span class="line">        <span class="comment">#获取时间</span></span><br><span class="line">        sec_per_batch = float(duration)</span><br><span class="line">        print(<span class="string">"step:%d,duration:%.3f,per_images_second:%.2f,loss:%.3f"</span>%(step,duration</span><br><span class="line">              ,per_images_second,loss_value))</span><br></pre></td></tr></table></figure><h2 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算测试集上的准确率</span></span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">num_iter = int(math.ceil(num_examples / batch_size))</span><br><span class="line">true_count = <span class="number">0</span></span><br><span class="line">total_sample_count = num_iter * batch_size</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> step &lt; num_iter:</span><br><span class="line">    images_batch,labels_batch = sess.run([images_test,labels_test])</span><br><span class="line">    pred = sess.run([top_k_op],feed_dict=&#123;image_holder:images_batch,label_holder:labels_batch&#125;)</span><br><span class="line">    true_count += np.sum(pred)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"><span class="comment">#计算测试集的准确率</span></span><br><span class="line">precision = true_count / total_sample_count</span><br><span class="line">print(<span class="string">"test accuracy:%.3f"</span>%precision)</span><br></pre></td></tr></table></figure><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C1.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CIFAR-10数据集简介&quot;&gt;&lt;a href=&quot;#CIFAR-10数据集简介&quot; class=&quot;headerlink&quot; title=&quot;CIFAR-10数据集简介&quot;&gt;&lt;/a&gt;CIFAR-10数据集简介&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;官网链接：&lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; CIFAR-10是一个更接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10个类别的RGB彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有50000 张训练图片和10000 张测试图片。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 16.04安装Pycharm</title>
    <link href="http://zhengyujie.cn/2019/08/22/ubuntu%E5%AE%89%E8%A3%85pycharm/"/>
    <id>http://zhengyujie.cn/2019/08/22/ubuntu安装pycharm/</id>
    <published>2019-08-22T07:28:22.000Z</published>
    <updated>2019-08-22T07:42:24.666Z</updated>
    
    <content type="html"><![CDATA[<p>记录Ubuntu 16.04下Pycharm的安装步骤</p><a id="more"></a><p>首先去官网下载最新的Pycharm安装包，官网地址：<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">http://www.jetbrains.com/pycharm/</a><br>这里我下载了社区版(Community)</p><p>下载完成后，右键压缩包，选择提取到此处</p><p>然后<code>cd</code>进入Pychram安装包下的<code>bin</code>文件夹，我们可以看到有一个名叫<code>pychram.sh</code>的文件。<br>在终端输入命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./pycharm.sh</span><br></pre></td></tr></table></figure></p><p>由于我之前没有安装过Pycharm，所以直接选择<code>Do not import settings</code>即可，然后点击<code>OK</code>进入下一步操作。根据提示的内容进行勾选同意协议，然后点击continue进入下一步，大家可以点击don’t send进入下一步，选择喜欢的界面风格，然后再次点击next进入下一步，直到提示start启动程序。即可完成Pycharm的安装。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录Ubuntu 16.04下Pycharm的安装步骤&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/categories/Ubuntu/"/>
    
    
      <category term="Pycharm" scheme="http://zhengyujie.cn/tags/Pycharm/"/>
    
      <category term="软件安装" scheme="http://zhengyujie.cn/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Hexo折腾日记之侧栏篇</title>
    <link href="http://zhengyujie.cn/2019/08/22/hexo%E8%AE%BE%E7%BD%AE%E4%BE%A7%E6%A0%8F/"/>
    <id>http://zhengyujie.cn/2019/08/22/hexo设置侧栏/</id>
    <published>2019-08-22T06:11:16.000Z</published>
    <updated>2019-08-22T07:09:10.976Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>现在我相信兴趣是最好的学习动力了。作为一个对前段完全提不起兴趣的人，在见识到别人的博客有多好看之后，我居然对美化博客产生了强烈的兴趣。以至于在折腾的过程中浑然不知时间的流逝，有时候明明已经饥肠辘辘了却依然乐之不疲，连我自己都在怀疑自己是不是着了魔。在高强度折腾了将近一天后，我终于将我的博客侧栏打造成了我满意的样子。对于我这种毫无任何HTML，CSS等基础的人，在折腾过程中遇到了难以计数的bug，所以能有这样的成果已经是相当满足了。这篇博客记录我对侧栏进行魔改的整个过程。</p><a id="more"></a><h1 id="折腾过程"><a href="#折腾过程" class="headerlink" title="折腾过程"></a>折腾过程</h1><h2 id="改造思路"><a href="#改造思路" class="headerlink" title="改造思路"></a>改造思路</h2><p>我希望主页的侧栏以及浏览文章时的侧栏显示的内容是不同的。</p><ul><li>在主页时，博客侧栏要尽可能多地显示和我博客有关的各种信息，比如博主的信息，文章分类信息，文章标签信息等等。</li><li>在浏览文章时，侧栏的信息要加以精简，仅仅保留博主信息以及文章目录，同时在滚动页面时侧栏要固定在头部。</li></ul><p>由于NexT主题默认在任何页面侧栏在滚动到页面顶部时都固定不动。但我想在主页面时侧栏能够显示更多的信息，于是我的想法就是写一个<code>if</code>语句判断当前页面是否为浏览文章页面，若不是，则侧栏随页面滚动，并且尽可能多的显示信息。若是，则仅仅显示博主信息以及文章目录。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>首先打开<code>/next/layout/_macro</code>文件夹下的<code>sidebar.swig</code>文件，这就是我们博客侧栏的源代码了，我们开始进行魔改</p><p>首先开头第一段<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line-wrap"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-first"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-middle"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-last"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>我们保持不动，然后接下来<code>aside</code>标签下的内容全部删除，是的你没听错，全部删除，我们重新实现。<br>首先先写个<code>if</code>语句来判断当前的页面<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;%- <span class="keyword">set</span> display_toc = theme.toc.enable and display_toc %&#125;</span><br><span class="line">&#123;%- <span class="keyword">if</span> not display_toc or toc(page.content).length &lt;= <span class="number">1</span> %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure></p><p>接着我们在if语句内部协商我们想在主页面侧栏现实的内容</p><ol><li><p>博主信息。包括头像，自我介绍以及附加博客的文章数，类别数以及标签数。这段代码在<code>next/layout/_partials/sidebar/</code>的<code>site-overview.swig</code>文件内有现成的，我们只要保留我们想显示的部分就好了</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"author-overview"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author motion-element"</span> itemprop=<span class="string">"author"</span> itemscope itemtype=<span class="string">"http://schema.org/Person"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> theme.avatar.url %&#125;</span><br><span class="line">        &lt;img <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-image"</span> itemprop=<span class="string">"image"</span></span><br><span class="line">            src=<span class="string">"&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span></span><br><span class="line">            alt=<span class="string">"&#123;&#123; author &#125;&#125;"</span>&gt;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">        &lt;p <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-name"</span> itemprop=<span class="string">"name"</span>&gt;&#123;&#123; author &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="site-description motion-element" itemprop="description"&gt;&#123;&#123; description &#125;&#125;&lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &#123;%- if theme.site_state %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;nav class="site-state motion-element"&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- if config.archive_dir != '/</span><span class="string">' and site.posts.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-posts"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.menu.archives %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.archives).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">            &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.archive_dir) &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.posts<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if site.categories.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set categoriesPageQuery = site.pages.find(&#123;type: '</span>categories<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasCategoriesPage = categoriesPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-categories"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasCategoriesPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.categories %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.categories).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.category_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set visibleCategories = 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- for cat in site.categories %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if cat.length %&#125;&#123;%- set visibleCategories += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleCategories &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.categories<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasCategoriesPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if site.tags.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set tagsPageQuery = site.pages.find(&#123;type: '</span>tags<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasTagsPage = tagsPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-tags"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasTagsPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.tags %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.tags).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.tag_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set visibleTags = 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- for tag in site.tags %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if tag.length %&#125;&#123;%- set visibleTags += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleTags &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.tags<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasTagsPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &lt;/nav&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.social %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="links-of-author motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- for name, link in theme.social %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="links-of-author-item"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarURL = link.split('</span>||<span class="string">')[0] | trim %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if not (theme.social_icons.enable) or (not theme.social_icons.icons_only) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarText = name %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.social_icons.enable %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarIcon = '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-fw fa-' + link.split('||')[1] | trim | default('globe') + '"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;&#123; next_url(sidebarURL, sidebarIcon + sidebarText, &#123;title: name + '</span> &amp;rarr; <span class="string">' + sidebarURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="string">            &lt;/span&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.rss %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="feed-link motion-element"&gt;</span></span><br><span class="line"><span class="string">        &lt;a href="&#123;&#123; url_for(theme.rss) &#125;&#125;" rel="alternate"&gt;</span></span><br><span class="line"><span class="string">            &lt;i class="fa fa-rss"&gt;&lt;/i&gt;RSS</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.chat.enable and theme.chat.service !== '</span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="chat motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>chatra<span class="string">' and theme.chatra.enable %&#125;</span></span><br><span class="line"><span class="string">        &lt;a onclick="Chatra('</span>openChat<span class="string">', true)"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>tidio<span class="string">' and theme.tidio.enable %&#125;</span></span><br><span class="line"><span class="string">        &lt;a onclick="tidioChatApi.open();"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.icon %&#125;&lt;i class="fa fa-&#123;&#123; theme.chat.icon &#125;&#125;"&gt;&lt;/i&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;&#123; theme.chat.text &#125;&#125;</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="cc-license motion-element" itemprop="license"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- set ccLanguage = theme.creative_commons.language %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license === '</span>zero<span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccType = '</span>publicdomain/zero/<span class="number">1.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;% else %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccType = '</span>licenses/<span class="string">' + theme.creative_commons.license + '</span>/<span class="number">4.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccURL = '</span>https:<span class="comment">//creativecommons.org/' + ccType %&#125;</span></span><br><span class="line">        &#123;%- <span class="keyword">set</span> ccImage = '&lt;img src="' + url_for(theme.images + '/cc-' + theme.creative_commons.license + '.svg') + '" alt="Creative Commons"&gt;' %&#125;</span><br><span class="line">        &#123;&#123; next_url(ccURL, ccImage, &#123;<span class="attr">class</span>: <span class="string">'cc-opacity'</span>&#125;) &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>博客公告。内容就自由发挥了。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;card-announcement&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;card-content&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;item_headline&quot;&gt;</span><br><span class="line">        &lt;i class=&quot;fa fa-bullhorn card-announcement-animation&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;</span><br><span class="line">        &lt;span&gt;公告&lt;/span&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;announcement_content&quot;&gt;</span><br><span class="line">        谢谢你这么帅，这么漂亮还来看我的博客，如果喜欢的话记得收藏哦</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure></li><li><p>博客信息。我主要就显示文章数量，博客运行天数，访问人数以及点击量四条信息。</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-overview"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;<span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-line-chart"</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span>网站资讯<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info"</span>&gt;</span><br><span class="line">        &lt;script&#123;&#123; pjax &#125;&#125; <span class="keyword">async</span> src=<span class="string">"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-name"</span>&gt;文章数目&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="blogs-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        &lt;li class="blog-info-list"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="blog-info-name"&gt;运行天数&lt;/</span>span&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"days-count"</span> id=<span class="string">"sitedays"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">        &lt;script language=javascript&gt;</span><br><span class="line">            <span class="function"><span class="keyword">function</span> <span class="title">siteTime</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">                <span class="built_in">window</span>.setTimeout(<span class="string">"siteTime()"</span>, <span class="number">1000</span>);</span><br><span class="line">                <span class="keyword">var</span> seconds = <span class="number">1000</span>;</span><br><span class="line">                <span class="keyword">var</span> minutes = seconds * <span class="number">60</span>;</span><br><span class="line">                <span class="keyword">var</span> hours = minutes * <span class="number">60</span>;</span><br><span class="line">                <span class="keyword">var</span> days = hours * <span class="number">24</span>;</span><br><span class="line">                <span class="keyword">var</span> years = days * <span class="number">365</span>;</span><br><span class="line">                <span class="keyword">var</span> today = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line">                <span class="keyword">var</span> todayYear = today.getFullYear();</span><br><span class="line">                <span class="keyword">var</span> todayMonth = today.getMonth()+<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">var</span> todayDate = today.getDate();</span><br><span class="line">                <span class="keyword">var</span> todayHour = today.getHours();</span><br><span class="line">                <span class="keyword">var</span> todayMinute = today.getMinutes();</span><br><span class="line">                <span class="keyword">var</span> todaySecond = today.getSeconds();</span><br><span class="line">                <span class="keyword">var</span> t1 = <span class="built_in">Date</span>.UTC(<span class="number">2019</span>,<span class="number">07</span>,<span class="number">26</span>,<span class="number">00</span>,<span class="number">00</span>,<span class="number">00</span>); <span class="comment">//北京时间2018-2-13 00:00:00</span></span><br><span class="line">                <span class="keyword">var</span> t2 = <span class="built_in">Date</span>.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);</span><br><span class="line">                <span class="keyword">var</span> diff = t2-t1;</span><br><span class="line">                <span class="keyword">var</span> diffYears = <span class="built_in">Math</span>.floor(diff/years);</span><br><span class="line">                <span class="keyword">var</span> diffDays = <span class="built_in">Math</span>.floor((diff/days)-diffYears*<span class="number">365</span>);</span><br><span class="line">                <span class="built_in">document</span>.getElementById(<span class="string">"sitedays"</span>).innerHTML=diffDays+<span class="string">" 天 "</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            siteTime();</span><br><span class="line">        &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;访问数&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="site-uv" title="&#123;&#123; __('footer.total_visitors') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">            &lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-name"</span>&gt;点击量&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="site-pv" title="&#123;&#123; __('footer.total_views') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">            &lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>分类信息。</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-categoreus"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-folder-open"</span> aria-hidden=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">    &lt;span&gt;分类&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &lt;div&gt;</span><br><span class="line">    &#123;&#123; list_categories() &#125;&#125; </span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>标签云。GitHub地址：<a href="https://github.com/MikeCoder/hexo-tag-cloud" target="_blank" rel="noopener">https://github.com/MikeCoder/hexo-tag-cloud</a></p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-tags"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-tags"</span> aria-hidden=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">    &lt;span&gt;标签云&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &#123;% <span class="keyword">if</span> site.tags.length &gt; <span class="number">1</span> %&#125;</span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span> charset=<span class="string">"utf-8"</span> src=<span class="string">"&#123;&#123; url_for('/js/tagcloud.js') &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span> charset=<span class="string">"utf-8"</span> src=<span class="string">"&#123;&#123; url_for('/js/tagcanvas.js') &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"widget-wrap"</span>&gt;</span><br><span class="line">        &lt;div id=<span class="string">"myCanvasContainer"</span> <span class="class"><span class="keyword">class</span></span>=<span class="string">"widget tagcloud"</span>&gt;</span><br><span class="line">        &lt;canvas width=<span class="string">"250"</span> height=<span class="string">"250"</span> id=<span class="string">"resCanvas"</span> style=<span class="string">"width=100%"</span>&gt;</span><br><span class="line">            &#123;&#123; list_tags() &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/canvas&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>友情链接</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"link-of-blogroll"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-handshake-o"</span> aria-hidden=<span class="string">"true"</span>&gt;</span><br><span class="line">    &lt;<span class="regexp">/i&gt;&lt;span&gt;友情链接&lt;/</span>span&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- if theme.links %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;div class="links-of-blogroll motion-element &#123;&#123; "links-of-blogroll-" + theme.links_layout | default('inline') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="links-of-blogroll-title"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;i class="fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;"&gt;&lt;/i</span>&gt;</span><br><span class="line">        &#123;&#123; theme.links_title &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &lt;ul class="links-of-blogroll-list"&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- for blogrollText, blogrollURL in theme.links %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;li class="links-of-blogroll-item"&gt;</span></span><br><span class="line"><span class="regexp">            &#123;&#123; next_url(blogrollURL, blogrollText, &#123;title: blogrollURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="regexp">            &lt;/</span>li&gt;</span><br><span class="line">        &#123;%- endfor %&#125;</span><br><span class="line">        &lt;<span class="regexp">/ul&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>主页的侧栏就设置完成了。</p><p>接下来就是浏览文章时的侧栏的设计，这里就直接上我的代码了<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-inner"</span>&gt; </span><br><span class="line">    &#123;%- <span class="keyword">if</span> display_toc and toc(page.content).length &gt; <span class="number">1</span> %&#125;</span><br><span class="line">    &lt;!--noindex--&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"author-overview"</span>&gt;</span><br><span class="line">        &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author motion-element"</span> itemprop=<span class="string">"author"</span> itemscope itemtype=<span class="string">"http://schema.org/Person"</span>&gt;</span><br><span class="line">            &#123;%- <span class="keyword">if</span> theme.avatar.url %&#125;</span><br><span class="line">            &lt;img <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-image"</span> itemprop=<span class="string">"image"</span></span><br><span class="line">                src=<span class="string">"&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span></span><br><span class="line">                alt=<span class="string">"&#123;&#123; author &#125;&#125;"</span>&gt;</span><br><span class="line">            &#123;%- endif %&#125;</span><br><span class="line">            &lt;p <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-name"</span> itemprop=<span class="string">"name"</span>&gt;&#123;&#123; author &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">            &lt;div class="site-description motion-element" itemprop="description"&gt;&#123;&#123; description &#125;&#125;&lt;/</span>div&gt;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- if theme.site_state %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;nav class="site-state motion-element"&gt;</span></span><br><span class="line"><span class="regexp">            &#123;%- if config.archive_dir != '/</span><span class="string">' and site.posts.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-posts"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.archives %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.archives).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.archive_dir) &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.posts<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            &#123;%- if site.categories.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set categoriesPageQuery = site.pages.find(&#123;type: '</span>categories<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasCategoriesPage = categoriesPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-categories"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasCategoriesPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.categories %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(theme.menu.categories).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(config.category_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set visibleCategories = 0 %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- for cat in site.categories %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if cat.length %&#125;&#123;%- set visibleCategories += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleCategories &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.categories<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasCategoriesPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            &#123;%- if site.tags.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set tagsPageQuery = site.pages.find(&#123;type: '</span>tags<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasTagsPage = tagsPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-tags"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasTagsPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.tags %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(theme.menu.tags).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(config.tag_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set visibleTags = 0 %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- for tag in site.tags %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if tag.length %&#125;&#123;%- set visibleTags += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleTags &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.tags<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasTagsPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &lt;/nav&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.social %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="links-of-author motion-element"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- for name, link in theme.social %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="links-of-author-item"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarURL = link.split('</span>||<span class="string">')[0] | trim %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if not (theme.social_icons.enable) or (not theme.social_icons.icons_only) %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set sidebarText = name %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.social_icons.enable %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set sidebarIcon = '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-fw fa-' + link.split('||')[1] | trim | default('globe') + '"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;&#123; next_url(sidebarURL, sidebarIcon + sidebarText, &#123;title: name + '</span> &amp;rarr; <span class="string">' + sidebarURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="string">            &lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.rss %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="feed-link motion-element"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="&#123;&#123; url_for(theme.rss) &#125;&#125;" rel="alternate"&gt;</span></span><br><span class="line"><span class="string">            &lt;i class="fa fa-rss"&gt;&lt;/i&gt;RSS</span></span><br><span class="line"><span class="string">            &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.enable and theme.chat.service !== '</span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="chat motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>chatra<span class="string">' and theme.chatra.enable %&#125;</span></span><br><span class="line"><span class="string">            &lt;a onclick="Chatra('</span>openChat<span class="string">', true)"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>tidio<span class="string">' and theme.tidio.enable %&#125;</span></span><br><span class="line"><span class="string">            &lt;a onclick="tidioChatApi.open();"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.icon %&#125;&lt;i class="fa fa-&#123;&#123; theme.chat.icon &#125;&#125;"&gt;&lt;/i&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;&#123; theme.chat.text &#125;&#125;</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="cc-license motion-element" itemprop="license"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- set ccLanguage = theme.creative_commons.language %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license === '</span>zero<span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set ccType = '</span>publicdomain/zero/<span class="number">1.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;% else %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set ccType = '</span>licenses/<span class="string">' + theme.creative_commons.license + '</span>/<span class="number">4.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccURL = '</span>https:<span class="comment">//creativecommons.org/' + ccType %&#125;</span></span><br><span class="line">        &#123;%- <span class="keyword">set</span> ccImage = '&lt;img src="' + url_for(theme.images + '/cc-' + theme.creative_commons.license + '.svg') + '" alt="Creative Commons"&gt;' %&#125;</span><br><span class="line">            &#123;&#123; next_url(ccURL, ccImage, &#123;<span class="attr">class</span>: <span class="string">'cc-opacity'</span>&#125;) &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-toc-wrap motion-element sidebar-panel sidebar-panel-active"</span> id=<span class="string">"post-toc-wrap"</span>&gt;</span><br><span class="line">        &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">        &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-list"</span> aria-hidden=<span class="string">"true"</span>&gt;</span><br><span class="line">        &lt;<span class="regexp">/i&gt;&lt;span&gt;目录&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="post-toc"&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;%- set next_toc_number = theme.toc.number %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- if page.toc_number !== undefined %&#125;</span></span><br><span class="line"><span class="regexp">            &#123;%- set next_toc_number = page.toc_number %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- set next_toc_max_depth = page.toc_max_depth|default(theme.toc.max_depth)|default(6) %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- set toc = toc(page.content, &#123; "class": "nav", list_number: next_toc_number, max_depth: next_toc_max_depth &#125;) %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;%- if toc.length &lt;= 1 %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;p class="post-toc-empty"&gt;&#123;&#123; __('post.toc_empty') &#125;&#125;&lt;/</span>p&gt;</span><br><span class="line">        &#123;% <span class="keyword">else</span> %&#125;</span><br><span class="line">            &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-toc-content"</span>&gt;&#123;&#123; toc &#125;&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--/</span>noindex--&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></p><p>之后就可以自由发挥添加CSS样式了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;现在我相信兴趣是最好的学习动力了。作为一个对前段完全提不起兴趣的人，在见识到别人的博客有多好看之后，我居然对美化博客产生了强烈的兴趣。以至于在折腾的过程中浑然不知时间的流逝，有时候明明已经饥肠辘辘了却依然乐之不疲，连我自己都在怀疑自己是不是着了魔。在高强度折腾了将近一天后，我终于将我的博客侧栏打造成了我满意的样子。对于我这种毫无任何HTML，CSS等基础的人，在折腾过程中遇到了难以计数的bug，所以能有这样的成果已经是相当满足了。这篇博客记录我对侧栏进行魔改的整个过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo框架" scheme="http://zhengyujie.cn/categories/Hexo%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Hexo" scheme="http://zhengyujie.cn/tags/Hexo/"/>
    
      <category term="NexT" scheme="http://zhengyujie.cn/tags/NexT/"/>
    
      <category term="JavaScript" scheme="http://zhengyujie.cn/tags/JavaScript/"/>
    
      <category term="Django" scheme="http://zhengyujie.cn/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>Hexo折腾日记之标签篇</title>
    <link href="http://zhengyujie.cn/2019/08/20/hexo%E5%BD%A9%E8%89%B2%E6%A0%87%E7%AD%BE/"/>
    <id>http://zhengyujie.cn/2019/08/20/hexo彩色标签/</id>
    <published>2019-08-20T05:24:27.000Z</published>
    <updated>2019-08-22T06:12:28.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天又耗费一个上午给我的博客折腾出了彩色标签，接下来记录我的设置过程</p><a id="more"></a><h1 id="page-swig文件设置"><a href="#page-swig文件设置" class="headerlink" title="page.swig文件设置"></a>page.swig文件设置</h1><p>首先先打开<code>/next/layout</code>文件夹下的page.swig文件，在文件中找到以下内容<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud-title"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">set</span> visibleTags = 0 %&#125;</span><br><span class="line">        &#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> site.tags %&#125;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> tag.length %&#125;</span><br><span class="line">            &#123;%- <span class="keyword">set</span> visibleTags += 1 %&#125;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">        &#123;%- endfor %&#125;</span><br><span class="line">        &#123;&#123; _p(<span class="string">'counter.tag_cloud'</span>, visibleTags) &#125;&#125;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></p><p>接下来的就是我们标签页面的主要代码<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud-tags"</span> id=<span class="string">"tags"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> not theme.tagcloud %&#125;</span><br><span class="line">        &#123;&#123; tagcloud(&#123;<span class="attr">min_font</span>: <span class="number">12</span>, <span class="attr">max_font</span>: <span class="number">30</span>, <span class="attr">amount</span>: <span class="number">200</span>, <span class="attr">color</span>: <span class="literal">true</span>, <span class="attr">start_color</span>: <span class="string">'#fff'</span>, <span class="attr">end_color</span>: <span class="string">'#fff'</span>&#125;) &#125;&#125;</span><br><span class="line">        &#123;% <span class="keyword">else</span> %&#125;</span><br><span class="line">        &#123;&#123; tagcloud(&#123;<span class="attr">min_font</span>: theme.tagcloud.min, <span class="attr">max_font</span>: theme.tagcloud.max, <span class="attr">amount</span>: theme.tagcloud.amount, <span class="attr">color</span>: <span class="literal">true</span>, <span class="attr">start_color</span>: theme.tagcloud.start, <span class="attr">end_color</span>: theme.tagcloud.end&#125;) &#125;&#125;</span><br><span class="line">        &#123;%- endif %&#125;             </span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></p><p>这里我们已经将起始颜色和终止颜色都换成了<code>#fff</code>即纯白色，接下来我们就开始随机设置每个标签的背景颜色。<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">    <span class="keyword">var</span> alltags=<span class="built_in">document</span>.getElementById(<span class="string">'tags'</span>);</span><br><span class="line">    <span class="keyword">var</span> tags=alltags.getElementsByTagName(<span class="string">'a'</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = tags.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">var</span> r=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">130</span>);</span><br><span class="line">        <span class="keyword">var</span> g=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">var</span> b=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">80</span>);</span><br><span class="line">        tags[i].style.background = <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure></p><p>然后我们再调整标签的CSS样式即可，这里附上我的CSS样式<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.tag-cloud</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">0px</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0px</span> <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">10px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="post-swig文件设置"><a href="#post-swig文件设置" class="headerlink" title="post.swig文件设置"></a>post.swig文件设置</h1><p>接下来我们设置主页的标签，首先NexT主题首页的文章预览里没有标签，所以我们要先在文章预览里添加标签。打开<code>/home/zyj/blog/themes/next/layout/_macro</code>下的post.swig文件，找到<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elif post.excerpt %&#125;</span><br><span class="line">    &#123;&#123; post.excerpt &#125;&#125;</span><br></pre></td></tr></table></figure></p><p>在其后面添加<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;%- <span class="keyword">if</span> post.tags and post.tags.length %&#125;</span><br><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"home-post-tags"</span>&gt;</span><br><span class="line">    &#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> post.tags %&#125;</span><br><span class="line">    &lt;a href=<span class="string">"&#123;&#123; url_for(tag.path) &#125;&#125;"</span> rel=<span class="string">"tag"</span>&gt;&#123;&#123; tag.name &#125;&#125;&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;script type="text/</span>javascript<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    var tagsall=document.getElementsByClassName("</span>home-post-tags<span class="string">")</span></span><br><span class="line"><span class="string">    for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span></span><br><span class="line"><span class="string">        var tags=tagsall[i].getElementsByTagName("</span>a<span class="string">");</span></span><br><span class="line"><span class="string">        for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span></span><br><span class="line"><span class="string">            var r=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            var g=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            var b=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            tags[j].style.background = "</span>rgb(<span class="string">"+r+"</span>,<span class="string">"+g+"</span>,<span class="string">"+b+"</span>)<span class="string">";</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#123;%- endif %&#125;</span></span><br></pre></td></tr></table></figure></p><p>即可在文章预览中添加彩色标签</p><p>接下来将文章主页面末尾的标签换成彩色标签，同样在post.swig中找到：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-tags"</span> id=<span class="string">"post-tags"</span>&gt;</span><br><span class="line">&#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> post.tags %&#125;</span><br><span class="line">    &lt;a href=<span class="string">"&#123;&#123; url_for(tag.path) &#125;&#125;"</span> rel=<span class="string">"tag"</span>&gt;&#123;&#123; tag_indicate &#125;&#125; &#123;&#123; tag.name &#125;&#125;&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">&#123;%- endfor %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></p><p>在其后添加<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line"><span class="keyword">var</span> tagsall=<span class="built_in">document</span>.getElementsByClassName(<span class="string">"post-tags"</span>)</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = tagsall.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">    <span class="keyword">var</span> tags=tagsall[i].getElementsByTagName(<span class="string">"a"</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> j = tags.length - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">        <span class="keyword">var</span> r=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">130</span>);</span><br><span class="line">        <span class="keyword">var</span> g=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">var</span> b=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">80</span>);</span><br><span class="line">        tags[j].style.background = <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;                        </span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;今天又耗费一个上午给我的博客折腾出了彩色标签，接下来记录我的设置过程&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo框架" scheme="http://zhengyujie.cn/categories/Hexo%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Hexo" scheme="http://zhengyujie.cn/tags/Hexo/"/>
    
      <category term="NexT" scheme="http://zhengyujie.cn/tags/NexT/"/>
    
      <category term="JavaScript" scheme="http://zhengyujie.cn/tags/JavaScript/"/>
    
      <category term="HTML" scheme="http://zhengyujie.cn/tags/HTML/"/>
    
      <category term="CSS" scheme="http://zhengyujie.cn/tags/CSS/"/>
    
  </entry>
  
  <entry>
    <title>Hexo添加Valine评论功能</title>
    <link href="http://zhengyujie.cn/2019/08/18/valine%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/"/>
    <id>http://zhengyujie.cn/2019/08/18/valine评论系统/</id>
    <published>2019-08-18T02:41:28.000Z</published>
    <updated>2019-08-18T03:42:26.436Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一直想给自己的博客添加评论功能，比较靠谱的评论系统有GitHub上的gitment，gitalk，gitter，来必力(有点儿花哨)，以及一些国外的评论系统(需要翻墙)。偶然间了解到一款国人开发的评论系统叫Valine用的是LeanCloud作为数据库，洁面很简洁，不像来必力那样花哨只是单纯的评论，简洁到没有后台，删除和管理评论直接操作数据库，于是决定就是你了。本片博客用于记录添加Valine评论功能的完整过程。</p><a id="more"></a><h1 id="添加Valine评论"><a href="#添加Valine评论" class="headerlink" title="添加Valine评论"></a>添加Valine评论</h1><ol><li><p>注册LeanCloud</p></li><li><p>创建一个开发版应用</p></li><li><p>创建 Class<br> 在LeanCloud -&gt; 存储 -&gt; 创建Class -&gt; 无限制的Class</p></li><li>关闭服务开关<br> 在LeanCloud -&gt; 设置 -&gt; 安全中心 -&gt; 服务开关，把除数据存储其他选项都关闭。</li><li>添加域名<br> 在LeanCloud -&gt; 设置 -&gt; 安全中心 -&gt; Wwb安全域名，注意格式</li><li><p>修改主题配置文件<br> 打开主题配置文件<code>_config.yml</code>，搜索找到<code>Valine</code>部分，填写appid以及appkey</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Valine</span></span><br><span class="line"><span class="comment"># You can get your appid and appkey from https://leancloud.cn</span></span><br><span class="line"><span class="comment"># For more information: https://valine.js.org, https://github.com/xCss/Valine</span></span><br><span class="line"><span class="attr">valine:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version</span></span><br><span class="line"><span class="attr">appid:</span> <span class="comment"># your appid</span></span><br><span class="line"><span class="attr">appkey:</span> <span class="comment"># your appkey</span></span><br><span class="line"><span class="attr">notify:</span> <span class="literal">true</span> <span class="comment"># Mail notifier. See: https://github.com/xCss/Valine/wiki</span></span><br><span class="line"><span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line"><span class="attr">placeholder:</span> <span class="string">Just</span> <span class="string">go</span> <span class="string">go</span> <span class="comment"># Comment box placeholder</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">mm</span> <span class="comment"># Gravatar style</span></span><br><span class="line"><span class="attr">guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># Custom comment header</span></span><br><span class="line"><span class="attr">pageSize:</span> <span class="number">10</span> <span class="comment"># Pagination size</span></span><br><span class="line"><span class="attr">language:</span> <span class="comment"># Language, available values: en, zh-cn</span></span><br><span class="line"><span class="attr">visitor:</span> <span class="literal">false</span> <span class="comment"># leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br><span class="line"><span class="attr">comment_count:</span> <span class="literal">true</span> <span class="comment"># If false, comment count will only be displayed in post page, not in home page</span></span><br></pre></td></tr></table></figure></li><li><p>开启email通知<br> 若要开启邮件通知，首先修改主题配置文件：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">notify: true # Mail notifier. See: https://github.com/xCss/Valine/wiki</span><br></pre></td></tr></table></figure><p> 然后在LeanCloud -&gt; 设置 -&gt; 邮件模板<br> 用于重置密码的邮件主题修改为：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你在&#123;&#123;appname&#125;&#125; 的评论收到了新的回复</span><br></pre></td></tr></table></figure><p> 内容修改为：</p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>Hi, &#123;&#123;username&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">你在 &#123;&#123;appname&#125;&#125; 的评论收到了新的回复，请点击查看：</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"你的网址首页链接"</span> <span class="attr">style</span>=<span class="string">"display: inline-block; padding: 10px 20px; border-radius: 4px; background-color: #3090e4; color: #fff; text-decoration: none;"</span>&gt;</span>马上查看<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>最后欢迎大家在下方评论区测试评论</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;一直想给自己的博客添加评论功能，比较靠谱的评论系统有GitHub上的gitment，gitalk，gitter，来必力(有点儿花哨)，以及一些国外的评论系统(需要翻墙)。偶然间了解到一款国人开发的评论系统叫Valine用的是LeanCloud作为数据库，洁面很简洁，不像来必力那样花哨只是单纯的评论，简洁到没有后台，删除和管理评论直接操作数据库，于是决定就是你了。本片博客用于记录添加Valine评论功能的完整过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo框架" scheme="http://zhengyujie.cn/categories/Hexo%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Hexo" scheme="http://zhengyujie.cn/tags/Hexo/"/>
    
      <category term="NexT" scheme="http://zhengyujie.cn/tags/NexT/"/>
    
      <category term="Valine" scheme="http://zhengyujie.cn/tags/Valine/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客部署到阿里云服务器</title>
    <link href="http://zhengyujie.cn/2019/08/17/%E9%83%A8%E7%BD%B2%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://zhengyujie.cn/2019/08/17/部署到阿里云服务器/</id>
    <published>2019-08-17T10:30:42.000Z</published>
    <updated>2019-08-19T08:11:43.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这几天一折腾起来就发现完全停不下来了，前几天我把我的博客部署到了Coding平台，前天又手痒在阿里云上购买了一个域名。今早又没忍住购买了阿里云的学生服务器。于是我最终决定把我的博客部署到阿里云服务器上。在折腾了一个上午后，我终于成功的完成了博客的部署，这篇博客记录下我的整个部署流程。</p><a id="more"></a><h1 id="服务器环境搭建"><a href="#服务器环境搭建" class="headerlink" title="服务器环境搭建"></a>服务器环境搭建</h1><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><p>这没啥好说的：</p><pre><code>apt-get install git</code></pre><h2 id="配置Nginx"><a href="#配置Nginx" class="headerlink" title="配置Nginx"></a>配置Nginx</h2><ul><li><p>安装nginx</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install nginx</span><br></pre></td></tr></table></figure></li><li><p>nginx</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service start nginx</span><br></pre></td></tr></table></figure><p>  然后在浏览器输入服务器的公网地址，就可以看见nginx的默认页面。</p><p>  <strong>注意:</strong> 这里有个超级大坑，一定要在服务器的安全组规则中添加80端口，否则不会有任何输出。这里耗费了我一个多小时的时间，说多了都是泪。</p></li><li><p>默认配置<br>  <code>cd</code>进入nginx的配置文件目录，然后使用<code>vim</code>打开文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/nginx/sites-available</span><br><span class="line">sudo vim default</span><br></pre></td></tr></table></figure><p>  其中<code>server_name</code>修改为自己的域名，没有域名则无需修改。<br>  <code>root</code>修改为<code>/var/www/html/blog</code>，这是我们博客网站的根目录。</p></li></ul><h2 id="创建git用户"><a href="#创建git用户" class="headerlink" title="创建git用户"></a>创建git用户</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">adduser git</span><br><span class="line">chmod 740 /etc/sudoers</span><br><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure><p>找到以下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL=(ALL)    ALL</span><br></pre></td></tr></table></figure></p><p>在下面添加一行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git ALL=(ALL) ALL</span><br></pre></td></tr></table></figure></p><p>获得root权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd git</span><br></pre></td></tr></table></figure></p><h2 id="设置SSH"><a href="#设置SSH" class="headerlink" title="设置SSH"></a>设置SSH</h2><p>切换为git用户，创建 ~/.ssh 文件夹和 ~/.ssh/authorized_keys 文件，并赋予相应的权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su git</span><br><span class="line">mkdir ~/.ssh</span><br><span class="line">vim ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>然后将客户端<code>.ssh</code>文件夹下的<code>id_rsa.pub</code>文件里的内容复制到<code>authorized_keys</code>中，接着赋予相应的权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorzied_keys</span><br><span class="line">chmod 700 ~/.ssh</span><br></pre></td></tr></table></figure></p><p>然后我们在客户端终端上输入<code>ssh -v git@ip地址</code>就可以免密登录了</p><h2 id="git仓库设置"><a href="#git仓库设置" class="headerlink" title="git仓库设置"></a>git仓库设置</h2><p>切换到git用户，然后再服务器上初始化一个git裸库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su git</span><br><span class="line">cd ~</span><br><span class="line">git init --bare blog.git</span><br></pre></td></tr></table></figure></p><p>接着新建一个post-receive文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/blog.git/hooks/post-receive</span><br></pre></td></tr></table></figure></p><p>在文件中输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#！/bin/sh</span><br><span class="line">git --work-tree=/var/www/html/blog --git-dir=/home/git/blog.git checkout -f</span><br></pre></td></tr></table></figure></p><p>保存退出后再赋予该文件执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x ~/blog.git/hooks/post-receive</span><br></pre></td></tr></table></figure></p><h1 id="本地设置"><a href="#本地设置" class="headerlink" title="本地设置"></a>本地设置</h1><p>这里nodejs,npm,git,hexo等的安装就不再叙述了。本地的设置就很简单，只需要修改博客配置文件即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">    aliyun: git@ip地址:/home/git/blog.git,master</span><br></pre></td></tr></table></figure></p><p>然后我们在终端执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><p>博客文件就会上传到我们在服务器上的git仓库，然后再部署到我们创建的博客根目录。我们在浏览器中访问服务器地址，就可以看到我们的博客了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;这几天一折腾起来就发现完全停不下来了，前几天我把我的博客部署到了Coding平台，前天又手痒在阿里云上购买了一个域名。今早又没忍住购买了阿里云的学生服务器。于是我最终决定把我的博客部署到阿里云服务器上。在折腾了一个上午后，我终于成功的完成了博客的部署，这篇博客记录下我的整个部署流程。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo框架" scheme="http://zhengyujie.cn/categories/Hexo%E6%A1%86%E6%9E%B6/"/>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/categories/Ubuntu/"/>
    
    
      <category term="Hexo" scheme="http://zhengyujie.cn/tags/Hexo/"/>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/tags/Ubuntu/"/>
    
      <category term="SSH" scheme="http://zhengyujie.cn/tags/SSH/"/>
    
      <category term="阿里云" scheme="http://zhengyujie.cn/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
      <category term="Git" scheme="http://zhengyujie.cn/tags/Git/"/>
    
      <category term="Nginx" scheme="http://zhengyujie.cn/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu深度学习环境搭建</title>
    <link href="http://zhengyujie.cn/2019/08/16/ubuntu%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
    <id>http://zhengyujie.cn/2019/08/16/ubuntu深度学习环境/</id>
    <published>2019-08-16T02:15:48.000Z</published>
    <updated>2019-08-19T08:10:52.925Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>上次搭建深度学习环境已经是很久以前了，但我还记得被其支配的恐惧，各种出错，各种版本对应不上，搞得我头皮发麻。这次因为系统奔溃需要重装系统，也就意味着我需要重新搭建Ubuntu的深度学习环境，我心里是一百个不愿意的。但也没有办法，只能硬着头皮上。但出乎意料，这次搭建过程还算顺利，只有在tensorflow运行出了点小状况(后面会提到)。本篇博客会详细记录我的整个搭建过程，说不定那天我的系统又被我玩崩溃了。先附上我的深度学习环境基本信息：</p><ul><li>Ubuntu: 16.04</li><li>NVIDIA: GeForce GTX 1080TI</li><li>Driver Version: 430.40</li><li>CUDA: 10.0</li><li>cuDNN: 7.5.1</li><li>TensorFlow: 1.13.1</li></ul><a id="more"></a><h1 id="安装显卡驱动"><a href="#安装显卡驱动" class="headerlink" title="安装显卡驱动"></a>安装显卡驱动</h1><h2 id="禁用nouveau"><a href="#禁用nouveau" class="headerlink" title="禁用nouveau"></a>禁用nouveau</h2><p>打开终端输入：</p><pre><code>sudo vim /etc/modprobe.d/blacklist.conf</code></pre><p>在文件最后加上：</p><pre><code>blacklist nouveauoptions nouveau modeset=0</code></pre><p>保存退出后输入命令：</p><pre><code>sudo update-initramfs -u</code></pre><p>然后重启电脑，接着输入命令：</p><pre><code>lsmod | grep nouveau</code></pre><p>若没有输出信息，说明禁用成功</p><h2 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h2><p>首先去NVIDIA官网上下载对应的Linux版本的显卡驱动。</p><p>然后按Ctrl+Alt+F1进入命令行模式，输入用户名和密码</p><p>接着关闭图形界面：</p><pre><code>sudo service lightdm stop</code></pre><p>然后<code>cd</code>进去驱动所在的文件夹，获取权限：</p><pre><code>sudo chmod a+x NVIDIA-Linux-x86_64-xxx.run</code></pre><p>安装驱动：</p><pre><code>sudo ./NVIDIA-Linux-x86_64-410.78.run –no-x-check –no-nouveau-check –no-opengl-files</code></pre><ul><li>–no-x-check 安装驱动时关闭X服务</li><li>–no-nouveau-check 安装驱动时禁用nouveau</li><li>–no-opengl-files 只安装驱动文件，不安装OpenGL文件；安装了，如果是双显卡的话，会出现重复登录的问题</li></ul><p>完成安装后启动图形界面：</p><pre><code>sudo service lightdm start</code></pre><p>最后在终端输入：</p><pre><code>nvidia-smi</code></pre><p>若有输出显卡信息，说明驱动安装成功</p><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>下载CUDA10.0，下载地址：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p><code>cd</code>进入CUDA所在文件夹，赋予文件执行权限后安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_10.0.130_410.48_linux.run</span><br><span class="line">sudo ./cuda_10.0.130_410.48_linux.run</span><br></pre></td></tr></table></figure></p><p>开始安装后需要按空格键阅读条款，时间比较长，等不及的可以直接Ctrl+C跳过。阅读完使用条款后开始配置，一步一步慢慢来<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">accept/decline/quit: accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 410.48?</span><br><span class="line">(y)es/(n)o/(q)uit: n（这里不要再安装驱动了！！！）</span><br><span class="line"></span><br><span class="line">Install the CUDA 10.0 Toolkit?（是否安装CUDA 10 ，这里必须要安装）</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location（安装路径，使用默认，直接回车就行）</span><br><span class="line"> [ default is /usr/local/cuda-10.0 ]:  </span><br><span class="line"></span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda?（同意创建软链接）</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Install the CUDA 10.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-10.0 ...（开始安装）</span><br></pre></td></tr></table></figure></p><p>安装完成后加入环境变量</p><pre><code>sudo gedit ~/.bashrc</code></pre><p>最后一行加入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=$PATH:$CUDA_HOME/bin</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>保存退出后输入：</p><pre><code>source .bashrc</code></pre><p>最后终端输入：</p><pre><code>nvcc --version</code></pre><p>若输出CUDA版本信息说明安装成功</p><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p>从 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a> 上下载cudnn相应版本的压缩包（需要注册或登录）。</p><p>解压当前的.tgz格式的软件包</p><pre><code>tar -xzvf cudnn-10.0-linux-x64-v7.5.1.10.tgz</code></pre><p>解压后的文件夹名为cuda，文件夹中包含两个文件夹：一个为include，另一个为lib64。最好把这个cuda文件夹放在home目录下，方便操作。</p><p>将解压后的lib64文件夹关联到环境变量中：</p><pre><code>sudo gedit ~/.bashrc</code></pre><p>最后一行加入：</p><pre><code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></pre><p>保存退出后输入：</p><pre><code>source .bashrc</code></pre><p>最后一步就是将解压后的/home/cuda/include目录下的一些文件拷贝到/usr/local/cuda/include中。由于进入了系统路径，因此执行该操作时需要获取管理员权限：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p><p>完成cuDNN的配置</p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><p>这步比较简单，直接使用pip命令安装即可(我安装的是1.13.1版本)：</p><pre><code>pip install tensorflow-gpu==1.13.1</code></pre><p>然后进入交互式命令行看看是否安装成功：</p><pre><code>import tensorflow as tf</code></pre><p>接着就突然出现了FutureWarning警告信息，吓出我一身冷汗。我赶紧去网上查阅相关资料，看到有人说是因为numpy的版本是1.17太高了。我赶紧查看了我安装的numpy的版本，果然是1.17。于是我进行了降级安装，安装了1.16.4版本的numpy，最终问题得以完美解决。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;上次搭建深度学习环境已经是很久以前了，但我还记得被其支配的恐惧，各种出错，各种版本对应不上，搞得我头皮发麻。这次因为系统奔溃需要重装系统，也就意味着我需要重新搭建Ubuntu的深度学习环境，我心里是一百个不愿意的。但也没有办法，只能硬着头皮上。但出乎意料，这次搭建过程还算顺利，只有在tensorflow运行出了点小状况(后面会提到)。本篇博客会详细记录我的整个搭建过程，说不定那天我的系统又被我玩崩溃了。先附上我的深度学习环境基本信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu: 16.04&lt;/li&gt;
&lt;li&gt;NVIDIA: GeForce GTX 1080TI&lt;/li&gt;
&lt;li&gt;Driver Version: 430.40&lt;/li&gt;
&lt;li&gt;CUDA: 10.0&lt;/li&gt;
&lt;li&gt;cuDNN: 7.5.1&lt;/li&gt;
&lt;li&gt;TensorFlow: 1.13.1&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/categories/Ubuntu/"/>
    
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/tags/Ubuntu/"/>
    
      <category term="CUDA" scheme="http://zhengyujie.cn/tags/CUDA/"/>
    
      <category term="cuDNN" scheme="http://zhengyujie.cn/tags/cuDNN/"/>
    
  </entry>
  
  <entry>
    <title>Windows下安装Ubuntu16.04双系统</title>
    <link href="http://zhengyujie.cn/2019/08/16/%E5%AE%89%E8%A3%85Ubuntu%E7%B3%BB%E7%BB%9F/"/>
    <id>http://zhengyujie.cn/2019/08/16/安装Ubuntu系统/</id>
    <published>2019-08-16T01:00:55.000Z</published>
    <updated>2019-08-19T08:11:19.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>昨天在更新tensorflow时我的Ubuntu系统突然崩溃了，无奈只能重装系统。但距离上次安装Ubuntu系统已经过去许久，只能去网上搜教程折腾，好在整个过程异常顺利，于是用本博客记录下我的整个安装步骤。</p><a id="more"></a><h1 id="完整步骤"><a href="#完整步骤" class="headerlink" title="完整步骤"></a>完整步骤</h1><ol><li><p><strong>空间准备</strong></p><p> 在Windows 10中打开“磁盘管理器”，找一个空闲的磁盘分区，压缩出来一部分空间给Ubuntu使用，压缩出来的硬盘应处于未分配状态。或者通过删除某个不使用的本地磁盘使其处于未分配状态。这里我给Ubuntu系统分配了100G空间</p></li><li><p><strong>制作启动盘</strong></p><ol><li>插入用来制作启动盘的U盘（会被格式化，请备份好重要文件），打开UltraISO刻录软件（免费无限期试用）。</li><li>选择“文件(F)”-&gt;“打开”，找到“Ubuntu-16.04-desktop-amd64.iso”镜像文件，然后点击“打开”。</li><li>选择“启动(B)”-&gt;“写入硬盘映像”，打开启动盘制作界面。</li><li>然后点击下方的“写入”，会弹出警告提示，确定后，就会开始制作启动盘。写入完成后关闭UltraISO软件即可。</li></ol></li><li><p><strong>BIOS设置</strong></p><p> 我的主板为微星主板，重启电脑猛按DEL键即可进入BIOS界面，然后设置U盘为第一启动项。最后按F10保存设置并重启。</p></li><li><p><strong>安装Ubuntu系统</strong></p><ol><li>重启后便进入安装界面，通过上下键选择则第二项：<code>Install Ubuntu</code></li><li>选择语言</li><li><strong>不要</strong>选择为图形或者无线硬件，以及Mp3和其他媒体安装第三方软件。</li><li>选择最后一项：其它选项</li><li>选中我们预留好的100G空间，然后点击+号新建分区</li><li>新建分区：<ol><li>大小一般为8G</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于交换空间</li><li>挂载点/swap</li></ol></li><li>新建分区：<ol><li>剩余空间的一半分给它，我分了50G</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于EXT4日志文件系统</li><li>挂载点/home</li></ol></li><li>新建分区：<ol><li>大小剩下的空间都给它</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于EXT4日志文件系统</li><li>挂载点/</li></ol></li><li>设置安装引导的启动设置，在下拉中选择Windows Boot Manager所在的整块硬盘（不带数字标号）</li><li>点击继续</li><li>选择Shanghai，定位时区</li><li>键盘布局中英都行，以后可以改</li><li>设置用户名以及密码</li><li>等待进度条结束即可完成安装</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;昨天在更新tensorflow时我的Ubuntu系统突然崩溃了，无奈只能重装系统。但距离上次安装Ubuntu系统已经过去许久，只能去网上搜教程折腾，好在整个过程异常顺利，于是用本博客记录下我的整个安装步骤。&lt;/p&gt;
    
    </summary>
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/categories/Ubuntu/"/>
    
    
      <category term="Ubuntu" scheme="http://zhengyujie.cn/tags/Ubuntu/"/>
    
      <category term="系统安装" scheme="http://zhengyujie.cn/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Hexo部署博客到Coding</title>
    <link href="http://zhengyujie.cn/2019/08/13/%E9%83%A8%E7%BD%B2%E5%88%B0coding/"/>
    <id>http://zhengyujie.cn/2019/08/13/部署到coding/</id>
    <published>2019-08-13T11:07:46.000Z</published>
    <updated>2019-08-19T08:11:30.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前我的博客部署在GitHub上，访问速度有点揪心，于是萌生了把我的博客部署在Coding上的想法。正好今天下午想偷个懒，于是马上查资料开始捣鼓。在耗费了我快一个半小时的时间后，终于成功的将我的博客部署在Coding上。用这篇博客记录下我整个折腾过程。</p><a id="more"></a><h1 id="Coding平台介绍"><a href="#Coding平台介绍" class="headerlink" title="Coding平台介绍"></a>Coding平台介绍</h1><p>Coding 是基于云计算技术的软件开发平台，集项目管理、代码托管、运行空间、质量控制为一体。在云计算时代，Coding推动软件开发的云端化，使开发者能用一个浏览器完成开发的各个环节。开发人员可以专心构建业务问题的解决方案，而非管理运营或发布堆栈，确保应用满足产品层目标服务等级，同时更为企业层级的项目应用提供了代码质量检验以及项目质量把控的渠道和标准。在保证私有项目的数据安全和稳定的同时，Coding 还结合了冒泡及评论、公开项目发布与讨论等一系列社交化协作功能，打造具有技术支撑的开发者社区。——摘自百度百科</p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>关于git,node.js,hexo等安装步骤这里不再赘述。</p><h2 id="Coding平台设置"><a href="#Coding平台设置" class="headerlink" title="Coding平台设置"></a>Coding平台设置</h2><ol><li>注册Coding<br> <strong>注意：</strong> 一定要注册Coding个人版，而不要注册Coding企业版，说多了都是泪。</li><li><p>新建项目<br> 项目名称填</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yourname.coding.me</span><br></pre></td></tr></table></figure><p> 这里的yourname最好是你注册Coding时的username<br> 建议勾选启用README.MD文件初始化项目</p></li><li>开启Pages服务<br> 在开启后我们就可以通过用户名+网站后缀来访问博客，而且还可以绑定域名通过固定域名来访问。</li></ol><h2 id="SSH设置"><a href="#SSH设置" class="headerlink" title="SSH设置"></a>SSH设置</h2><ol><li><p>检查电脑是否已生成SSH Key:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><p> 若在目录下存在id_rsa.pub或者id_dsa.pub文件，那么直接到第三步</p></li><li><p>创建SSH Key:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p> 参数含义：</p><ul><li>-t 指定密钥类型，默认是rsa</li><li><p>-C 设置注释文字，比如用户名</p><p>此处我们直接按下回车使用默认文件名创建，那么就会生成id_rsa和id_rsa.pub两个秘钥文件。接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是Coding管理者的密码），当然，你也可以不输入密码，我直接按回车。那么push的时候就不需要输入密码，直接提交到Git服务器上了</p></li></ul></li><li><p>在Coding中配置SSH<br> 点击个人设置，再选择SSH公钥。然后点击新填公钥，用记事本打开id_rsa.pub文件，将里面的内容全部复制到公钥内容中，公约名称可以不填。最后点击添加即可。</p></li><li><p>SSH测试</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@git.coding.net</span><br></pre></td></tr></table></figure><p> 若出现以下说明则说明SSH配置成功</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello username You&apos;ve connected to Coding.net by SSH successfully!</span><br></pre></td></tr></table></figure></li></ol><h2 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h2><ol><li><p>修改配置文件<br> 打开博客根目录配置文件_config.yml，找到<code>deploy</code>，填写以下内容</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo:</span><br><span class="line">    github: git@github.com:yourname/yourname.github.io.git,master</span><br><span class="line">    coding: git@git.coding.net:yourname/yourname.git,master</span><br></pre></td></tr></table></figure></li><li><p>部署博客<br> <code>cd</code>进博客根目录，输入以下命令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g #生成静态网页</span><br><span class="line">hexo d #部署博客</span><br></pre></td></tr></table></figure><p> 最后打开 <a href="http://yourname.coding.me" target="_blank" rel="noopener">http://yourname.coding.me</a> 即可看到你的博客主页</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;之前我的博客部署在GitHub上，访问速度有点揪心，于是萌生了把我的博客部署在Coding上的想法。正好今天下午想偷个懒，于是马上查资料开始捣鼓。在耗费了我快一个半小时的时间后，终于成功的将我的博客部署在Coding上。用这篇博客记录下我整个折腾过程。&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo框架" scheme="http://zhengyujie.cn/categories/Hexo%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Hexo" scheme="http://zhengyujie.cn/tags/Hexo/"/>
    
      <category term="Coding" scheme="http://zhengyujie.cn/tags/Coding/"/>
    
      <category term="git" scheme="http://zhengyujie.cn/tags/git/"/>
    
      <category term="SSH" scheme="http://zhengyujie.cn/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>Non-local PyTorch部分源码解读</title>
    <link href="http://zhengyujie.cn/2019/08/13/non-local%E7%9A%84pytorch%E6%BA%90%E7%A0%81/"/>
    <id>http://zhengyujie.cn/2019/08/13/non-local的pytorch源码/</id>
    <published>2019-08-13T07:16:47.000Z</published>
    <updated>2019-08-19T08:17:57.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>代码地址：<a href="https://github.com/AlexHex7/Non-local_pytorch" target="_blank" rel="noopener">https://github.com/AlexHex7/Non-local_pytorch</a></p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我只看了non-local_embedded_gaussian.py文件下的源码，以下为我的解读</p><a id="more"></a><h1 id="结构图示"><a href="#结构图示" class="headerlink" title="结构图示"></a>结构图示</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/P6cAD8NTqf3a.png?imageslim" alt="mark"></p><h1 id="部分代码解读"><a href="#部分代码解读" class="headerlink" title="部分代码解读"></a>部分代码解读</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_NonLocalBlockND</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    in_channels为输入的通道数</span></span><br><span class="line"><span class="string">    inter_channels为中间过程的通道数</span></span><br><span class="line"><span class="string">    dimension为维度数</span></span><br><span class="line"><span class="string">    sub_sample标志是否进行下采样(subsampled)</span></span><br><span class="line"><span class="string">    bn_layer标示是否进行Batch Norm</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, inter_channels=None, dimension=<span class="number">3</span>, sub_sample=True, bn_layer=True)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># assert用来检查条件，不符合就终止</span></span><br><span class="line">        <span class="comment"># 只能处理一维，二维以及三维的输入数据</span></span><br><span class="line">        <span class="keyword">assert</span> dimension <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">        self.sub_sample = sub_sample</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.inter_channels = inter_channels</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若没有指定中间过程的通道数，则指定为输入通道数的一半</span></span><br><span class="line">        <span class="keyword">if</span> self.inter_channels <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.inter_channels = in_channels // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> self.inter_channels == <span class="number">0</span>:</span><br><span class="line">                self.inter_channels = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据输入的维数来指定对应的卷积函数，池化函数以及归一化函数</span></span><br><span class="line">        <span class="keyword">if</span> dimension == <span class="number">3</span>:</span><br><span class="line">            conv_nd = nn.Conv3d</span><br><span class="line">            max_pool_layer = nn.MaxPool3d(kernel_size=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm3d</span><br><span class="line">        <span class="keyword">elif</span> dimension == <span class="number">2</span>:</span><br><span class="line">            conv_nd = nn.Conv2d</span><br><span class="line">            max_pool_layer = nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv_nd = nn.Conv1d</span><br><span class="line">            max_pool_layer = nn.MaxPool1d(kernel_size=(<span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm1d</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 指定g函数</span></span><br><span class="line">        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                         kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否需要进行归一化操作</span></span><br><span class="line">        <span class="keyword">if</span> bn_layer:</span><br><span class="line">            self.W = nn.Sequential(</span><br><span class="line">                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,</span><br><span class="line">                        kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">                bn(self.in_channels)</span><br><span class="line">            )</span><br><span class="line">            nn.init.constant(self.W[<span class="number">1</span>].weight, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant(self.W[<span class="number">1</span>].bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,</span><br><span class="line">                             kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 初始化为0</span></span><br><span class="line">            nn.init.constant(self.W.weight, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant(self.W.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                             kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                           kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否需要进行下采样</span></span><br><span class="line">        <span class="keyword">if</span> sub_sample:</span><br><span class="line">            self.g = nn.Sequential(self.g, max_pool_layer)</span><br><span class="line">            self.phi = nn.Sequential(self.phi, max_pool_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param x: (b, c, t, h, w)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得batch的大小</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># g(x)的size为batch_size*inter_channels*W*H</span></span><br><span class="line">        <span class="comment"># view类似于resize，使得个g_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        g_x = self.g(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 维度换位，g_x的size变成batch_size*(W*H)*inter_channels</span></span><br><span class="line">        g_x = g_x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        theta_x = self.theta(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta_x的size为batch_size*(W*H)*inter_channels</span></span><br><span class="line">        theta_x = theta_x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># phi_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        phi_x = self.phi(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># f的size为batch_size*(W*H)*(W*H)</span></span><br><span class="line">        f = torch.matmul(theta_x, phi_x)</span><br><span class="line"></span><br><span class="line">        f_div_C = F.softmax(f, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># y的size为batch_size*(H*W)*inter_channels</span></span><br><span class="line">        y = torch.matmul(f_div_C, g_x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，</span></span><br><span class="line">        <span class="comment"># 需要用contiguous()来返回一个contiguous copy。</span></span><br><span class="line">        <span class="comment"># y的size为batch_size*inter_channels*(H*W)</span></span><br><span class="line">        y = y.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># y的size为batch_size*inter_channels*H*W</span></span><br><span class="line">        y = y.view(batch_size, self.inter_channels, *x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># W_y的size为batch_size*out_channels*W*H</span></span><br><span class="line">        W_y = self.W(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到最终输出</span></span><br><span class="line">        z = W_y + x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;代码地址：&lt;a href=&quot;https://github.com/AlexHex7/Non-local_pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/AlexHex7/Non-local_pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;我只看了non-local_embedded_gaussian.py文件下的源码，以下为我的解读&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="PyTorch" scheme="http://zhengyujie.cn/tags/PyTorch/"/>
    
      <category term="源码解读" scheme="http://zhengyujie.cn/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>空洞卷积(Dilated Convolution)学习笔记</title>
    <link href="http://zhengyujie.cn/2019/08/13/dilated_conv/"/>
    <id>http://zhengyujie.cn/2019/08/13/dilated_conv/</id>
    <published>2019-08-13T06:24:15.000Z</published>
    <updated>2019-08-19T08:16:06.629Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>引入空洞卷积不得不提的是感受野，感受野就是卷积神经网络的每一层输出的特征图(feature map)上的像素点在原图像上映射的区域大小。空洞卷积主要为了解决图像分割中的一些问题而提出的，在FCN中通过pooling增大感受野缩小图像尺寸，然后通过upsampling还原图像尺寸，但是这个过程中造成了精度的损失，那么为了减小这种损失理所当然想到的是去掉pooling层，然而这样就导致特征图感受野太小，因此空洞卷积应运而生。</p><a id="more"></a><h1 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>Dilated Convolutions，翻译为扩张卷积或空洞卷积。扩张卷积与普通的卷积相比，除了卷积核的大小以外，还有一个扩张率(dilation rate)参数，主要用来表示扩张的大小。扩张卷积与普通卷积的相同点在于，卷积核的大小是一样的，在神经网络中即参数数量不变，区别在于扩张卷积具有更大的感受野。</p><h2 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/yljmAFjJxXBF.png?imageslim" alt="mark"></p><ul><li>图(a)为1-dilated conv，感受野为3×3</li><li>图(b)为2-dilated conv，跟在1-dilated conv后面，感受野扩大为为7×7</li><li>图(c)为4-dilated conv，同样跟在1-dilated conv以及1-dilated conv后面，感受野扩大为为15×15</li><li>相比之下，使用stride为1的普通卷积，三层后的感受野仅为7×7</li></ul><h2 id="空洞卷积的作用"><a href="#空洞卷积的作用" class="headerlink" title="空洞卷积的作用"></a>空洞卷积的作用</h2><ul><li><p><strong>扩大感受野</strong><br>  在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</p></li><li><p><strong>捕获多尺度上下文信息</strong><br>  空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。</p></li></ul><h2 id="空洞卷积存在的问题"><a href="#空洞卷积存在的问题" class="headerlink" title="空洞卷积存在的问题"></a>空洞卷积存在的问题</h2><ul><li><p><strong>The Gridding Effect</strong><br>  假设我们仅仅多次叠加 dilation rate 2 的 3 x 3 kernel 的话，则会出现这个问题：</p><p>  <img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/9kvcJLQc3gW6.png?imageslim" alt="mark"></p><p>  很明显，感受野不连续（我们上一小结的例子就没这个问题，所以空洞卷积依赖网络设计）。</p></li><li><p><strong>Long-ranged information might be not relevant</strong><br>  我们从 dilated convolution 的设计背景来看就能推测出这样的设计是用来获取 long-ranged information。然而光采用大 dilation rate 的信息或许只对一些大物体分割有效果，而对小物体来说可能则有弊无利了。如何同时处理不同大小的物体的关系，则是设计好 dilated convolution 网络的关键。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;引入空洞卷积不得不提的是感受野，感受野就是卷积神经网络的每一层输出的特征图(feature map)上的像素点在原图像上映射的区域大小。空洞卷积主要为了解决图像分割中的一些问题而提出的，在FCN中通过pooling增大感受野缩小图像尺寸，然后通过upsampling还原图像尺寸，但是这个过程中造成了精度的损失，那么为了减小这种损失理所当然想到的是去掉pooling层，然而这样就导致特征图感受野太小，因此空洞卷积应运而生。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="空洞卷积" scheme="http://zhengyujie.cn/tags/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记11：ResNet</title>
    <link href="http://zhengyujie.cn/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/"/>
    <id>http://zhengyujie.cn/2019/08/12/tf学习笔记11/</id>
    <published>2019-08-12T11:24:47.000Z</published>
    <updated>2019-08-19T08:15:38.429Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>ResNet是由Kaiming He等4名华人提出，通过使用Residual Unit成功训练了152层的深度神经网络，在ILSVRC 2015比赛中获得冠军，取得了3.57%的top-5错误率，同时参数却比VGGNet少。ResNet的结构可以极快的加速超深神经网络的训练，模型的准确率也有较大的提升。之后很多方法都建立在ResNet的基础上完成的，例如检测，分割，识别等领域都纷纷使用ResNet。在ResNet推出不久，Google就借鉴了ResNet的精髓，提出了Inception V4和Inception-ResNet-V2，并通过融合这两个模型，在ILSVRC数据集上取得了惊人的3.08%的错误率。所以可见ResNet确实很好用。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/z6ycUHr6A6NC.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><h2 id="导入包并设计Block模块组"><a href="#导入包并设计Block模块组" class="headerlink" title="导入包并设计Block模块组"></a>导入包并设计Block模块组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">slim = tf.contrib.slim</span><br></pre></td></tr></table></figure><p>我们使用collections.namedtuple设计ResNet基本Block模块组的named tuple<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">scope为生成的Block的名称</span></span><br><span class="line"><span class="string">unit_fn为残差学习元生成函数</span></span><br><span class="line"><span class="string">args是一个长度等于Block中单元数目的序列，序列中每个元素</span></span><br><span class="line"><span class="string">包含第三层通道数，前两层通道数以及中间层步长(depth,depth_bottleneck,stride)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">'Block'</span>,[<span class="string">'scope'</span>,<span class="string">'unit_fn'</span>,<span class="string">'args'</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'A named tuple describing a ResNet block'</span></span><br></pre></td></tr></table></figure></p><p>以下面Block(‘block1’, bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)])为例</p><ul><li>block1: 是这个Block的名称</li><li>bottleneck: 前面定义的残差学习单元（有三层）</li><li>[(256, 64, 1)] * 2 + [(256, 64, 2)]: 是一个列表，其中每个元素都对应一个bottleneck残差学习单元，前面两个元素都是(256, 64, 1),最后一个是(256, 64, 2)。每个元素都时一个3元组，即（depth, depth_bottleneck, stride）,代表构建的bottleneck残差学习单元中，第三层的输出通道为256（depth），前两层的输出通道数为64（depth_bottleneck）且中间那层的步长stride为1（stride）</li></ul><h2 id="定义部分方法"><a href="#定义部分方法" class="headerlink" title="定义部分方法"></a>定义部分方法</h2><ul><li><p>定义一个降采样subsample的方法</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">降采样函数   </span></span><br><span class="line"><span class="string">input为输入</span></span><br><span class="line"><span class="string">factor为采样因子</span></span><br><span class="line"><span class="string">使用slim.max_pool2d来实现</span></span><br><span class="line"><span class="string">'''</span>    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsample</span><span class="params">(inputs,factor,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> factor==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> slim.max_pool2d(inputs,[<span class="number">1</span>,<span class="number">1</span>],stride=factor,scope=scope)</span><br></pre></td></tr></table></figure></li><li><p>定义一个conv2d_same函数创建卷积层</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_same</span><span class="params">(inputs,num_outputs,kernel_size,stride,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> stride==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> slim.conv2d(inputs,num_outputs,kernel_size,stride=<span class="number">1</span>,padding=<span class="string">'SAME'</span>,scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 显式地pad zero</span></span><br><span class="line">        pad_total=kernel_size - <span class="number">1</span></span><br><span class="line">        pad_beg=pad_total // <span class="number">2</span></span><br><span class="line">        pad_end=pad_total-pad_beg</span><br><span class="line">        <span class="comment">#使用tf.pad对图像进行填充</span></span><br><span class="line">        inputs = tf.pad(inputs, [[<span class="number">0</span>, <span class="number">0</span>], [pad_beg, pad_end], [pad_beg, pad_end], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">return</span> slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,</span><br><span class="line">                        padding=<span class="string">'VALID'</span>, scope=scope)</span><br></pre></td></tr></table></figure></li><li><p>定义堆叠Blocks的函数</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">net为输入</span></span><br><span class="line"><span class="string">blocks为定义的Block的class的列表</span></span><br><span class="line"><span class="string">outputs_collections是用来收集各个end_points的collections</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="meta">@slim.add_arg_scope</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack_blocks_dense</span><span class="params">(net,blocks,outputs_collections=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(block.scope,<span class="string">'block'</span>,[net])<span class="keyword">as</span> sc:</span><br><span class="line">            <span class="comment"># 拿到每个残差学习单元的args</span></span><br><span class="line">            <span class="keyword">for</span> i,unit <span class="keyword">in</span> enumerate(block.args):</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">'unit_%d'</span>%(i+<span class="number">1</span>),values=[net]):</span><br><span class="line">                    <span class="comment">#获取每个Block中的参数，包括第三层通道数，前两层通道数以及中间层步长</span></span><br><span class="line">                    unit_depth,unit_depth_bottleneck,unit_stride=unit    </span><br><span class="line">                    <span class="comment">#unit_fn是Block类的残差神经元生成函数，它按顺序创建残差学习元并进行连接</span></span><br><span class="line">                    net=block.unit_fn(net,</span><br><span class="line">                                    depth=unit_depth,</span><br><span class="line">                                    depth_bottleneck=unit_depth_bottleneck,</span><br><span class="line">                                    stride=unit_stride)</span><br><span class="line">            <span class="comment"># 将输出的net添加到collection中</span></span><br><span class="line">            net=slim.utils.collect_named_outputs(outputs_collections,sc.name,net)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure></li><li><p>创建ResNet通用的arg_scope</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_arg_scope</span><span class="params">(is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                    weight_decay=<span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_decay=<span class="number">0.997</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_epsilon=<span class="number">1e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_scale=True)</span>:</span></span><br><span class="line">    batch_norm_params = &#123;</span><br><span class="line">    <span class="string">'decay'</span>: batch_norm_decay,</span><br><span class="line">    <span class="string">'epsilon'</span>: batch_norm_epsilon,</span><br><span class="line">    <span class="string">'scale'</span>: batch_norm_scale,</span><br><span class="line">    <span class="string">'updates_collections'</span>: tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">    <span class="string">'fused'</span>: <span class="literal">None</span>,  <span class="comment"># Use fused batch norm if possible.</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置slim.conv2d函数的默认参数</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d],</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(weight_decay),</span><br><span class="line">    weights_initializer=slim.variance_scaling_initializer(),</span><br><span class="line">    activation_fn=tf.nn.relu,</span><br><span class="line">    normalizer_fn=slim.batch_norm,</span><br><span class="line">    normalizer_params=batch_norm_params):</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm], **batch_norm_params):</span><br><span class="line">            <span class="keyword">with</span> slim.arg_scope([slim.max_pool2d], padding=<span class="string">'SAME'</span>) <span class="keyword">as</span> arg_sc:</span><br><span class="line">                <span class="keyword">return</span> arg_sc</span><br></pre></td></tr></table></figure></li><li><p>定义核心的bottleneck残差学习单元</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用add_arg_scope使bottleneck函数能够直接使用slim.arg_scope设置默认参数</span></span><br><span class="line"><span class="meta">@slim.add_arg_scope</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bottleneck</span><span class="params">(inputs,depth,depth_bottleneck,stride,</span></span></span><br><span class="line"><span class="function"><span class="params">            outputs_collections=None,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'bottleneck_v2'</span>,[inputs])<span class="keyword">as</span> sc:</span><br><span class="line">        <span class="comment"># 获取输入的最后一个维度，即输入的通道数目</span></span><br><span class="line">        <span class="comment"># 参数min_rank=4可以限定最少为4个维度</span></span><br><span class="line">        depth_in=slim.utils.last_dimension(inputs.get_shape(),min_rank=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 先对输入进行Batch Normalization，再进行非线性激活</span></span><br><span class="line">        preact=slim.batch_norm(inputs,activation_fn=tf.nn.relu,scope=<span class="string">'preact'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果残差神经元的输出通道数目和输入的通道数目相同，那么直接对图像进行降采样，以保证shortcut尺寸和经历三个卷积层后的输出的此存相同        </span></span><br><span class="line">        <span class="keyword">if</span> depth==depth_in:</span><br><span class="line">            shortcut=subsample(inputs,stride,<span class="string">'shortcut'</span>)</span><br><span class="line">        <span class="comment"># 如果残差神经元的输出通道数目和输入的通道数目不同，利用尺寸为1x1的卷积核对输入进行卷积，使输入通道数相同；</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shortcut=slim.conv2d(preact,depth,[<span class="number">1</span>,<span class="number">1</span>],stride=stride,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                activation_fn=<span class="literal">None</span>,scope=<span class="string">'shortcut'</span>)</span><br><span class="line">        <span class="comment"># 然后，定义残差，即三个卷积层           </span></span><br><span class="line">        residual=slim.conv2d(preact,depth_bottleneck,[<span class="number">1</span>,<span class="number">1</span>],stride=<span class="number">1</span>,scope=<span class="string">'conv1'</span>)</span><br><span class="line">        residual=conv2d_same(residual,depth_bottleneck,<span class="number">3</span>,stride,scope=<span class="string">'conv2'</span>)</span><br><span class="line">        residual=slim.conv2d(residual,depth,[<span class="number">1</span>,<span class="number">1</span>],stride=<span class="number">1</span>,normalizer_fn=<span class="literal">None</span>,activation_fn=<span class="literal">None</span>,scope=<span class="string">'conv3'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将shortcut和residual相加，作为输出        </span></span><br><span class="line">        output=shortcut+residual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slim.utils.collect_named_outputs(outputs_collections,sc.name,output)</span><br></pre></td></tr></table></figure></li></ul><h2 id="定义生成ResNet-V2的主函数"><a href="#定义生成ResNet-V2的主函数" class="headerlink" title="定义生成ResNet V2的主函数"></a>定义生成ResNet V2的主函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">input是输入</span></span><br><span class="line"><span class="string">blocks包含残差学习元的参数</span></span><br><span class="line"><span class="string">num_classes是输出分类数</span></span><br><span class="line"><span class="string">global_pool标志是否加上最后一层全局平均年池化</span></span><br><span class="line"><span class="string">include_root_block标志是否加上ResNet网络最前面通常使用的7×7卷积和最大池化</span></span><br><span class="line"><span class="string">reuse标志是否重用</span></span><br><span class="line"><span class="string">scope是整个网络的名称 </span></span><br><span class="line"><span class="string">'''</span>     </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2</span><span class="params">(inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">              blocks,</span></span></span><br><span class="line"><span class="function"><span class="params">              num_classes=None,</span></span></span><br><span class="line"><span class="function"><span class="params">              global_pool=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              include_root_block=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              reuse=None,</span></span></span><br><span class="line"><span class="function"><span class="params">              scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'resnet_v2'</span>,[inputs],reuse=reuse) <span class="keyword">as</span> sc:</span><br><span class="line">        end_points_collection=sc.original_name_scope+<span class="string">'_end_points'</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d,bottleneck,stack_blocks_dense],</span><br><span class="line">                            outputs_collections=end_points_collection):</span><br><span class="line">            net=inputs</span><br><span class="line">            <span class="keyword">if</span> include_root_block:</span><br><span class="line">                <span class="keyword">with</span> slim.arg_scope([slim.conv2d],activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>):</span><br><span class="line">                    <span class="comment">#卷积核为7x7步长为2的卷积层</span></span><br><span class="line">                    net=conv2d_same(net,<span class="number">64</span>,<span class="number">7</span>,stride=<span class="number">2</span>,scope=<span class="string">'conv1'</span>)</span><br><span class="line">                <span class="comment">#最大值池化</span></span><br><span class="line">                net=slim.max_pool2d(net,[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">'pool1'</span>)</span><br><span class="line">            <span class="comment">#调用stack_blocks_dense堆叠残差学习元，每个有三个卷积层</span></span><br><span class="line">            net=stack_blocks_dense(net,blocks)</span><br><span class="line">            <span class="comment">#先做batch norm然后使用relu激活</span></span><br><span class="line">            net=slim.batch_norm(net,activation_fn=tf.nn.relu,scope=<span class="string">'postnorm'</span>)</span><br><span class="line">            <span class="keyword">if</span> global_pool:     </span><br><span class="line">                <span class="comment">#进行全局平均池化</span></span><br><span class="line">                net=tf.reduce_mean(net,[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">'pool5'</span>,keep_dims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment">#一个输出为num_classes的卷积层，不进行激活也不归一正则化。</span></span><br><span class="line">            <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                net=slim.conv2d(net,num_classes,[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,scope=<span class="string">'logits'</span>)</span><br><span class="line">            <span class="comment"># 将collection转化为Python的dict</span></span><br><span class="line">            end_points=slim.utils.convert_collection_to_dict(end_points_collection)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#使用softmax进行分类         </span></span><br><span class="line">            <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                end_points[<span class="string">'predictions'</span>]=slim.softmax(net,scope=<span class="string">'predictions'</span>)</span><br><span class="line">            <span class="keyword">return</span> net,end_points</span><br></pre></td></tr></table></figure><h2 id="定义不同深度的ResNet网络结构"><a href="#定义不同深度的ResNet网络结构" class="headerlink" title="定义不同深度的ResNet网络结构"></a>定义不同深度的ResNet网络结构</h2><ul><li><p>50层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_50</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_50'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">5</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>101层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 101层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_101</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_101'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span>+ [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">22</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>152层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 152层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_152</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_152'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">7</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>200层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 200层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_200</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_200'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">23</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li></ul><h2 id="耗时测试"><a href="#耗时测试" class="headerlink" title="耗时测试"></a>耗时测试</h2><ul><li><p>定义测试耗时函数</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session, target, info_string)</span>:</span></span><br><span class="line">    num_steps_burn_in = <span class="number">10</span>  <span class="comment"># 打印阈值</span></span><br><span class="line">    total_duration = <span class="number">0.0</span>    <span class="comment"># 每一轮所需要的迭代时间</span></span><br><span class="line">    total_duration_aquared = <span class="number">0.0</span>  <span class="comment"># 每一轮所需要的迭代时间的平方</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time    <span class="comment"># 计算耗时</span></span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_steps_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</span><br><span class="line">                print(<span class="string">"%s : step %d, duration = %.3f"</span> % (datetime.now(), i - num_steps_burn_in, duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_aquared += duration * duration</span><br><span class="line">    mn = total_duration / num_batches   <span class="comment"># 计算均值</span></span><br><span class="line">    vr = total_duration_aquared / num_batches - mn * mn  <span class="comment"># 计算方差</span></span><br><span class="line">    sd = math.sqrt(vr) <span class="comment"># 计算标准差</span></span><br><span class="line">    print(<span class="string">"%s : %s across %d steps, %.3f +/- %.3f sec/batch"</span> % (datetime.now(), info_string, num_batches, mn, sd))</span><br></pre></td></tr></table></figure></li><li><p>耗时测试</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">height, width = <span class="number">224</span>, <span class="number">224</span></span><br><span class="line">inputs = tf.random_uniform((batch_size, height, width, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope(resnet_arg_scope(is_training = <span class="literal">False</span>)):</span><br><span class="line">    net, end_points = resnet_v2_152(inputs, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    num_batches = <span class="number">100</span></span><br><span class="line">    time_tensorflow_run(sess, net, <span class="string">"Forward"</span>)</span><br></pre></td></tr></table></figure></li><li><p>测试结果</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2019-01-26 08:10:51.879413 : step 0, duration = 0.486</span><br><span class="line">2019-01-26 08:10:56.748640 : step 10, duration = 0.487</span><br><span class="line">2019-01-26 08:11:01.628659 : step 20, duration = 0.489</span><br><span class="line">2019-01-26 08:11:06.511324 : step 30, duration = 0.489</span><br><span class="line">2019-01-26 08:11:11.410210 : step 40, duration = 0.490</span><br><span class="line">2019-01-26 08:11:16.311633 : step 50, duration = 0.491</span><br><span class="line">2019-01-26 08:11:21.219118 : step 60, duration = 0.493</span><br><span class="line">2019-01-26 08:11:26.133231 : step 70, duration = 0.492</span><br><span class="line">2019-01-26 08:11:31.054586 : step 80, duration = 0.493</span><br><span class="line">2019-01-26 08:11:35.984226 : step 90, duration = 0.494</span><br><span class="line">2019-01-26 08:11:40.435636 : Forward across 100 steps, 0.490 +/- 0.002 sec/batch</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ResNet&quot;&gt;&lt;a href=&quot;#ResNet&quot; class=&quot;headerlink&quot; title=&quot;ResNet&quot;&gt;&lt;/a&gt;ResNet&lt;/h1&gt;&lt;p&gt;ResNet是由Kaiming He等4名华人提出，通过使用Residual Unit成功训练了152层的深度神经网络，在ILSVRC 2015比赛中获得冠军，取得了3.57%的top-5错误率，同时参数却比VGGNet少。ResNet的结构可以极快的加速超深神经网络的训练，模型的准确率也有较大的提升。之后很多方法都建立在ResNet的基础上完成的，例如检测，分割，识别等领域都纷纷使用ResNet。在ResNet推出不久，Google就借鉴了ResNet的精髓，提出了Inception V4和Inception-ResNet-V2，并通过融合这两个模型，在ILSVRC数据集上取得了惊人的3.08%的错误率。所以可见ResNet确实很好用。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="ResNet" scheme="http://zhengyujie.cn/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记10：Inception V3</title>
    <link href="http://zhengyujie.cn/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/"/>
    <id>http://zhengyujie.cn/2019/08/12/tf学习笔记10/</id>
    <published>2019-08-12T10:38:17.000Z</published>
    <updated>2019-08-19T08:15:22.885Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Inception-V3结构"><a href="#Inception-V3结构" class="headerlink" title="Inception V3结构"></a>Inception V3结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/OtRKyOfuqBS3.png?imageslim" alt="mark"></p><a id="more"></a><p>值得借鉴的设计CNN的思想和Trick：</p><ul><li>Factorization into small convolutions很有效，可以降低参数量，减轻过拟合，增加网络非线性的表达能力。</li><li>卷积网络从输入到输出，应该让图片尺寸逐渐减少，输出通道逐渐增加，即让空间结构简化，将空间信息转化为高阶抽象的特征信息。</li><li>Inception Module用多个分支提取不同抽象程度的高阶特征的思路很有效，可以丰富网络的表达能力。</li></ul><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><h2 id="定义函数-inception-v3-arg-scope"><a href="#定义函数-inception-v3-arg-scope" class="headerlink" title="定义函数 inception_v3_arg_scope"></a>定义函数 inception_v3_arg_scope</h2><p>函数 inception_v3_arg_scope 用来生成网络中经常用到的函数的默认参数，比如卷记的激活函数，权重初始化方式，标准化器等等。接下来嵌套一个<code>slim.arg_scope</code>，对卷积层生成函数<code>slim.conv2d</code>的几个人参数赋予默认值。最后返回定义好的scope。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"><span class="comment">#定义简单的函数产生截断的正态分布</span></span><br><span class="line">trunc_normal = <span class="keyword">lambda</span> stddev:tf.truncated_normal_initializer(<span class="number">0.0</span>,stddev)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义函数 inception_v3_arg_scope 用来生成网络中经常用到的函数的默认参数</span></span><br><span class="line"><span class="string">L2正则的Weight_decay默认值为0.0004</span></span><br><span class="line"><span class="string">标准差stddev默认值为0.1</span></span><br><span class="line"><span class="string">参数batch_norm_var_collection默认值为moving_vars</span></span><br><span class="line"><span class="string">batch normalization参数字典：</span></span><br><span class="line"><span class="string">    衰减系数decay为0.9997</span></span><br><span class="line"><span class="string">    epsilon为0.001</span></span><br><span class="line"><span class="string">    updates_collections为tf.GraphKeys.UPDATE_OPS</span></span><br><span class="line"><span class="string">    字典variables_collections:</span></span><br><span class="line"><span class="string">        beta和gamma设置为None</span></span><br><span class="line"><span class="string">        moving_mean和moving_variance设置为batch_norm_var_collection</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3_arg_scope</span><span class="params">(weight_decay=<span class="number">0.00004</span>,stddev=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        batch_norm_var_collection=<span class="string">"moving_vars"</span>)</span>:</span></span><br><span class="line">    batch_norm_params = &#123;</span><br><span class="line">        <span class="string">"decay"</span>:<span class="number">0.9997</span>,<span class="string">"epsilon"</span>:<span class="number">0.001</span>,<span class="string">"updates_collections"</span>:tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">        <span class="string">"variables_collections"</span>:&#123;</span><br><span class="line">            <span class="string">"beta"</span>:<span class="literal">None</span>,<span class="string">"gamma"</span>:<span class="literal">None</span>,<span class="string">"moving_mean"</span>:[batch_norm_var_collection],</span><br><span class="line">            <span class="string">"moving_variance"</span>:[batch_norm_var_collection]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    嵌套slim.arg_scope</span></span><br><span class="line"><span class="string">    权重初始化器weight_initializer设置为trunc_normal(stddev)</span></span><br><span class="line"><span class="string">    激活函数设置为ReLU</span></span><br><span class="line"><span class="string">    标准化器设置为slim.batch_norm</span></span><br><span class="line"><span class="string">    标准化器的参数设置为batch_norm_patams</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.fully_connected],</span><br><span class="line">                        weights_regularizer=slim.l2_regularizer(weight_decay)):</span><br><span class="line">        <span class="comment">#对卷积层生成函数的几个参数赋予默认值</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d],</span><br><span class="line">                            weights_regularizer = tf.truncated_normal_initializer(stddev=stddev),</span><br><span class="line">                            activation_fc = tf.nn.relu,</span><br><span class="line">                            normalizer_fc = slim.batch_norm,</span><br><span class="line">                            normalizer_params = batch_norm_params) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> scope</span><br></pre></td></tr></table></figure></p><h2 id="定义Inception-V3的卷积部分"><a href="#定义Inception-V3的卷积部分" class="headerlink" title="定义Inception V3的卷积部分"></a>定义Inception V3的卷积部分</h2><p>它可以生成Inception V3网络的卷积部分。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义Inception V3的卷积部分</span></span><br><span class="line"><span class="string">inputs为输入图片数据的tensor</span></span><br><span class="line"><span class="string">scope为包含了函数默认参数的环境</span></span><br><span class="line"><span class="string">用字典表end_points来保存某些关键节点供之后使用</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3_base</span><span class="params">(inputs,scope=None)</span>:</span></span><br><span class="line">    end_points = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">"InceptionV3"</span>,[inputs]):</span><br><span class="line">        <span class="comment">#对slim.conv2d,slim.max_pool2d,slim.avg_pool2d三个函数的参数设置默认值</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],stride = <span class="number">1</span>,padding = <span class="string">"VALID"</span>):                               </span><br><span class="line">            <span class="comment">#各参数分别为输入的tensor,输出的通道,卷积核尺寸,步长stride，padding模式</span></span><br><span class="line">            net = slim.conv2d(inputs,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_2a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],padding=<span class="string">"SAME"</span>,scope=<span class="string">"Conv2d_2b_3x3"</span>)</span><br><span class="line">            net = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"MaxPool_3a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">80</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_3b_1x1"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_4a_3x3"</span>)</span><br><span class="line">            net = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"MaxPool_5a_3x3"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="定义第一个Inception模块"><a href="#定义第一个Inception模块" class="headerlink" title="定义第一个Inception模块"></a>定义第一个Inception模块</h3><ul><li><p><strong>第一个Inception Module</strong><br>  第一个Inception Module名称为Mixed_5b。这个Inception Module中有4个分支：</p><ol><li>第一个分支有64输出通道的1×1卷积</li><li>第二个分支有48输出通道的1×1卷积，连接有64输出通道的5×5卷积</li><li>第三个分支有64输出通道的1×1卷积，再连续连接两个有96通道的3×3卷积</li><li><p>第四个分支有为3×3的平均池化，连接有32输出通道的1×1卷积</p><p>最后使用<code>tf.concat</code>将四个分支的输出合并在一起，生成这个Inception Module的最终输出。4个分支的输出通道数之和为64+64+96+32=256，即最终输出的图片尺寸为35×35×256。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义第一个Inception模块组</span></span><br><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],</span><br><span class="line">                    stride = <span class="number">1</span>,padding = <span class="string">"SAME"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5b"</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">            batch_0 = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">            batch_1 = slim.conv2d(net,num_outputs=<span class="number">48</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_1 = slim.conv2d(batch_1,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],scope=<span class="string">"Conv2d_0b_5x5"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">            batch_2 = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">96</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">96</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">            batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">            batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"> </span><br><span class="line">        net = tf.concat([batch_0,batch_1,batch_2,batch_3],<span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  第二个Inception Module的名称为Mixed_5c。它同样也有四个分支，唯一不同的是第四个分支最后接的是64输出通道的1×1卷积。因此我们输出的tensor的最终尺寸为35×5×288。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5c"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">48</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">5</span>, <span class="number">5</span>], scope=<span class="string">"Conv2d_0c_5x5"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">    </span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2, batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三个Inception Module</strong><br>  第三个Inception Module的名称为Mixed_5d，和上一个Inception Module完全相同。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5d"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">48</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">5</span>, <span class="number">5</span>], scope=<span class="string">"Conv2d_0c_5x5"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2, batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="定义第二个Inception模块组"><a href="#定义第二个Inception模块组" class="headerlink" title="定义第二个Inception模块组"></a>定义第二个Inception模块组</h3><p>第二个Inception模块组是一个非常大的模块组，包含了5个Inception Module。其中第二个到第五个Inception Module的结构非常相似。</p><ul><li><p><strong>第一个Inception Module</strong><br>  第一个Inception Module的名称为Mixed_6a，包含三个分支：</p><ol><li>第一个分支为384通道的3×3卷积，步长为2</li><li>第二个分支有三层，分别为64输出通道的1×1卷积，和两个96输出通道的3×3卷积，最后一层的步长为2。</li><li>第三个分支为3×3的池化层，步长为2<br>最后用<code>tf.concat</code>将三个分支在输出通道上合并，最后的输出尺寸为17×17×(384+96+256)=17×17×768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6a"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                                stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                    scope=<span class="string">"MaxPool_1a_3x3"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  名称为Mixed_6b，它有四个分支：</p><ol><li>第一个分支为193输出通道的1×1卷积</li><li>第二个分支有三个卷积层，分别为128输出通道的1×1卷积，128输出通道的1×7卷积，以及192输出通道的7×1卷积，这里用到了Factorization into small convolutions思想，串联的1×7卷积和7×1卷积相当于合成一个7×7卷积。大大减少了参数，减轻了过拟合，同事多了一个激活函数增加了非线性特征变换。</li><li>第三个分支有五个卷积层，分别为128输出通道的1×1卷积，128输出通道的7×1卷积，128输出通道的1×1卷积，128输出通道的1×7卷积，128输出通道的7×1卷积和192输出通道的1×7卷积</li><li>第四个分支为3×3的平均池化层，再连接192输出通道的1×1卷积。<br>最后四个分支合并，输出的tensor尺寸为17×17×(192+192+192+192)=17×17×768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6b"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第三个Inception Module</strong><br>  名称为Mixed_6c，和前一个Inception Module非常相似，唯一不同的地方就是第二和第三分支中前几个卷积层的输出通道从128变成了160。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第三个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6c"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第四个Inception Module</strong><br>  名称为Mixed_d，和Mixed_6c完全一致。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第四个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6d"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第五个Inception Module</strong><br>  名称为Mixed_6e，和前两个Inception Module也完全一致。这是第二个Inception模块组的最后一个Inception Module。我们将Mixed_6e存储于end_points中，作为Auxiliary Classifier辅助模型的分类。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第五个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6e"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br><span class="line"><span class="comment">#第二个模块组的最后一个Inception模块，将Mixed_6e存储于end_points中</span></span><br><span class="line">end_points[<span class="string">"Mixed_6e"</span>] = net</span><br></pre></td></tr></table></figure></li></ul><h3 id="定义第三个Inception模块组"><a href="#定义第三个Inception模块组" class="headerlink" title="定义第三个Inception模块组"></a>定义第三个Inception模块组</h3><p>第三个Inception模块组包含了3个Inception Module。其中后两个Inception Module的结构非常相似。</p><ul><li><p><strong>第一个Inception Module</strong><br>  名称为Mixed_7a，包含了三个分支：</p><ol><li>第一个分支为192输出通道的1×1卷积，再接320输出通道的3×3卷积，步长为2</li><li>第二个分支有四个卷积层，分别为192输出通道的1×1卷积，192输出通道的1×7卷积，192输出通道的7×1卷积和192输出通道的3×3卷积。最后一个卷积层步长为2，padding为VALID</li><li>第三个分支为一个3×3最大池化层，步长为2，padding为VALID。<br>最后合并得到的tensor尺寸为为3×3×(320+192+768)=8×8×1280。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第三个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7a"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>],stride=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.max_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                    scope=<span class="string">"MaxPool_1a_3x3"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  名称为Mixed_7b，它有四个分支：</p><ol><li>第一个分支为320输出通道的1×1卷积</li><li>第二个分支先是一个384输出通道的1×1卷积，随后在分支内开两个分支，分别为384输出通道的1×3卷积和384输出通道的3×1卷积，然后用<code>tf.concat</code>合并，得到的tensor的尺寸为8×8×(384+384)=8×8×768</li><li>第三个分支先是48输出通道的1×1卷积，然后是384输出通道的3×3卷积，然后同样在分支内拆成两个分支，分别为384输出通道的1×3卷积和384输出通道的3×1卷积</li><li>第四个分支为一个3×3的平均池化层后接一个192输出通道的1×1卷积。<br>最后合并得到的tensor的尺寸为8×8×(320+768+768+192)=8×8×2048<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第三个Inception模块组,第二个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7b"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = tf.concat([</span><br><span class="line">            slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_1x3"</span>),</span><br><span class="line">            slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net,num_outputs=<span class="number">448</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = tf.concat([</span><br><span class="line">            slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_1x3"</span>),</span><br><span class="line">            slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0d_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><em>第三个Inception Module*</em><br>  名称为Mixed_7c，它和前一个Inception Module完全一致。最后我们返回这个Inception Module的结果，作为inception_v3_base函数的最终输出</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment"># 定义第三个Inception模块组,第三个Inception模块</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7c"</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">            batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">            batch_1 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_1 = tf.concat([</span><br><span class="line">                slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_1x3"</span>),</span><br><span class="line">                slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">            batch_2 = slim.conv2d(net,num_outputs=<span class="number">448</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">            batch_2 = tf.concat([</span><br><span class="line">                slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_1x3"</span>),</span><br><span class="line">                slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0d_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">            batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">            batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"> </span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">return</span> net,end_points</span><br></pre></td></tr></table></figure></li></ul><h2 id="实现Inception-v3函数"><a href="#实现Inception-v3函数" class="headerlink" title="实现Inception_v3函数"></a>实现Inception_v3函数</h2><p>实现Inception V3网络的最后一部分——全局平均池化，Softmax和Auxiliary Logits。</p><ul><li><p><strong>函数Inception_v3的输入参数</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3</span><span class="params">(inputs,num_classes=<span class="number">1000</span>,is_training=True,droupot_keep_prob = <span class="number">0.8</span>,prediction_fn = slim.softmax,spatial_squeeze = True,reuse = None, scope=<span class="string">"InceptionV3"</span>)</span>:</span> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">InceptionV3整个网络的构建</span></span><br><span class="line"><span class="string">param :</span></span><br><span class="line"><span class="string">inputs -- 输入tensor</span></span><br><span class="line"><span class="string">num_classes -- 最后分类数目</span></span><br><span class="line"><span class="string">is_training -- 是否是训练过程</span></span><br><span class="line"><span class="string">droupot_keep_prob -- dropout保留节点比例</span></span><br><span class="line"><span class="string">prediction_fn -- 最后分类函数，默认为softmax</span></span><br><span class="line"><span class="string">patial_squeeze -- 是否对输出去除维度为1的维度</span></span><br><span class="line"><span class="string">reuse -- 是否对网络和Variable重复使用</span></span><br><span class="line"><span class="string">scope -- 函数默认参数环境</span></span><br><span class="line"><span class="string">return:</span></span><br><span class="line"><span class="string">logits -- 最后输出结果</span></span><br><span class="line"><span class="string">end_points -- 包含辅助节点的重要节点字典表</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Auxiliary Logits部分的逻辑</strong><br>  Auxiliary Logits作为辅助分类的节点，对分类结果预测有很大的帮助。通过end_points得到Mixed_6e后：</p><ul><li>连接一个5×5的平均池化，步长为3，padding设为VALID，这样输出的尺寸就从17×17×768变成5×5×768。</li><li>接着连接一个128输出通道的1×1卷积和一个768输出通道的5×5卷积，这里权重初始化方式重设为标准差为0.01的正态分布，padding设置为VALID，输出尺寸变为1×1×768</li><li>然后再接一个输出通道为num_classes的1×1卷积，不设激活函数和规范化函数权重初始方式重设为标准差为0.001的正态分布，这样输出就变成了1×1×1000</li><li>最后使用<code>tf.squeeze</code>函数消除输出tensor中前两个为1的维度。将辅助分类节点的输出aux_logits储存到字典表end_points中。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">aux_logits = end_points[<span class="string">"Mixed_6e"</span>]</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"AuxLogits"</span>):</span><br><span class="line">    aux_logits = slim.avg_pool2d(aux_logits,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],stride=<span class="number">3</span>,</span><br><span class="line">                                    padding=<span class="string">"VALID"</span>,scope=<span class="string">"Avgpool_1a_5x5"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">128</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">768</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],</span><br><span class="line">                                weights_initializer=trunc_normal(<span class="number">0.01</span>),padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                scope=<span class="string">"Conv2d_2a_5x5"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                                activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                weights_initializer=trunc_normal(<span class="number">0.001</span>),scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">    <span class="comment">#消除tensor中前两个维度为1的维度</span></span><br><span class="line">    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">        aux_logits = tf.squeeze(aux_logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将辅助节点分类的输出aux_logits存到end_points中</span></span><br><span class="line">    end_points[<span class="string">"AuxLogits"</span>] = aux_logits</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>分类预测部分逻辑</strong><br>  得到Mixed_7e即最后一个卷积层的输出后：</p><ul><li>接一个8×8全局平均池化，padding设置为VALID，tensor的尺寸就变成了1×1×2048</li><li>然后接一个Dropout层，节点保留率为dropout_keep_prob</li><li>接着连接一个输出通道为1000的1×1卷积，激活函数和规范化函数设为空</li><li>然后用<code>tf.squeeze</code>去除输出tensor中维数为1的维度</li><li>最后连接一个Softmax对结果进行分类预测，输出的结果存储到end_points中<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正常分类预测</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Logits"</span>):</span><br><span class="line">    net = slim.avg_pool2d(net,kernel_size=[<span class="number">8</span>,<span class="number">8</span>],padding=<span class="string">"VALID"</span>,</span><br><span class="line">                            scope=<span class="string">"Avgpool_1a_8x8"</span>)</span><br><span class="line">    net = slim.dropout(net,keep_prob=droupot_keep_prob,scope=<span class="string">"Dropout_1b"</span>)</span><br><span class="line">    end_points[<span class="string">"Logits"</span>] = net</span><br><span class="line"></span><br><span class="line">    logits = slim.conv2d(net,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,</span><br><span class="line">                            normalizer_fn=<span class="literal">None</span>,scope=<span class="string">"Conv2d_1c_1x1"</span>)</span><br><span class="line">    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">        logits = tf.squeeze(logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line"></span><br><span class="line">end_points[<span class="string">"Logits"</span>] = logits</span><br><span class="line">end_points[<span class="string">"Predictions"</span>] = prediction_fn(logits,scope=<span class="string">"Predictions"</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>inception_v3函数实现代码汇总：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3</span><span class="params">(inputs,num_classes=<span class="number">1000</span>,is_training=True,droupot_keep_prob = <span class="number">0.8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 prediction_fn = slim.softmax,spatial_squeeze = True,reuse = None,scope=<span class="string">"InceptionV3"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    InceptionV3整个网络的构建</span></span><br><span class="line"><span class="string">    param :</span></span><br><span class="line"><span class="string">    inputs -- 输入tensor</span></span><br><span class="line"><span class="string">    num_classes -- 最后分类数目</span></span><br><span class="line"><span class="string">    is_training -- 是否是训练过程</span></span><br><span class="line"><span class="string">    droupot_keep_prob -- dropout保留节点比例</span></span><br><span class="line"><span class="string">    prediction_fn -- 最后分类函数，默认为softmax</span></span><br><span class="line"><span class="string">    patial_squeeze -- 是否对输出去除维度为1的维度</span></span><br><span class="line"><span class="string">    reuse -- 是否对网络和Variable重复使用</span></span><br><span class="line"><span class="string">    scope -- 函数默认参数环境</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">    logits -- 最后输出结果</span></span><br><span class="line"><span class="string">    end_points -- 包含辅助节点的重要节点字典表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">"InceptionV3"</span>,[inputs,num_classes],</span><br><span class="line">                           reuse=reuse) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm,slim.dropout],</span><br><span class="line">                            is_training = is_training):</span><br><span class="line">            net,end_points = inception_v3_base(inputs,scope=scope)     <span class="comment">#前面定义的整个卷积网络部分</span></span><br><span class="line"> </span><br><span class="line">            <span class="comment">#辅助分类节点部分</span></span><br><span class="line">            <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],</span><br><span class="line">                                stride = <span class="number">1</span>,padding = <span class="string">"SAME"</span>):</span><br><span class="line">                <span class="comment">#通过end_points取到Mixed_6e</span></span><br><span class="line">                aux_logits = end_points[<span class="string">"Mixed_6e"</span>]</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">"AuxLogits"</span>):</span><br><span class="line">                    aux_logits = slim.avg_pool2d(aux_logits,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],stride=<span class="number">3</span>,</span><br><span class="line">                                                 padding=<span class="string">"VALID"</span>,scope=<span class="string">"Avgpool_1a_5x5"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">128</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">768</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],</span><br><span class="line">                                             weights_initializer=trunc_normal(<span class="number">0.01</span>),padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                             scope=<span class="string">"Conv2d_2a_5x5"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                                             activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                             weights_initializer=trunc_normal(<span class="number">0.001</span>),scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">                    <span class="comment">#消除tensor中前两个维度为1的维度</span></span><br><span class="line">                    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                        aux_logits = tf.squeeze(aux_logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line"> </span><br><span class="line">                    end_points[<span class="string">"AuxLogits"</span>] = aux_logits    <span class="comment">#将辅助节点分类的输出aux_logits存到end_points中</span></span><br><span class="line"> </span><br><span class="line">                <span class="comment">#正常分类预测</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">"Logits"</span>):</span><br><span class="line">                    net = slim.avg_pool2d(net,kernel_size=[<span class="number">8</span>,<span class="number">8</span>],padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                          scope=<span class="string">"Avgpool_1a_8x8"</span>)</span><br><span class="line">                    net = slim.dropout(net,keep_prob=droupot_keep_prob,scope=<span class="string">"Dropout_1b"</span>)</span><br><span class="line">                    end_points[<span class="string">"Logits"</span>] = net</span><br><span class="line"> </span><br><span class="line">                    logits = slim.conv2d(net,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,</span><br><span class="line">                                         normalizer_fn=<span class="literal">None</span>,scope=<span class="string">"Conv2d_1c_1x1"</span>)</span><br><span class="line">                    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                        logits = tf.squeeze(logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line">                </span><br><span class="line">                end_points[<span class="string">"Logits"</span>] = logits</span><br><span class="line">                end_points[<span class="string">"Predictions"</span>] = prediction_fn(logits,scope=<span class="string">"Predictions"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> logits,end_points</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Inception-V3结构&quot;&gt;&lt;a href=&quot;#Inception-V3结构&quot; class=&quot;headerlink&quot; title=&quot;Inception V3结构&quot;&gt;&lt;/a&gt;Inception V3结构&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://pwbhioup3.bkt.clouddn.com/blog/20190816/OtRKyOfuqBS3.png?imageslim&quot; alt=&quot;mark&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="InceptionNet" scheme="http://zhengyujie.cn/tags/InceptionNet/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记10：FCOS:Fully Convolutional One-Stage Object Detection</title>
    <link href="http://zhengyujie.cn/2019/08/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B010/"/>
    <id>http://zhengyujie.cn/2019/08/09/论文笔记10/</id>
    <published>2019-08-09T02:21:15.000Z</published>
    <updated>2019-08-19T08:10:23.389Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a><br>代码地址：<a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">https://github.com/tianzhi0549/FCOS</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutional One Stage）, 它以对每个像素进行预测的方式来解决目标检测问题，类似于语义分割。几乎所有的 SOTA 物体检测器，如 RetinaNet，SSD，YOLOv3 和 Faster R-CNN 都依赖于预定义的 anchor box。相比之下，我们提出的 FCOS 不需要 anchor box，同时也不需要 proposals (即 One-Stage)。通过消除对预定义 anchor 的依赖，FCOS 完全避免了与 anchor box 相关的复杂计算，例如在训练期间计算 overlapping 并显着减少 training memory footprint。更重要的是，我们还避免了与 anchor 相关的所有超参数，这些参数通常对最终检测性能非常敏感。凭借唯一的后处理操作非最大抑制（NMS），我们的 FCOS 优于之前的 anchor-based one-stage detectors，并且结构更简单。我们首次展示了一种更加简单灵活的检测框架，可以提高检测精度。我们希望 FCOS 框架可以作为许多其他实例级任务简单而强大的替代方案。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Anchor-based检测器存在的缺点：</p><ul><li>如 Faster R-CNN 和 Focal Loss 所示，检测性能对于尺寸，宽高比和 anchor 数量非常敏感。 例如，在RetinaNet 中，根据 COCO 的 benchmark 上，仅仅改变这些超参数就会影响AP的性能提升4％[13]。 因此，对于 anchor-based 检测器需要仔细调整这些超参数。</li><li>即使经过精心设计，由于 anchor box 的比例和宽高比保持固定，detectors 在处理具有较大形状变化的物体集合时会遇到困难，特别是对于小物体。 预定义的 anchor box 也妨碍了探测器的泛化能力，因为它们需要在具有不同物体尺寸或宽高比的新探测任务上进行重新设定。</li><li>为了实现高召回率，anchor-based 检测器需要将 anchor box 密集地放置在输入图像上（例如，在特征金字塔网络(FPN)中, 对于短边像素为 800 的输入图像, 会产生超过 180K 的 anchor box）。 大多数这些 anchor box 在训练期间会被标记为 negative samples。 过多的 negative samples 加剧了 training 过程中正负样本之间的不平衡性。</li><li>当在训练期间计算所有 anchor box 和 GT box 之间的 IOU 时，过多数量的 anchor box 也显著增加了计算量和存储器占用量。</li></ul><p>基于 anchor-based 的检测方法偏离全卷积预测的框架，而本文尝试类似于语义分割的像素级预测应用至目标检测任务中。因此，目标检测，语义分割，关键点检测等几种任务可以糅合到一个模型中。一些工作如 Dense-Box，UnitBox 曾利用基于 FCN-based 的框架进行目标检测。但这些方法在某一层的 feature map 上直接预测4D坐标及类别，如下图左侧所示，4D向量表示了边框的边距离该像素点的偏移。这些方法与语义分割的全卷积相类似，但每个位置需要回归一个连续的4D向量。为了处理不同大小的边框，DenseBox 将训练图片调整为固定尺寸。DenseBox 必须要在图像金字塔上进行检测，违反了全卷积对所有卷积只计算一次的思想。而且，这些方法大多应用在目标检测的特殊场景中，比如文本检测或者人脸检测。如下图右侧所示，较多的重叠框造成了模糊，无法确定重叠区域应该对哪个框进行回归。本文证明通过FPN结构可以消除这种模糊。本文发现在距离目标中心较远的位置会预测一定数量低质量的边界框。为了打压这些边框，本文设计了一个新的分支”center-ness”，用于预测一个像素与对应边框中心的偏差。所得的分数用于 down-weight 低质量的检测框，最后通过NMS将检测结果进行融合。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/IgTSrBOLa2Sx.png?imageslim" alt="mark"></p><p>这个新的检测框架拥有以下优点：</p><ul><li>检测任务现在与许多其他 FCN 可解决的任务（例如语义分割）相统一，从而可以更轻松地重复使用这些任务中的想法。</li><li>检测变为 proposal free 和 anchor free，这显著减少了超参数的数量。 超参数通常需要启发式调整，并且涉及许多技巧才能获得良好的性能。 而我们的新检测框架使检测器，特别是使它的 training 阶段变得相当简单。 此外，通过消除 anchor box，我们的新探测器完全避免了复杂的 IOU 计算以及训练期间 anchor box 和 GT box 之间的匹配，并将总的训练内存占用(training memory footprint)减少了2倍左右。</li><li>我们在 One-Stage Detectors 中实现了 SOTA 的结果。 我们的实验还表明，本文所提出的 FCOS 可以用作 Two-Stage Detectors 中的 RPN，并且可以实现比基于 anchor 的 RPN 更好的性能。 鉴于更简单的 anchor free Detectors 具有更好的性能，我们鼓励大家重新考虑物体检测中 anchor 的必要性，虽然目前这被认为是检测任务的事实标准(defacto standard for detection)。</li><li>我们所提出的 detector 只需做很小的修改就可以立即扩展到其他视觉任务，包括实例分割和关键点检测。 我们相信这种新方法可以成为许多实例级预测问题的新 baseline。</li></ul><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><p><strong>Anchor-based Detectors</strong></p></li><li><p><strong>Anchor-free Detectors</strong></p></li></ul><h1 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h1><p>在本节中，我们首先以逐像素预测的方式重新构造目标检测任务。 接下来，我们展示了我们如何利用多级预测(multi-level prediction)来改善召回率并解决训练中重叠边界框导致的模糊性。 最后，我们展示了我们提出的 “center-ness” 分支，它有助于抑制低质量的检测边界框并大幅提高整体性能.</p><h2 id="Fully-Convolutional-One-Stage-Object-Detector"><a href="#Fully-Convolutional-One-Stage-Object-Detector" class="headerlink" title="Fully Convolutional One-Stage Object Detector"></a>Fully Convolutional One-Stage Object Detector</h2><p>设$F_i \in R^{H×W×C}$是 backbone CNN 第 $i$ 层的 feature maps，$s$ 为该层之前的总步长。输入图片的真实框定义为$\lbrace B_i \rbrace$，$B_i=(x_0^{(i)},y_0^{(i)},x_1^{(i)},y_1^{(i)},c^{(i)})\in R^4×\lbrace1,2…C\rbrace$。其中$(x_0^{(i)},y_0^{(i)})$和$(x_1^{(i)},y_1^{(i)})$分别为边界框左上角点和右下角点的坐标，$c^{(i)}$为边界框内的object的类别。</p><p>对于 feature maps $F_i$ 上的的每一个位置$(x,y)$，我们都可以将其映射回输入图像的坐标$(\lfloor\frac{s}{2}\rfloor+xs,\lfloor\frac{s}{2}\rfloor+ys)$，它差不多刚好位于位置$(x,y)$的感受野中心附近。与 anchor based detectors 将输入图像上的位置视为 anchor box 的中心并对这些 anchor box 的目标边界框进行回归不同，我们直接回归每个位置的目标边界框。 换句话说，我们的 Detector 直接将 location 视为训练样本而不是将 anchor box 视为训练样本，这与用于语义分割的FCN相同。</p><p>具体而言，如果位置$(x,y)$落入到任何 GT Box 内部, 那么久将其视为正样本, 并且该位置的类标签$c^\star$就是$B_i$的类标签，否则它就是负样本并且$c^\star=0$（背景类）。除了用于分类的标签之外，我们还有一个 4D 的实数向量$t^\star=(l^\star,t^\star,r^\star,b^\star)$, 该向量是每个样本的回归目标。这里$l^\star,t^\star,r^\star,b^\star$是从 location 到 bbox 四条边的距离，如图1（左）所示。如果某个位置属于多个边界框，则会将其视为模糊样本。现在，我们只选择具有最小面积的边界框作为其回归目标(最简单的策略)。 在下一节中，我们将 展示通过多级预测，可以显著减少模糊样本的数量。 形式上，如果位置$(x,y)$与边界框$B_i$相关联，则该位置的训练回归目标可以表示为:</p><script type="math/tex; mode=display">\left\{  \begin{array}{lr}    l^\star=x-x_0^{(i)}, & t^\star=y-Y_0^{(i)} \\    r^\star=x_1^{(i)}-x, & b^\star=y_1^{(i)}-y  \end{array}\right.</script><p>值得注意的是，FCOS 可以利用尽可能多的前景样本来训练回归量。(GT box 内的每个像素点都是正样本) 它与基于 anchor 的探测器不同，anchor-based detectors 仅仅将与 GT box 具有足够 IOU 的anchor box 作为正样本。我们认为，这可能是 FCOS 优于 anchor-based 的原因之一。</p><p><strong>Network Outputs</strong><br>对应于 training targets，我们网络的最后一层会预测用于分类的 80D 向量$\vec{p}$和 bounding box 坐标 4D 向量$\vec{t}=(l，t，r，b)$。跟随 R-CNN 的做法，我们不是训练多类分类器，而是训练 C 个二元分类器。与 R-CNN 类似，我们在 backbone 网络的特征图谱之后分别为分类和回归分支添加了 四个卷积层。此外，由于回归目标总是正的，我们使用$exp(x)$将任意的实数都映射到回归分支顶部的$(0,\infty)$。值得注意的是，FCOS 的网络输出变量比常用的 anchor based detectors 少 9 倍，其中每个位置有 9 个 anchor boxes。</p><p><strong>Loss Function</strong><br>我们定义我们的训练损失函数如下：</p><script type="math/tex; mode=display">L(\lbrace p_{x,y}\rbrace,\lbrace t_{x,y}\rbrace)=\frac{1}{N_{pos}}\sum_{x,y}L_cls(p_{x,y},c_{x,y}^\star)+\frac{\lambda}{N_pos}\sum_{x,y}I_{\lbrace c_{x,y}^\star >0\rbrace}L_{reg}(t_{x,y},t_{x,y}^\star)</script><p>其中$L_{cls}$为 focal loss，$L_{reg}$为 IOU loss。$N_{pos}$为正样本数，$\lambda$在本文中为1来平衡$L_{reg}$的权重。求和是通过对feature maps $F_i$ 上的所有点进行计算得到的。$I_{\lbrace c_{x,y}^\star &gt;0\rbrace}$是一个indicator function，当$c_i^\star&gt;0$时为1，否则为0。</p><p><strong>Inference</strong><br>FCOS 的 Inference 很简单。给定输入图像，我们将其放入网络进行一次 forward 计算, 并获得 feature map $F_i$ 上的每个位置的分类分数$p_{x,y}$和回归预测值$t_{x,y}$。 跟随 R-CNN 的设定，我们选择$p_{x,y}&gt;0.0$的位置作为正样本并通过上述公式来获得预测的边界框。</p><h2 id="Multi-level-Prediction-with-FPN-for-FCOS"><a href="#Multi-level-Prediction-with-FPN-for-FCOS" class="headerlink" title="Multi-level Prediction with FPN for FCOS"></a>Multi-level Prediction with FPN for FCOS</h2><p>在这里，我们展示了如何通过 FPN 的多级预测来解决所提出的 FCOS 存在的两个可能问题。</p><ol><li>CNN 中最后的 feature maps 的大步幅（例如，16）可能回导致相对较低的 best possible recall (BPR)。对于基于 anchor 的检测器，由于大步幅导致的低召回率可以通过降低 positive anchor boxes 所需的 IOU 分数来在一定程度上得到缓解。而对于 FCOS，乍一看可能认为其 BPR 会远低于基于 anchor 的检测器，因为 网络无法召回由于大步幅而最终在 feature map 上没有位置编码的对象。在这里，我们凭经验证明，即使步幅很大，基于 FCN 的 FCOS 仍然能够产生良好的BPR，它甚至可以比官方实现的 Detectron 中基于 anchor 的检测器 RetinaNet 的 BPR 更好。（参见表1）。因此，BPR 实际上不是 FCOS 无法解决的问题。此外，利用多级 FPN 预测，可以进一步改进 BPR 以匹配基于 anchor 的 RetinaNet 最佳BPR。</li><li>与 GT box 的多个重叠会导致在训练期间产生难以理解的模糊性，即哪个边界框应该在重叠位置进行回归？这种模糊性导致基于 FCN 的检测器的性能下降。在本文中，我们表明，使用多级预测可以极大地解决模糊性，并且与基于 anchor 的检测器相比，基于 FCN 的检测器可以获得相同的，有时甚至更好的性能。</li></ol><p>我们在不同级别的特征图上检测到不同大小的对象。具体来说，我们使用定义为 $\lbrace P_3,P_4,P_5,P_6,P_7\rbrace$ 的五个级别的 feature map。$P_3$，$P_4$ 和 $P_5$ 由 backbone CNN 的特征图 $C_3$，$C_4$ 和 $C_5$ 和具有横向连接的1×1卷积层产生，如下图所示. $P_6$ 和 $P_7$ 通过分别在 $P_5$ 和 $P_6$ 上使用一个步长为 2 的卷积层产生. 最终，特征层级 $P_3$，$P_4$，$P_5$，$P_6$ 和 $P_7$ 具有的步幅分别为 8,16,32,64和128。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/sNtrzUOWkbhi.png?imageslim" alt="mark"></p><p>与 anchor based detectors 将不同大小的 anchor 分配给不同的特征级别不一样的是，我们直接限制边界框回归的范围。 更具体地说，我们首先计算所有特征级别上每个位置的回归目标$l^\star$，$t^\star$，$r^\star$和$b^\star$。如果位置满足 $max(l^\star,t^\star,r^\star,b^\star)&gt; m_i$或$max(l^\star,t^\star,r^\star,b^\star)&lt; m_{i-1}$，我们就将其设置为负样本并且再也不会对该位置进行回归操作。这里的 $m_i$ 是特征层级 $i$ 需要回归的最大距离。在本文中，$m_2$，$m_3$，$m_4$，$m_5$，$m_6$和$m_7$分别设置为0,64,128,256,512和$\infty$。 由于 具有不同大小的对象被分配给不同的特征级别(这里是与 FoveaBox 的一处重要区别) 并且 大多数重叠发生在具有显著不同大小的对象之间，因此多级预测可以在很大程度上减轻上述模糊性并且将基于 FCN 的检测器提升到与基于 anchor 的检测器相同的检测性能，如我们后面的实验所示。</p><p>最后，跟随 R-CNN 和 Fast R-CNN 的设定，我们 共享不同特征级别之间的头部(这是与其他 Detector 的不同之处, 其他的都是每个特征层级独立的执行分类和回归)，这样不仅使检测器参数高效，而且能够提高检测性能。 然而，我们观察到不同的特征水平需要回归不同的尺寸范围（例如，$P_3$ 的尺寸范围是 [0,64] 而 $P_4$ 的尺寸范围是 [64,128]），因此 在不同的特征层使用相同的回归 heads 是不合理的。 故此, 我们不使用标准的 $exp(x)$，而是使用带有可训练标量 $s_i% 的 $exp(s_i x)$ 来自动调整特征级 $P_i$ 的指数函数的基数，从而凭经验提高检测性能。</p><h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p>在 FCOS 中使用多级预测后，FCOS 和 anchor based 的检测器之间仍存在性能差距。 我们观察到这是由于远离物体中心的位置产生的许多低质量预测边界框造成的。</p><p>我们提出了一种简单而有效的策略来抑制这些低质量的检测边界框而不引入任何超参数。 具体来说，我们添加一个单层分支，与分类分支并行，以预测一个位置的“中心概率(center-ness)”（即，从该位置到该位置所负责的对象的中心的距离）如图2所示， 给定位置的回归目标$l^\star$，$t^\star$，$r^\star$和$b^\star$，center-ness target 定义为</p><script type="math/tex; mode=display">centerness^\star = \sqrt{\frac{min(l^\star,r^\star)}{max(l^\star,r^\star)}×\frac{min(t^\star,b^\star)}{max(t^\star,b^\star)}}</script><p>我们在这里使用 sqrt 来减缓中心的衰减。center-ness 从0到1，因此用 二元交叉熵（BCE）损失训练。 损失被添加到上述损失函数公式中。 在测试时，通过将预测的 center-ness 与相应的分类得分相乘来计算最终得分（用于对检测到的边界框进行排名）。因此，center-ness 可以使远离物体中心的边界框的 scores 减小。结果，这些低质量的边界框很可能被最终的非最大抑制（NMS）过程滤除，从而显著提高了检测性能。</p><p>基于 anchor 的检测器使用两个 IOU 阈值 $T_{low}$ 和 $T_{high}$ 将 anchor boxes 标记为负、忽略和正样本，center-ness 可以看作是一个 软阈值。它是在网络训练中学习的，不需要调整。此外，利用该策略，我们的检测器仍然可以将任何落在 GT Box 中的位置视为正样本，除了上述多层预测中设置为负样本的位置外，这样就可以为回归器使用尽可能多的训练样本。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>不再叙述</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><h3 id="Multi-level-Prediction-with-FPN"><a href="#Multi-level-Prediction-with-FPN" class="headerlink" title="Multi-level Prediction with FPN"></a>Multi-level Prediction with FPN</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/YBbIVpisIPb8.png?imageslim" alt="mark"></p><h3 id="With-or-Without-Center-ness"><a href="#With-or-Without-Center-ness" class="headerlink" title="With or Without Center-ness"></a>With or Without Center-ness</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/qrjP0Ygn5DxK.png?imageslim" alt="mark"></p><h3 id="FCOS-vs-Anchor-based-Detectors"><a href="#FCOS-vs-Anchor-based-Detectors" class="headerlink" title="FCOS vs. Anchor-based Detectors"></a>FCOS vs. Anchor-based Detectors</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/bXD3bdM9o5df.png?imageslim" alt="mark"></p><h2 id="Comparison-with-State-of-the-art-Detectors"><a href="#Comparison-with-State-of-the-art-Detectors" class="headerlink" title="Comparison with State-of-the-art Detectors"></a>Comparison with State-of-the-art Detectors</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/s4EFKSNIFo6K.png?imageslim" alt="mark"></p><h1 id="Extensions-on-Region-Proposal-Networks"><a href="#Extensions-on-Region-Proposal-Networks" class="headerlink" title="Extensions on Region Proposal Networks"></a>Extensions on Region Proposal Networks</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/2vzzVPyj2UrS.png?imageslim" alt="mark"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种 anchor freee, proposal free 的单级探测器FCOS。 如实验所示，FCOS与流行的 anchor based 的一级探测器相比更具有优势，包括RetinaNet，YOLO和SSD，但设计复杂性要低得多。 FCOS完全避免了与 anchor 相关的所有计算和超参数，并以每像素预测方式解决了对象检测，类似于其他密集预测任务，例如语义分割。 FCOS还在一级探测器中实现了最先进的性能。 我们还表明，FCOS可用作两级探测器中的RPN，速度更快的R-CNN，并且大幅优于其RPN。 鉴于其有效性和效率，我们希望FCOS可以作为当前主流锚点探测器的强大而简单的替代方案。 我们还相信FCOS可以扩展到解决许多其他实例级识别任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1904.01355.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1904.01355.pdf&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/tianzhi0549/FCOS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tianzhi0549/FCOS&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutional One Stage）, 它以对每个像素进行预测的方式来解决目标检测问题，类似于语义分割。几乎所有的 SOTA 物体检测器，如 RetinaNet，SSD，YOLOv3 和 Faster R-CNN 都依赖于预定义的 anchor box。相比之下，我们提出的 FCOS 不需要 anchor box，同时也不需要 proposals (即 One-Stage)。通过消除对预定义 anchor 的依赖，FCOS 完全避免了与 anchor box 相关的复杂计算，例如在训练期间计算 overlapping 并显着减少 training memory footprint。更重要的是，我们还避免了与 anchor 相关的所有超参数，这些参数通常对最终检测性能非常敏感。凭借唯一的后处理操作非最大抑制（NMS），我们的 FCOS 优于之前的 anchor-based one-stage detectors，并且结构更简单。我们首次展示了一种更加简单灵活的检测框架，可以提高检测精度。我们希望 FCOS 框架可以作为许多其他实例级任务简单而强大的替代方案。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="目标检测" scheme="http://zhengyujie.cn/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="anchor-free" scheme="http://zhengyujie.cn/tags/anchor-free/"/>
    
      <category term="论文笔记" scheme="http://zhengyujie.cn/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="关键点检测" scheme="http://zhengyujie.cn/tags/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow-Slim简介</title>
    <link href="http://zhengyujie.cn/2019/08/08/tensorflow-slim/"/>
    <id>http://zhengyujie.cn/2019/08/08/tensorflow-slim/</id>
    <published>2019-08-08T12:25:20.000Z</published>
    <updated>2019-08-20T05:11:06.583Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Slim简介"><a href="#Slim简介" class="headerlink" title="Slim简介"></a>Slim简介</h1><p>slim是一个使构建，训练，评估神经网络变得简单的库。它可以消除原生tensorflow里面很多重复的模板性的代码，让代码更紧凑，更具备可读性。另外slim提供了很多计算机视觉方面的著名模型（VGG, AlexNet等），我们不仅可以直接使用，甚至能以各种方式进行扩展。</p><a id="more"></a><ul><li>导入Slim<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></table></figure></li></ul><h1 id="TF-Slim的优点"><a href="#TF-Slim的优点" class="headerlink" title="TF-Slim的优点"></a>TF-Slim的优点</h1><ul><li>允许用户通过减少模板代码使得模型更加简洁。这个可以通过使用argument scoping和大量的高层layers、variables来实现</li><li>通过使用常用的正则化（ regularizers）使得建立模型更加简单；</li><li>一些广泛使用的计算机视觉相关的模型（比如VGG，AlexNet）已经在slim中定义好了，用户可以很方便的使用；这些既可以当成黑盒使用，也可以被扩展使用，比如添加一些“multiple heads”到不同的内部的层；</li><li>Slim使得扩展复杂模型变得容易，可以使用已经存在的模型的checkpoints来开始训练算法。</li></ul><h1 id="TF-Slim的主要组件"><a href="#TF-Slim的主要组件" class="headerlink" title="TF-Slim的主要组件"></a>TF-Slim的主要组件</h1><p>TF-Slim由几个独立存在的组件组成，主要包括以下几个：</p><ul><li><p><strong>arg_scope</strong><br>  提供一个新的作用域（scope），称为arg_scope，在该作用域（scope）中，用户可以定义一些默认的参 数，用于特定的操作；<br>  如果你的网络中有大量的相同的参数：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure><p>  用arg_scope处理一下：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d], padding=<span class="string">'SAME'</span>,</span><br><span class="line">                  weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">                  weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv2'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure><p>  arg_scope在作用范围内定义了指定层的默认参数，若想特别指定某些层的参数，可以重新赋值（相当于重写）</p></li><li><p><strong>data</strong><br>  包含TF-Slim的dataset定义，data providers，parallel_reader，和 decoding utilities；</p></li><li><strong>evaluation</strong><br>  包含用于模型评估的常规函数；</li><li><p><strong>layers</strong><br>  包含用于建立模型的高级layers；<br>  比如分别用TensorFlow和Slim实现一个卷积层的案例：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensorflow实现卷积层</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32,</span><br><span class="line">                                       stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32),</span><br><span class="line">                   trainable=<span class="literal">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv1 = tf.nn.relu(bias, name=scope)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#slim实现卷积层</span></span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure><p>  另外，比较吸引人的是slim中的repeat和stack操作，假设定义三个相同的卷积层，</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure><p>  在slim中的repeat操作可减少代码量：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure><p>  stack是处理卷积核或者输出不一样的情况：假设定义三层FC：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">'fc/fc_1'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">'fc/fc_2'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">'fc/fc_3'</span>)</span><br></pre></td></tr></table></figure><p>  使用stack操作：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure><p>  卷积层使用stack操作：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通方法:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_1'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_2'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_3'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_4'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简便方法:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">'core'</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>learning</strong><br>  包含一些用于训练模型的常规函数；</p></li><li><strong>losses</strong><br>  包含一些用于loss function的函数；</li><li><strong>metrics</strong><br>  包含一些热门的评价标准；</li><li><strong>nets</strong><br>  包含一些热门的网络定义，如VGG，AlexNet等模型；</li><li><strong>queues</strong><br>  提供一个内容管理者，使得可以很容易、很安全地启动和关闭QueueRunners；</li><li><strong>regularizers</strong><br>  包含权重正则化；</li><li><strong>variables</strong><br>  提供一个方便的封装，用于变量创建和使用。<br>  变量分为两类：模型变量和局部变量。局部变量是不作为模型参数保存的，而模型变量会再save的时候保存下来。诸如global_step之类的就是局部变量。slim中可以写明变量存放的设备，正则和初始化规则。还有获取变量的函数也需要注意一下，get_variables是返回所有的变量。<br>  slim中定义一个变量的实例：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Variables</span></span><br><span class="line">weights = slim.model_variable(<span class="string">'weights'</span>,</span><br><span class="line">                            shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span> , <span class="number">3</span>],</span><br><span class="line">                            initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">                            regularizer=slim.l2_regularizer(<span class="number">0.05</span>),</span><br><span class="line">                            device=<span class="string">'/CPU:0'</span>)</span><br><span class="line">model_variables = slim.get_model_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Regular variables</span></span><br><span class="line">my_var = slim.variable(<span class="string">'my_var'</span>,</span><br><span class="line">                    shape=[<span class="number">20</span>, <span class="number">1</span>],</span><br><span class="line">                    initializer=tf.zeros_initializer())</span><br><span class="line">regular_variables_and_model_variables = slim.get_variables()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Slim简介&quot;&gt;&lt;a href=&quot;#Slim简介&quot; class=&quot;headerlink&quot; title=&quot;Slim简介&quot;&gt;&lt;/a&gt;Slim简介&lt;/h1&gt;&lt;p&gt;slim是一个使构建，训练，评估神经网络变得简单的库。它可以消除原生tensorflow里面很多重复的模板性的代码，让代码更紧凑，更具备可读性。另外slim提供了很多计算机视觉方面的著名模型（VGG, AlexNet等），我们不仅可以直接使用，甚至能以各种方式进行扩展。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记8：AlexNet</title>
    <link href="http://zhengyujie.cn/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/"/>
    <id>http://zhengyujie.cn/2019/08/08/tf学习笔记8/</id>
    <published>2019-08-08T10:38:17.000Z</published>
    <updated>2019-08-19T08:14:48.069Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>AlexNet是Hinton的学生Alex Krizhevsky在2012年提出的深度卷积神经网络，它是LeNet一种更深更宽的版本。在AlexNet上首次应用了几个trick，ReLU、Dropout和LRN。AlexNet包含了6亿3000万个连接，6000万个参数和65万个神经元，有5个卷积层，3个全连接层。在ILSVRC 2012比赛中，AlexNet以top-5的错误率为16.4%的显著优势夺得冠军，第二名的成绩是26.2%。AlexNet的trick主要包括：</p><a id="more"></a><ol><li>成功使用RELU作为CNN的激活函数，并验证其效果在较深的网络中的效果超过了sigmoid，解决了sigmoid在深层的网络中的梯度弥散的问题。</li><li>使用Dropout来随机使得一部分神经元失活，来避免模型的过拟合，在AlexNet中，dropout主要应用在全连接层。</li><li>使用重叠的最大池化，以前在卷积神经网络中大部分都采用平均池化，在AlexNet中都是使用最大池化，最大池化可以避免平均池化的模糊化效果。重叠的最大池化是指卷积核的尺寸要大于步长，这样池化层的输出之间会有重叠和覆盖，提升特征的丰富性。在AlexNet中使用的卷积核大小为3×3，横向和纵向的步长都为2。</li><li>使用LRN层，对局部神经元的活动创建有竞争机制，让响应较大的值变得相对更大，并抑制反馈较小的神经元，来增强模型的泛化能力。</li><li>使用了CUDA来加速深度神经网络的训练。</li><li>数据增强，随机从256×256的原始图像中截取224×224的图像以及随机翻转。如果没有数据增强，在参数众多的情况下，卷积神经网络会陷入到过拟合中，使用数据增强可以减缓过拟合，提升泛化能力。进行预测的时候，提取图片的四个角加中间位置，并进行左右翻转，一共10张图片，对它们进行预测并取10次结果的平均值。在AlexNet论文中也提到了，对图像的RGB数据进行PCA处理，并做一个标准差为0.1的高斯扰动，增加一些噪声，可以降低1%的错误率。</li></ol><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/oqz082I49HMR.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>第一层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv1"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#设置卷积核11×11,3通道,64个卷积核</span></span><br><span class="line">    kernel1 = tf.Variable(tf.truncated_normal([<span class="number">11</span>,<span class="number">11</span>,<span class="number">3</span>,<span class="number">64</span>],mean=<span class="number">0</span>,stddev=<span class="number">0.1</span>,</span><br><span class="line">                                              dtype=tf.float32),name=<span class="string">"weights"</span>)</span><br><span class="line">    <span class="comment">#卷积,卷积的横向步长和竖向补偿都为4</span></span><br><span class="line">    conv = tf.nn.conv2d(images,kernel1,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0</span>,shape=[<span class="number">64</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活函数</span></span><br><span class="line">    conv1 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    <span class="comment">#输出该层的信息</span></span><br><span class="line">    print_tensor_info(conv1)</span><br><span class="line">    <span class="comment">#统计参数</span></span><br><span class="line">    parameters += [kernel1,biases]</span><br><span class="line">    <span class="comment">#lrn处理</span></span><br><span class="line">    lrn1 = tf.nn.lrn(conv1,<span class="number">4</span>,bias=<span class="number">1</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn1"</span>)</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool1 = tf.nn.max_pool(lrn1,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool1"</span>)</span><br><span class="line">    print_tensor_info(pool1)</span><br></pre></td></tr></table></figure></li><li><p>第二层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv2"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel2 = tf.Variable(tf.truncated_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                          ,name=<span class="string">"weights"</span>)</span><br><span class="line">    conv = tf.nn.conv2d(pool1,kernel2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0</span>,dtype=tf.float32,shape=[<span class="number">192</span>])</span><br><span class="line">                         ,trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活</span></span><br><span class="line">    conv2 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    print_tensor_info(conv2)</span><br><span class="line">    parameters += [kernel2,biases]</span><br><span class="line">    <span class="comment">#LRN</span></span><br><span class="line">    lrn2 = tf.nn.lrn(conv2,<span class="number">4</span>,<span class="number">1.0</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn2"</span>)</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool2 = tf.nn.max_pool(lrn2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool2"</span>)</span><br><span class="line">    print_tensor_info(pool2)</span><br></pre></td></tr></table></figure></li><li><p>第三层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv3"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">       <span class="comment">#初始化权重</span></span><br><span class="line">       kernel3 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">192</span>,<span class="number">384</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                             ,name=<span class="string">"weights"</span>)</span><br><span class="line">       conv = tf.nn.conv2d(pool2,kernel3,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">       biases = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">384</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">       bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">       <span class="comment">#RELU激活层</span></span><br><span class="line">       conv3 = tf.nn.relu(bias,name=scope)</span><br><span class="line">       parameters += [kernel3,biases]</span><br><span class="line">       print_tensor_info(conv3)</span><br></pre></td></tr></table></figure></li><li><p>第四层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv4"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel4 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">384</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weights"</span>)</span><br><span class="line">    <span class="comment">#卷积</span></span><br><span class="line">    conv = tf.nn.conv2d(conv3,kernel4,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活</span></span><br><span class="line">    conv4 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    parameters += [kernel4,biases]</span><br><span class="line">    print_tensor_info(conv4)</span><br></pre></td></tr></table></figure></li><li><p>第五层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv5"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel5 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weights"</span>)</span><br><span class="line">    conv = tf.nn.conv2d(conv4,kernel5,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#REUL激活层</span></span><br><span class="line">    conv5 = tf.nn.relu(bias)</span><br><span class="line">    parameters += [kernel5,bias]</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool5 = tf.nn.max_pool(conv5,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool5"</span>)</span><br><span class="line">    print_tensor_info(pool5)</span><br></pre></td></tr></table></figure></li><li><p>最后三层全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第六层全连接层</span></span><br><span class="line">pool5 = tf.reshape(pool5,(<span class="number">-1</span>,<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>))</span><br><span class="line">weight6 = tf.Variable(tf.truncated_normal([<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                       name=<span class="string">"weight6"</span>)</span><br><span class="line">ful_bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias1"</span>)</span><br><span class="line">ful_con1 = tf.nn.relu(tf.add(tf.matmul(pool5,weight6),ful_bias1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第七层第二层全连接层</span></span><br><span class="line">weight7 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                      name=<span class="string">"weight7"</span>)</span><br><span class="line">ful_bias2 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias2"</span>)</span><br><span class="line">ful_con2 = tf.nn.relu(tf.add(tf.matmul(ful_con1,weight7),ful_bias2))</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#第八层第三层全连接层</span></span><br><span class="line">weight8 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                      name=<span class="string">"weight8"</span>)</span><br><span class="line">ful_bias3 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">1000</span>]),name=<span class="string">"ful_bias3"</span>)</span><br><span class="line">ful_con3 = tf.nn.relu(tf.add(tf.matmul(ful_con2,weight8),ful_bias3))</span><br></pre></td></tr></table></figure></li><li><p>softmax层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weight9 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>),dtype=tf.float32,name=<span class="string">"weight9"</span>)</span><br><span class="line">bias9 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>]),dtype=tf.float32,name=<span class="string">"bias9"</span>)</span><br><span class="line">output_softmax = tf.nn.softmax(tf.matmul(ful_con3,weight9)+bias9)</span><br></pre></td></tr></table></figure></li><li><p>评估模型性能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,info_string)</span>:</span></span><br><span class="line">    <span class="comment">#前10次迭代不计入时间消耗</span></span><br><span class="line">    num_step_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_bathes + num_step_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_step_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span> :</span><br><span class="line">                print(<span class="string">"%s:step %d,duration=%.3f"</span>%(datetime.now(),i-num_step_burn_in,duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_squared += duration * duration</span><br><span class="line">    <span class="comment">#计算消耗时间的平均差</span></span><br><span class="line">    mn = total_duration / num_bathes</span><br><span class="line">    <span class="comment">#计算消耗时间的标准差</span></span><br><span class="line">    vr = total_duration_squared / num_bathes - mn * mn</span><br><span class="line">    std = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s:%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_bathes,mn,std))</span><br></pre></td></tr></table></figure></li></ol><h1 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> math,time</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_bathes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">获取tensor信息</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_tensor_info</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    print(<span class="string">"tensor name:"</span>,tensor.op.name,<span class="string">"-tensor shape:"</span>,tensor.get_shape().as_list())</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">计算每次迭代消耗时间</span></span><br><span class="line"><span class="string">session:TensorFlow的Session</span></span><br><span class="line"><span class="string">target:需要评测的运算算子</span></span><br><span class="line"><span class="string">info_string:测试的名称</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,info_string)</span>:</span></span><br><span class="line">    <span class="comment">#前10次迭代不计入时间消耗</span></span><br><span class="line">    num_step_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_bathes + num_step_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_step_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span> :</span><br><span class="line">                print(<span class="string">"%s:step %d,duration=%.3f"</span>%(datetime.now(),i-num_step_burn_in,duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_squared += duration * duration</span><br><span class="line">    <span class="comment">#计算消耗时间的平均差</span></span><br><span class="line">    mn = total_duration / num_bathes</span><br><span class="line">    <span class="comment">#计算消耗时间的标准差</span></span><br><span class="line">    vr = total_duration_squared / num_bathes - mn * mn</span><br><span class="line">    std = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s:%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_bathes,</span><br><span class="line">                                                             mn,std))</span><br><span class="line"><span class="comment">#主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_bechmark</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        image_size = <span class="number">224</span></span><br><span class="line">        <span class="comment">#以高斯分布产生一些图片</span></span><br><span class="line">        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,<span class="number">3</span>],</span><br><span class="line">                                              dtype=tf.float32,stddev=<span class="number">0.1</span>))</span><br><span class="line">        output,parameters = inference(images)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        time_tensorflow_run(sess,output,<span class="string">"Forward"</span>)</span><br><span class="line">        objective = tf.nn.l2_loss(output)</span><br><span class="line">        grad = tf.gradients(objective,parameters)</span><br><span class="line">        time_tensorflow_run(sess,grad,<span class="string">"Forward-backward"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义参数</span></span><br><span class="line">    parameters = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第一层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv1"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#设置卷积核11×11,3通道,64个卷积核</span></span><br><span class="line">        kernel1 = tf.Variable(tf.truncated_normal([<span class="number">11</span>,<span class="number">11</span>,<span class="number">3</span>,<span class="number">64</span>],mean=<span class="number">0</span>,stddev=<span class="number">0.1</span>,</span><br><span class="line">                                                  dtype=tf.float32),name=<span class="string">"weights"</span>)</span><br><span class="line">        <span class="comment">#卷积,卷积的横向步长和竖向补偿都为4</span></span><br><span class="line">        conv = tf.nn.conv2d(images,kernel1,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0</span>,shape=[<span class="number">64</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活函数</span></span><br><span class="line">        conv1 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        <span class="comment">#输出该层的信息</span></span><br><span class="line">        print_tensor_info(conv1)</span><br><span class="line">        <span class="comment">#统计参数</span></span><br><span class="line">        parameters += [kernel1,biases]</span><br><span class="line">        <span class="comment">#lrn处理</span></span><br><span class="line">        lrn1 = tf.nn.lrn(conv1,<span class="number">4</span>,bias=<span class="number">1</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn1"</span>)</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool1 = tf.nn.max_pool(lrn1,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool1"</span>)</span><br><span class="line">        print_tensor_info(pool1)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第二层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv2"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel2 = tf.Variable(tf.truncated_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                              ,name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(pool1,kernel2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0</span>,dtype=tf.float32,shape=[<span class="number">192</span>])</span><br><span class="line">                             ,trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活</span></span><br><span class="line">        conv2 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        print_tensor_info(conv2)</span><br><span class="line">        parameters += [kernel2,biases]</span><br><span class="line">        <span class="comment">#LRN</span></span><br><span class="line">        lrn2 = tf.nn.lrn(conv2,<span class="number">4</span>,<span class="number">1.0</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn2"</span>)</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool2 = tf.nn.max_pool(lrn2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool2"</span>)</span><br><span class="line">        print_tensor_info(pool2)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第三层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv3"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel3 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">192</span>,<span class="number">384</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                              ,name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(pool2,kernel3,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">384</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活层</span></span><br><span class="line">        conv3 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        parameters += [kernel3,biases]</span><br><span class="line">        print_tensor_info(conv3)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第四层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv4"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel4 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">384</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                              name=<span class="string">"weights"</span>)</span><br><span class="line">        <span class="comment">#卷积</span></span><br><span class="line">        conv = tf.nn.conv2d(conv3,kernel4,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活</span></span><br><span class="line">        conv4 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        parameters += [kernel4,biases]</span><br><span class="line">        print_tensor_info(conv4)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第五层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv5"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel5 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                              name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(conv4,kernel5,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#REUL激活层</span></span><br><span class="line">        conv5 = tf.nn.relu(bias)</span><br><span class="line">        parameters += [kernel5,bias]</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool5 = tf.nn.max_pool(conv5,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool5"</span>)</span><br><span class="line">        print_tensor_info(pool5)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第六层全连接层</span></span><br><span class="line">    pool5 = tf.reshape(pool5,(<span class="number">-1</span>,<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>))</span><br><span class="line">    weight6 = tf.Variable(tf.truncated_normal([<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                           name=<span class="string">"weight6"</span>)</span><br><span class="line">    ful_bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias1"</span>)</span><br><span class="line">    ful_con1 = tf.nn.relu(tf.add(tf.matmul(pool5,weight6),ful_bias1))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第七层第二层全连接层</span></span><br><span class="line">    weight7 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weight7"</span>)</span><br><span class="line">    ful_bias2 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias2"</span>)</span><br><span class="line">    ful_con2 = tf.nn.relu(tf.add(tf.matmul(ful_con1,weight7),ful_bias2))</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#第八层第三层全连接层</span></span><br><span class="line">    weight8 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weight8"</span>)</span><br><span class="line">    ful_bias3 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">1000</span>]),name=<span class="string">"ful_bias3"</span>)</span><br><span class="line">    ful_con3 = tf.nn.relu(tf.add(tf.matmul(ful_con2,weight8),ful_bias3))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#softmax层</span></span><br><span class="line">    weight9 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>),dtype=tf.float32,name=<span class="string">"weight9"</span>)</span><br><span class="line">    bias9 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>]),dtype=tf.float32,name=<span class="string">"bias9"</span>)</span><br><span class="line">    output_softmax = tf.nn.softmax(tf.matmul(ful_con3,weight9)+bias9)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_softmax,parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    run_bechmark()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;AlexNet&quot;&gt;&lt;a href=&quot;#AlexNet&quot; class=&quot;headerlink&quot; title=&quot;AlexNet&quot;&gt;&lt;/a&gt;AlexNet&lt;/h1&gt;&lt;p&gt;AlexNet是Hinton的学生Alex Krizhevsky在2012年提出的深度卷积神经网络，它是LeNet一种更深更宽的版本。在AlexNet上首次应用了几个trick，ReLU、Dropout和LRN。AlexNet包含了6亿3000万个连接，6000万个参数和65万个神经元，有5个卷积层，3个全连接层。在ILSVRC 2012比赛中，AlexNet以top-5的错误率为16.4%的显著优势夺得冠军，第二名的成绩是26.2%。AlexNet的trick主要包括：&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="AlexNet" scheme="http://zhengyujie.cn/tags/AlexNet/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记9：VGGNet-16</title>
    <link href="http://zhengyujie.cn/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/"/>
    <id>http://zhengyujie.cn/2019/08/08/tf学习笔记9/</id>
    <published>2019-08-08T10:38:17.000Z</published>
    <updated>2019-08-19T08:15:07.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><p>VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind公司一起研发的深度卷积神经网络。VGGNet探索了卷积神经网络的深度和其性能之间的关系，通过反复堆叠3×3的小型卷积核和2×2的最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络。到目前为止，VGGNet还经常被用来提取图像的特征。VGGNet训练后的模型参数在其官方网站上开源了，可用来在domain specific的图像分类任务上进行再训练(相当于提供了非常好的初始化权重)。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/a259vKoJSqwF.png?imageslim" alt="mark"></p><p>VGGNet有5段卷积，每一段内有2~3个卷积层，每段卷积的尾部会连接一个最大池化层来缩小图片的尺寸。每段内的卷积核数量一样，越靠后的段的卷积核数量越多，卷积核数量核段的关系:64-128-256-512-512。在段内有多个完全一样的3×3的卷积层堆叠在一起的情况，在卷积神经网络中这其实是一种非常有用的设计。两个3×3的卷积层串联相当于1个5×5的卷积层，即一个像素会和周围5×5的像素产生关联，也就是感受野为5×5。而3个3×3的卷积层串联的效果则相当于1个7×7的卷积层。同时，3个3×3卷积层比1个7×7的卷积层有着更少的参数，(3<em>3</em>3)/(7*7)=55%。最重要的是，3个3×3的卷积层拥有比1个7×7的卷积层更多的非线性变换，3个3×3的卷积层使用了3次RELU激活函数，而1个7×7的卷积层只使用了1次，这样可以让卷积神经网络对特征的学习能力更强。</p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>卷积层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义卷积层函数</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name：该层的名称</span></span><br><span class="line"><span class="string">kh:卷积核的高</span></span><br><span class="line"><span class="string">kw:卷积核的宽</span></span><br><span class="line"><span class="string">n_out:卷积核的数量(输出通道数)</span></span><br><span class="line"><span class="string">dh:步长的高</span></span><br><span class="line"><span class="string">dw:步长的宽</span></span><br><span class="line"><span class="string">p:参数列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_op</span><span class="params">(input_op,name,kh,kw,n_out,dh,dw,p)</span>:</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel = tf.get_variable(scope+<span class="string">"w"</span>,shape=[kh,kw,n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer_conv2d())</span><br><span class="line">                                 </span><br><span class="line">        <span class="comment">#卷积</span></span><br><span class="line">        conv = tf.nn.conv2d(input_op,kernel,(<span class="number">1</span>,dh,dw,<span class="number">1</span>),padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        bias_init_val = tf.constant(<span class="number">0.0</span>,shape=[n_out],dtype=tf.float32)</span><br><span class="line">        biases = tf.Variable(bias_init_val,trainable=<span class="literal">True</span>,name=<span class="string">"b"</span>)</span><br><span class="line">        z = tf.nn.bias_add(conv,biases)</span><br><span class="line">        activation = tf.nn.relu(z,name=scope)</span><br><span class="line">        <span class="comment">#保存参数</span></span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">    <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure></li><li><p>全连接层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义全连接层函数</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name:该层的名称</span></span><br><span class="line"><span class="string">n_out:输出的通道数</span></span><br><span class="line"><span class="string">p:参数列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_op</span><span class="params">(input_op,name,n_out,p)</span>:</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化全连接的权重</span></span><br><span class="line">        kernel = tf.get_variable(scope+<span class="string">"w"</span>,shape=[n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())              </span><br><span class="line">        <span class="comment">#初始化全连接层的偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[n_out],dtype=tf.float32),name=<span class="string">"b"</span>)</span><br><span class="line">        <span class="comment">#将输入与权重的乘法和偏置的加法合并</span></span><br><span class="line">        activation = tf.nn.relu_layer(input_op,kernel,biases,name=scope)</span><br><span class="line">        <span class="comment">#保存参数</span></span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">        <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure></li><li><p>最大池化层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义最大池化层</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name:该层的名称</span></span><br><span class="line"><span class="string">kh:池化层的高</span></span><br><span class="line"><span class="string">kw:池化层的宽</span></span><br><span class="line"><span class="string">dh:步长的高</span></span><br><span class="line"><span class="string">dw:步长的宽</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(input_op,name,kh,kw,dh,dw)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(input_op,ksize=[<span class="number">1</span>,kh,kw,<span class="number">1</span>],strides=[<span class="number">1</span>,dh,dw,<span class="number">1</span>],padding=<span class="string">"SAME"</span>,name=name)</span><br></pre></td></tr></table></figure></li><li><p>VGG实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">VGG16</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_op</span><span class="params">(input_op,keep_prob)</span>:</span></span><br><span class="line">    p = []</span><br><span class="line">    <span class="comment">#第一层的第一层卷积</span></span><br><span class="line">    conv1_1 = conv_op(input_op,name=<span class="string">"conv1_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第一层的第二层卷积</span></span><br><span class="line">    conv1_2 = conv_op(conv1_1,name=<span class="string">"conv1_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#最大池化层</span></span><br><span class="line">    pool1 = max_pool(conv1_2,name=<span class="string">"pool1"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dw=<span class="number">2</span>,dh=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第二层的第一层卷积</span></span><br><span class="line">    conv2_1 = conv_op(pool1,name=<span class="string">"conv2_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第二层的第二层卷积</span></span><br><span class="line">    conv2_2 = conv_op(conv2_1,name=<span class="string">"conv2_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第二层的最大池化</span></span><br><span class="line">    pool2 = max_pool(conv2_2,name=<span class="string">"pool2"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第三层</span></span><br><span class="line">    conv3_1 = conv_op(pool2,name=<span class="string">"conv3_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv3_2 = conv_op(conv3_1,name=<span class="string">"conv3_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv3_3 = conv_op(conv3_2,name=<span class="string">"conv3_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool3 = max_pool(conv3_3,name=<span class="string">"pool3"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第四层</span></span><br><span class="line">    conv4_1 = conv_op(pool3,name=<span class="string">"conv4_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv4_2 = conv_op(conv4_1,name=<span class="string">"conv4_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv4_3 = conv_op(conv4_2,name=<span class="string">"conv4_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool4 = max_pool(conv4_3,name=<span class="string">"pool4"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第五层</span></span><br><span class="line">    conv5_1 = conv_op(pool4,name=<span class="string">"conv5_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv5_2 = conv_op(conv5_1,name=<span class="string">"conv5_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv5_3 = conv_op(conv5_2,name=<span class="string">"conv5_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool5 = max_pool(conv5_3,name=<span class="string">"pool5"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#将pool5展平</span></span><br><span class="line">    pool5_shape = pool5.get_shape()</span><br><span class="line">    flattened_shape = pool5_shape[<span class="number">1</span>].value * pool5_shape[<span class="number">2</span>].value * pool5_shape[<span class="number">3</span>].value</span><br><span class="line">    resh1 = tf.reshape(pool5,[<span class="number">-1</span>,flattened_shape],name=<span class="string">"resh1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    fc6 = fc_op(resh1,name=<span class="string">"fc6"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc6_drop = tf.nn.dropout(fc6,keep_prob,name=<span class="string">"fc6_drop"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    fc7 = fc_op(fc6_drop,name=<span class="string">"fc7"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc7_drop = tf.nn.dropout(fc7,keep_prob,name=<span class="string">"fc7_drop"</span>)</span><br><span class="line"></span><br><span class="line">    fc8 = fc_op(fc7_drop,name=<span class="string">"fc8"</span>,n_out=<span class="number">1000</span>,p=p)</span><br><span class="line">    softmax = tf.nn.softmax(fc8)</span><br><span class="line">    predictions = tf.argmax(softmax,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions,softmax,fc8,p</span><br></pre></td></tr></table></figure></li><li><p>性能统计 性能统计模块主要统计网络迭代一次所需时间，由于刚开始运行程序的时候GPU需要加载内存会比较慢，所以统计通10次迭代以后才开始。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_batches = <span class="number">100</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,feed,info_string)</span>:</span></span><br><span class="line">    num_steps_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _= session.run(target,feed_dict=feed)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt; num_steps_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</span><br><span class="line">                print(<span class="string">"%s：step:%d,duration:%.3f"</span>%(datetime.now(),i-num_steps_burn_in,duration))</span><br><span class="line">                total_duration += duration</span><br><span class="line">                total_duration_squared += duration * duration</span><br><span class="line">    mn = total_duration / num_batches</span><br><span class="line">    vr = total_duration_squared / num_batches - mn * mn</span><br><span class="line">    sd = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s：%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_batches,mn,sd))</span><br></pre></td></tr></table></figure></li><li><p>训练过程 通过使用random_normal来随机产生224×224的图片，进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_benchmark</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        image_size = <span class="number">224</span></span><br><span class="line">        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,<span class="number">3</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>))</span><br><span class="line">        keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">        predictions,softmax,fc8,p=inference_op(images,keep_prob)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        time_tensorflow_run(sess,predictions,&#123;keep_prob:<span class="number">1.0</span>&#125;,<span class="string">"Forward"</span>)</span><br><span class="line">        objective = tf.nn.l2_loss(fc8)</span><br><span class="line">        grad = tf.gradients(objective,p)</span><br><span class="line">        time_tensorflow_run(sess,grad,&#123;keep_prob:<span class="number">0.5</span>&#125;,<span class="string">"Forward-backward"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    run_benchmark()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;VGGNet&quot;&gt;&lt;a href=&quot;#VGGNet&quot; class=&quot;headerlink&quot; title=&quot;VGGNet&quot;&gt;&lt;/a&gt;VGGNet&lt;/h1&gt;&lt;p&gt;VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind公司一起研发的深度卷积神经网络。VGGNet探索了卷积神经网络的深度和其性能之间的关系，通过反复堆叠3×3的小型卷积核和2×2的最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络。到目前为止，VGGNet还经常被用来提取图像的特征。VGGNet训练后的模型参数在其官方网站上开源了，可用来在domain specific的图像分类任务上进行再训练(相当于提供了非常好的初始化权重)。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="VGGNet" scheme="http://zhengyujie.cn/tags/VGGNet/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记7：卷积神经网络</title>
    <link href="http://zhengyujie.cn/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/"/>
    <id>http://zhengyujie.cn/2019/08/08/tf学习笔记7/</id>
    <published>2019-08-08T07:07:32.000Z</published>
    <updated>2019-08-19T08:14:24.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>用Tensorflow实现一个完整的卷积神经网络，用这个卷积神经网络来识别手写数字数据集（MNIST）。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/894wgqHtxSmc.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></li><li><p>导入MINIST数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># number 1 to 10 data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="定义Weight和Bias参数"><a href="#定义Weight和Bias参数" class="headerlink" title="定义Weight和Bias参数"></a>定义Weight和Bias参数</h2><ol><li><p>定义产生Weight参数的函数，传入shape，返回Weight参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></li><li><p>定义产生Bias参数的函数，传入shape，返回Bias参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></li></ol><h2 id="定义卷积和池化操作"><a href="#定义卷积和池化操作" class="headerlink" title="定义卷积和池化操作"></a>定义卷积和池化操作</h2><ol><li><p>定义卷积操作。tf.nn.conv2d函数是Tensorflow里面的二维的卷积函数，x是图片的所有参数，W是卷积层的权重，然后定义步长strides=[1,1,1,1]值。strides[0]和strides[3]的两个1是默认值，意思是不对样本个数和channel进行卷积，中间两个1代表padding是在x方向运动一步，y方向运动一步，padding采用的方式实“SAME”就是0填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="comment"># stride[1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="comment"># Must have strides[0] = strides[3] =1</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)  <span class="comment"># padding="SAME"用零填充边界</span></span><br></pre></td></tr></table></figure></li><li><p>定义池化操作。为了得到更多的图片信息，卷积时我们选择的是一次一步，也就是strides[1]=strides[2]=1,这样得到的图片尺寸没有变化，而我们希望压缩一下图片也就是参数能少一些从而减少系统的复杂度，因此我们采用pooling来稀疏化参数，也就是卷积神经网络中所谓的下采样层。pooling有两种，一种是最大值池化，一种是平均值池化，我采用的是最大值池化tf.max_pool()。池化的核函数大小为2*2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="输入处理"><a href="#输入处理" class="headerlink" title="输入处理"></a>输入处理</h2><ol><li><p>定义输入的placeholder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>]) <span class="comment"># 28*28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure></li><li><p>定义dropout的placeholder，它是解决过拟合的有效手段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义dropout的输入，解决过拟合问题</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure></li><li><p>处理xs输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理xs，把xs的形状变成[-1,28,28,1]</span></span><br><span class="line"><span class="comment"># -1代表先不考虑输入的图片例子多少这个维度。</span></span><br><span class="line"><span class="comment"># 后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1。如果是RGB图像，那么channel就是3.</span></span><br><span class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li></ol><h2 id="建立卷积层"><a href="#建立卷积层" class="headerlink" title="建立卷积层"></a>建立卷积层</h2><ol><li><p>第一层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>]) <span class="comment"># kernel 5*5, channel is 1, out size 32</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)  <span class="comment"># output size 28*28*32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)                          <span class="comment"># output size 14*14*32</span></span><br></pre></td></tr></table></figure></li><li><p>第二层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>]) <span class="comment"># kernel 5*5, in size 32, out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  <span class="comment"># output size 14*14*64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)                          <span class="comment"># output size 7*7*64</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="建立全连接层"><a href="#建立全连接层" class="headerlink" title="建立全连接层"></a>建立全连接层</h2><ol><li><p>展平输出。我们通过tf.reshape()将h_pool2的输出值从一个三维的变为一个一维的数据，-1表示先不考虑输入图片例子维度，将上一个输出结果展平。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [n_samples,7,7,64]-&gt;&gt;[n_samples, 7*7*64]</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br></pre></td></tr></table></figure></li><li><p>全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure></li><li><p>同时考虑了过拟合的问题，可以加一个dropout的处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure></li></ol><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><ol><li><p>输出层参数，输入为1024，输出为10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br></pre></td></tr></table></figure></li><li><p>softmax分类器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)</span><br></pre></td></tr></table></figure></li></ol><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><ol><li><p>交叉熵损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li><li><p>优化器</p></li></ol><ul><li><p>tf.train.AdamOptimizer()优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></li><li><p>tf.train.GradientDescentOptimizer()优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></li></ul><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol><li><p>定义session，初始化变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess =tf.Session()</span><br><span class="line"><span class="comment"># important step</span></span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br></pre></td></tr></table></figure></li><li><p>训练1000次，每50次检查模型精度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs,batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:batch_xs,ys:batch_ys, keep_prob:<span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># print(sess.run(prediction,feed_dict=&#123;xs:batch_xs&#125;))</span></span><br><span class="line">        print(compute_accuracy(mnist.test.images,mnist.test.labels))</span><br></pre></td></tr></table></figure></li></ol><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment"># 导入本次需要的模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># number 1 to 10 data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs,v_ys)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs:v_xs, keep_prob:<span class="number">1</span>&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>),tf.argmax(v_ys,<span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs:v_xs,ys:v_ys,keep_prob:<span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="comment"># stride[1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="comment"># Must have strides[0] = strides[3] =1</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)  <span class="comment"># padding="SAME"用零填充边界</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>]) <span class="comment"># 28*28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 定义dropout的输入，解决过拟合问题</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># 处理xs，把xs的形状变成[-1,28,28,1]</span></span><br><span class="line"><span class="comment"># -1代表先不考虑输入的图片例子多少这个维度。</span></span><br><span class="line"><span class="comment"># 后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1。如果是RGB图像，那么channel就是3.</span></span><br><span class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># print(x_image.shape) #[n_samples, 28,28,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## convl layer ##</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>]) <span class="comment"># kernel 5*5, channel is 1, out size 32</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)  <span class="comment"># output size 28*28*32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)                          <span class="comment"># output size 14*14*32</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## conv2 layer ##</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>]) <span class="comment"># kernel 5*5, in size 32, out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  <span class="comment"># output size 14*14*64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)                          <span class="comment"># output size 7*7*64</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## funcl layer ##</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># [n_samples,7,7,64]-&gt;&gt;[n_samples, 7*7*64]</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"> </span><br><span class="line"><span class="comment">## func2 layer ##</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[<span class="number">1</span>])) <span class="comment">#loss</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span></span><br><span class="line"> </span><br><span class="line">sess =tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs,batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:batch_xs,ys:batch_ys, keep_prob:<span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># print(sess.run(prediction,feed_dict=&#123;xs:batch_xs&#125;))</span></span><br><span class="line">        print(compute_accuracy(mnist.test.images,mnist.test.labels))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot; class=&quot;headerlink&quot; title=&quot;目标&quot;&gt;&lt;/a&gt;目标&lt;/h1&gt;&lt;p&gt;用Tensorflow实现一个完整的卷积神经网络，用这个卷积神经网络来识别手写数字数据集（MNIST）。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="卷积网络" scheme="http://zhengyujie.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="MNIST" scheme="http://zhengyujie.cn/tags/MNIST/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow学习笔记6：逻辑回归</title>
    <link href="http://zhengyujie.cn/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/"/>
    <id>http://zhengyujie.cn/2019/08/08/tf学习笔记6/</id>
    <published>2019-08-08T02:59:16.000Z</published>
    <updated>2019-08-19T08:13:59.717Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。</p><a id="more"></a><h1 id="MINIST数据集"><a href="#MINIST数据集" class="headerlink" title="MINIST数据集"></a>MINIST数据集</h1><p>MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成，是一个非常有名的手写体数字识别数据集，在很多资料中，这个数据集都会被用作深度学习的入门样例。 </p><ul><li><p>存储形式<br>共有四个压缩文件<br>train-images-idx3-ubyte.gz: training set images (9912422 bytes)<br>train-labels-idx1-ubyte.gz: training set labels (28881 bytes)<br>t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)<br>t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) </p></li><li><p>样本个数<br>训练样本：共55000个<br>验证样本：共5000个<br>测试样本：共10000个 </p></li></ul><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载MINIST数据集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输入,784为MINIST数据集的图片大小28*28</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出,10为预测的类别数</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定模型的权重和偏移量</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型的结构</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cross entropy来作为损失函数</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Gradient Descent</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行初始化</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># 循环每一个batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># 执行optimizer,获得cost</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># 计算平均损失</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># 显示每一轮的结果</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试模型</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0001 cost= 1.183872078</span><br><span class="line">Epoch: 0002 cost= 0.665350118</span><br><span class="line">Epoch: 0003 cost= 0.552830602</span><br><span class="line">Epoch: 0004 cost= 0.498699041</span><br><span class="line">Epoch: 0005 cost= 0.465488806</span><br><span class="line">Epoch: 0006 cost= 0.442619649</span><br><span class="line">Epoch: 0007 cost= 0.425471577</span><br><span class="line">Epoch: 0008 cost= 0.412201005</span><br><span class="line">Epoch: 0009 cost= 0.401415385</span><br><span class="line">Epoch: 0010 cost= 0.392391824</span><br><span class="line">Epoch: 0011 cost= 0.384738960</span><br><span class="line">Epoch: 0012 cost= 0.378136856</span><br><span class="line">Epoch: 0013 cost= 0.372445326</span><br><span class="line">Epoch: 0014 cost= 0.367273882</span><br><span class="line">Epoch: 0015 cost= 0.362716155</span><br><span class="line">Epoch: 0016 cost= 0.358604888</span><br><span class="line">Epoch: 0017 cost= 0.354853253</span><br><span class="line">Epoch: 0018 cost= 0.351472244</span><br><span class="line">Epoch: 0019 cost= 0.348347617</span><br><span class="line">Epoch: 0020 cost= 0.345449658</span><br><span class="line">Epoch: 0021 cost= 0.342724947</span><br><span class="line">Epoch: 0022 cost= 0.340273546</span><br><span class="line">Epoch: 0023 cost= 0.337938625</span><br><span class="line">Epoch: 0024 cost= 0.335751063</span><br><span class="line">Epoch: 0025 cost= 0.333709621</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9138</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;逻辑回归&quot;&gt;&lt;a href=&quot;#逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归&quot;&gt;&lt;/a&gt;逻辑回归&lt;/h1&gt;&lt;p&gt;逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="逻辑回归" scheme="http://zhengyujie.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>TensoeFlow学习笔记5：线性回归</title>
    <link href="http://zhengyujie.cn/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/"/>
    <id>http://zhengyujie.cn/2019/08/08/tf学习笔记5/</id>
    <published>2019-08-08T02:28:24.000Z</published>
    <updated>2019-08-19T08:13:44.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。</p><a id="more"></a><h1 id="简单实现"><a href="#简单实现" class="headerlink" title="简单实现"></a>简单实现</h1><p>具体代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">num_points=<span class="number">1000</span>    </span><br><span class="line">vectors_set=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_points):</span><br><span class="line">    <span class="comment"># 横坐标，进行随机高斯处理化，以0为均值，以0.55为标准差</span></span><br><span class="line">    x1=np.random.normal(<span class="number">0.0</span>,<span class="number">0.55</span>)</span><br><span class="line">    <span class="comment"># 纵坐标，数据点在y1=x1*0.1+0.3上小范围浮动</span></span><br><span class="line">    y1=x1*<span class="number">0.1</span>+<span class="number">0.3</span>+np.random.normal(<span class="number">0.0</span>,<span class="number">0.03</span>)</span><br><span class="line">    vectors_set.append([x1,y1])</span><br><span class="line">    x_data=[v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors_set]</span><br><span class="line">    y_data=[v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors_set]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成1维的W矩阵，取值是[-1,1]之间的随机数</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'W'</span>)</span><br><span class="line"><span class="comment"># 生成1维的b矩阵，初始值是0</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'b'</span>)</span><br><span class="line"><span class="comment"># 经过计算得出预估值y</span></span><br><span class="line">y = W * x_data + b</span><br><span class="line"><span class="comment"># 以预估值y和实际值y_data之间的均方误差作为损失</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data), name=<span class="string">'loss'</span>)</span><br><span class="line"><span class="comment"># 采用梯度下降法来优化参数  学习率为0.5</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 训练的过程就是最小化这个误差值</span></span><br><span class="line">train = optimizer.minimize(loss, name=<span class="string">'train'</span>)</span><br><span class="line"><span class="comment"># sess = tf.Session()</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 执行20次训练</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        sess.run(train) <span class="comment"># 输出训练好的W和b</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"W ="</span>, sess.run(W), <span class="string">"b ="</span>, sess.run(b), <span class="string">"loss ="</span>, sess.run(loss))</span><br><span class="line"></span><br><span class="line">    plt.scatter(x_data,y_data,c=<span class="string">'r'</span>)</span><br><span class="line">    plt.plot(x_data,sess.run(y))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">W = [0.41536754] b = [0.29331774] loss = 0.028788242</span><br><span class="line">W = [0.32664812] b = [0.29488927] loss = 0.015261736</span><br><span class="line">W = [0.26295245] b = [0.29606903] loss = 0.008289363</span><br><span class="line">W = [0.2172218] b = [0.29691604] loss = 0.004695383</span><br><span class="line">W = [0.18438922] b = [0.29752415] loss = 0.002842829</span><br><span class="line">W = [0.1608169] b = [0.29796076] loss = 0.0018879117</span><br><span class="line">W = [0.143893] b = [0.29827422] loss = 0.0013956894</span><br><span class="line">W = [0.1317424] b = [0.2984993] loss = 0.0011419685</span><br><span class="line">W = [0.12301882] b = [0.29866084] loss = 0.0010111856</span><br><span class="line">W = [0.11675566] b = [0.29877687] loss = 0.00094377215</span><br><span class="line">W = [0.112259] b = [0.29886016] loss = 0.0009090233</span><br><span class="line">W = [0.1090306] b = [0.29891995] loss = 0.0008911116</span><br><span class="line">W = [0.10671275] b = [0.29896286] loss = 0.00088187883</span><br><span class="line">W = [0.10504864] b = [0.2989937] loss = 0.0008771197</span><br><span class="line">W = [0.10385388] b = [0.29901582] loss = 0.00087466656</span><br><span class="line">W = [0.1029961] b = [0.2990317] loss = 0.00087340205</span><br><span class="line">W = [0.10238025] b = [0.29904312] loss = 0.0008727502</span><br><span class="line">W = [0.10193809] b = [0.2990513] loss = 0.0008724143</span><br><span class="line">W = [0.10162064] b = [0.2990572] loss = 0.0008722411</span><br><span class="line">W = [0.10139273] b = [0.29906142] loss = 0.00087215187</span><br></pre></td></tr></table></figure></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/buv6SxKq3CxD.png?imageslim" alt="mark"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h1&gt;&lt;p&gt;线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://zhengyujie.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Python" scheme="http://zhengyujie.cn/categories/Python/"/>
    
    
      <category term="TensorFlow" scheme="http://zhengyujie.cn/tags/TensorFlow/"/>
    
      <category term="线性回归" scheme="http://zhengyujie.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
