<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>我的小破站正式上线了</title>
      <link href="/2019/08/23/%E6%9D%82%E8%AE%B0/"/>
      <url>/2019/08/23/%E6%9D%82%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>刚刚不久前收到阿里云的短信，通知我我的域名终于通过备案审核了，这个过程比我想象中的快了不少。我赶紧给我的服务器绑定了我的域名，然后将我的博客重新部署到服务器上，至此我的个人博客终于正式上线了。接下来该折腾让百度和谷歌收录我的博客了。</p><a id="more"></a><p>当然还是要感慨一下的，下面是我的小破站的一些记事</p><ul><li>2019年7月23日开始有了搭建一个博客的念头</li><li>2019年7月25日开始着手使用Hexo框架搭建博客</li><li>2019年7月25日下午博客搭建完成，上传至GitHub托管。网址：<a href="http://zhengyujie.github.io" target="_blank" rel="noopener">http://zhengyujie.github.io</a></li><li>2019年7月25日晚上上传第一篇博客</li><li>2019年8月13日将我的博客部署至Coding平台，网址：<a href="http://zhengyujie.coding.me" target="_blank" rel="noopener">http://zhengyujie.coding.me</a></li><li>2019年8月16日在阿里云购买了域名</li><li>2019年8月17日购买了阿里云学生主机</li><li>2019年8月17日下午将我的博客部署至我的阿里云主机</li><li>2019年8月17日开始进行域名备案</li><li>2019年8月23日域名备案审核通过，小破站正式上线，网址：<a href="http://zhengyujie.cn">http://zhengyujie.cn</a></li></ul><p>未来研究生三年我会坚持在博客上记录我学习和生活的点滴,希望我和小破站都能变得越来越优秀。</p>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小破站记事 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记12：CIFAR-10数据集图片分类</title>
      <link href="/2019/08/23/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/"/>
      <url>/2019/08/23/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B012/</url>
      
        <content type="html"><![CDATA[<h1 id="CIFAR-10数据集简介"><a href="#CIFAR-10数据集简介" class="headerlink" title="CIFAR-10数据集简介"></a>CIFAR-10数据集简介</h1><blockquote><p>官网链接：<a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">http://www.cs.toronto.edu/~kriz/cifar.html</a></p></blockquote><p> CIFAR-10是一个更接近普适物体的彩色图像数据集。CIFAR-10 是由Hinton 的学生Alex Krizhevsky 和Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含10个类别的RGB彩色图片：飞机（ airplane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。每个图片的尺寸为32 × 32 ，每个类别有6000个图像，数据集中一共有50000 张训练图片和10000 张测试图片。</p><a id="more"></a><p><img src="http://pwbhioup3.bkt.clouddn.com/CIFAR-10.png" alt="CIFAR-10"></p><h1 id="TensenFlow实现"><a href="#TensenFlow实现" class="headerlink" title="TensenFlow实现"></a>TensenFlow实现</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>首先去官方库下载<code>cifar10.py</code>以及<code>cifar10_input.py</code>文件来下载CIFAR-10数据集二进制文件，以及读取文件内容。</p><blockquote><p>GitHub地址：<a href="https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10" target="_blank" rel="noopener">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p></blockquote><p>运行下载数据集函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.models.tutorials.image.cifar10 <span class="keyword">import</span> cifar10</span><br><span class="line">cifar10.maybe_download_and_extract()</span><br></pre></td></tr></table></figure></p><p>数据集文件默认下载在<code>./tmp/cifar10_data</code>文件下，可以将其移动到自己的工程文件夹下</p><h2 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h2><p>首先定义一个权重初始化函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_with_weight_loss</span><span class="params">(shape,std,w1)</span>:</span></span><br><span class="line">    var = tf.Variable(tf.truncated_normal(shape,stddev=std),dtype=tf.float32)</span><br><span class="line">    <span class="keyword">if</span> w1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        weight_loss = tf.multiply(tf.nn.l2_loss(var),w1,name=<span class="string">"weight_loss"</span>)</span><br><span class="line">        tf.add_to_collection(<span class="string">"losses"</span>,weight_loss)</span><br><span class="line">    <span class="keyword">return</span> var</span><br></pre></td></tr></table></figure></p><p>然后定义一个损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_func</span><span class="params">(logits,labels)</span>:</span></span><br><span class="line">    labels = tf.cast(labels,tf.int32)</span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,</span><br><span class="line">                           labels=labels,name=<span class="string">"cross_entropy_per_example"</span>)</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(tf.reduce_sum(cross_entropy))</span><br><span class="line">    tf.add_to_collection(<span class="string">"losses"</span>,cross_entropy_mean)</span><br><span class="line">    <span class="keyword">return</span> tf.add_n(tf.get_collection(<span class="string">"losses"</span>),name=<span class="string">"total_loss"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置每次训练的数据大小</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment">#下载解压数据</span></span><br><span class="line"><span class="comment"># cifar10.maybe_download_and_extract()</span></span><br><span class="line"><span class="comment"># 设置数据的存放目录</span></span><br><span class="line">cifar10_dir = <span class="string">"cifar10_data/cifar-10-batches-bin"</span></span><br><span class="line"><span class="comment">#获取数据增强后的训练集数据</span></span><br><span class="line">images_train,labels_train = cifar10_input.distorted_inputs(cifar10_dir,batch_size)</span><br><span class="line"><span class="comment">#获取裁剪后的测试数据</span></span><br><span class="line">images_test,labels_test = cifar10_input.inputs(eval_data=<span class="literal">True</span>,data_dir=cifar10_dir,                     </span><br><span class="line">                                               batch_size=batch_size)</span><br></pre></td></tr></table></figure><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#定义模型的输入和输出数据</span></span><br><span class="line">image_holder = tf.placeholder(dtype=tf.float32,shape=[batch_size,<span class="number">24</span>,<span class="number">24</span>,<span class="number">3</span>])</span><br><span class="line">label_holder = tf.placeholder(dtype=tf.int32,shape=[batch_size])</span><br><span class="line"></span><br><span class="line"><span class="comment">#设计第一层卷积</span></span><br><span class="line">weight1 = variable_with_weight_loss(shape=[<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">64</span>],std=<span class="number">5e-2</span>,w1=<span class="number">0</span>)</span><br><span class="line">kernel1 = tf.nn.conv2d(image_holder,weight1,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">bais1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">64</span>]))</span><br><span class="line">conv1 = tf.nn.relu(tf.nn.bias_add(kernel1,bais1))</span><br><span class="line">pool1 = tf.nn.max_pool(conv1,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">norm1 = tf.nn.lrn(pool1,<span class="number">4</span>,bias=<span class="number">1.0</span>,alpha=<span class="number">0.001</span> / <span class="number">9</span>,beta=<span class="number">0.75</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设计第二层卷积</span></span><br><span class="line">weight2 = variable_with_weight_loss(shape=[<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">64</span>],std=<span class="number">5e-2</span>,w1=<span class="number">0</span>)</span><br><span class="line">kernel2 = tf.nn.conv2d(norm1,weight2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">bais2 = tf.Variable(tf.constant(<span class="number">0.1</span>,dtype=tf.float32,shape=[<span class="number">64</span>]))</span><br><span class="line">conv2 = tf.nn.relu(tf.nn.bias_add(kernel2,bais2))</span><br><span class="line">norm2 = tf.nn.lrn(conv2,<span class="number">4</span>,bias=<span class="number">1.0</span>,alpha=<span class="number">0.01</span> / <span class="number">9</span>,beta=<span class="number">0.75</span>)</span><br><span class="line">pool2 = tf.nn.max_pool(norm2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一层全连接层</span></span><br><span class="line">reshape = tf.reshape(pool2,[batch_size,<span class="number">-1</span>])</span><br><span class="line">dim = reshape.get_shape()[<span class="number">1</span>].value</span><br><span class="line">weight3 = variable_with_weight_loss([dim,<span class="number">384</span>],std=<span class="number">0.04</span>,w1=<span class="number">0.004</span>)</span><br><span class="line">bais3 = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[<span class="number">384</span>],dtype=tf.float32))</span><br><span class="line">local3 = tf.nn.relu(tf.matmul(reshape,weight3)+bais3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二层全连接层</span></span><br><span class="line">weight4 = variable_with_weight_loss([<span class="number">384</span>,<span class="number">192</span>],std=<span class="number">0.04</span>,w1=<span class="number">0.004</span>)</span><br><span class="line">bais4 = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[<span class="number">192</span>],dtype=tf.float32))</span><br><span class="line">local4 = tf.nn.relu(tf.matmul(local3,weight4)+bais4)</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后一层</span></span><br><span class="line">weight5 = variable_with_weight_loss([<span class="number">192</span>,<span class="number">10</span>],std=<span class="number">1</span>/<span class="number">192.0</span>,w1=<span class="number">0</span>)</span><br><span class="line">bais5 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>],dtype=tf.float32))</span><br><span class="line">logits = tf.add(tf.matmul(local4,weight5),bais5)</span><br></pre></td></tr></table></figure><h2 id="训练和优化"><a href="#训练和优化" class="headerlink" title="训练和优化"></a>训练和优化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置最大迭代次数</span></span><br><span class="line">max_steps = <span class="number">10000</span></span><br><span class="line"><span class="comment">#获取损失函数</span></span><br><span class="line">loss = loss_func(logits,label_holder)</span><br><span class="line"><span class="comment">#设置优化算法使得成本最小</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</span><br><span class="line"><span class="comment">#获取最高类的分类准确率，取top1作为衡量标准</span></span><br><span class="line">top_k_op = tf.nn.in_top_k(logits,label_holder,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(max_steps):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    images_batch,labels_batch = sess.run([images_train,labels_train])</span><br><span class="line">    _,loss_value = sess.run([train_step,loss],feed_dict=&#123;image_holder:images_batch,</span><br><span class="line">                                                            label_holder:labels_batch&#125;)</span><br><span class="line">    <span class="comment">#获取计算时间</span></span><br><span class="line">    duration = time.time() - start_time</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#计算每秒处理多少张图片</span></span><br><span class="line">        per_images_second = batch_size / duration</span><br><span class="line">        <span class="comment">#获取时间</span></span><br><span class="line">        sec_per_batch = float(duration)</span><br><span class="line">        print(<span class="string">"step:%d,duration:%.3f,per_images_second:%.2f,loss:%.3f"</span>%(step,duration</span><br><span class="line">              ,per_images_second,loss_value))</span><br></pre></td></tr></table></figure><h2 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算测试集上的准确率</span></span><br><span class="line">num_examples = <span class="number">10000</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">num_iter = int(math.ceil(num_examples / batch_size))</span><br><span class="line">true_count = <span class="number">0</span></span><br><span class="line">total_sample_count = num_iter * batch_size</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> step &lt; num_iter:</span><br><span class="line">    images_batch,labels_batch = sess.run([images_test,labels_test])</span><br><span class="line">    pred = sess.run([top_k_op],feed_dict=&#123;image_holder:images_batch,label_holder:labels_batch&#125;)</span><br><span class="line">    true_count += np.sum(pred)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"><span class="comment">#计算测试集的准确率</span></span><br><span class="line">precision = true_count / total_sample_count</span><br><span class="line">print(<span class="string">"test accuracy:%.3f"</span>%precision)</span><br></pre></td></tr></table></figure><h2 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu 16.04安装Pycharm</title>
      <link href="/2019/08/22/ubuntu%E5%AE%89%E8%A3%85pycharm/"/>
      <url>/2019/08/22/ubuntu%E5%AE%89%E8%A3%85pycharm/</url>
      
        <content type="html"><![CDATA[<p>记录Ubuntu 16.04下Pycharm的安装步骤</p><a id="more"></a><p>首先去官网下载最新的Pycharm安装包，官网地址：<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">http://www.jetbrains.com/pycharm/</a><br>这里我下载了社区版(Community)</p><p>下载完成后，右键压缩包，选择提取到此处</p><p>然后<code>cd</code>进入Pychram安装包下的<code>bin</code>文件夹，我们可以看到有一个名叫<code>pychram.sh</code>的文件。<br>在终端输入命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ./pycharm.sh</span><br></pre></td></tr></table></figure></p><p>由于我之前没有安装过Pycharm，所以直接选择<code>Do not import settings</code>即可，然后点击<code>OK</code>进入下一步操作。根据提示的内容进行勾选同意协议，然后点击continue进入下一步，大家可以点击don’t send进入下一步，选择喜欢的界面风格，然后再次点击next进入下一步，直到提示start启动程序。即可完成Pycharm的安装。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
          <category> Ubuntu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pycharm </tag>
            
            <tag> 软件安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo折腾日记之侧栏篇</title>
      <link href="/2019/08/22/hexo%E8%AE%BE%E7%BD%AE%E4%BE%A7%E6%A0%8F/"/>
      <url>/2019/08/22/hexo%E8%AE%BE%E7%BD%AE%E4%BE%A7%E6%A0%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>现在我相信兴趣是最好的学习动力了。作为一个对前段完全提不起兴趣的人，在见识到别人的博客有多好看之后，我居然对美化博客产生了强烈的兴趣。以至于在折腾的过程中浑然不知时间的流逝，有时候明明已经饥肠辘辘了却依然乐之不疲，连我自己都在怀疑自己是不是着了魔。在高强度折腾了将近一天后，我终于将我的博客侧栏打造成了我满意的样子。对于我这种毫无任何HTML，CSS等基础的人，在折腾过程中遇到了难以计数的bug，所以能有这样的成果已经是相当满足了。这篇博客记录我对侧栏进行魔改的整个过程。</p><a id="more"></a><h1 id="折腾过程"><a href="#折腾过程" class="headerlink" title="折腾过程"></a>折腾过程</h1><h2 id="改造思路"><a href="#改造思路" class="headerlink" title="改造思路"></a>改造思路</h2><p>我希望主页的侧栏以及浏览文章时的侧栏显示的内容是不同的。</p><ul><li>在主页时，博客侧栏要尽可能多地显示和我博客有关的各种信息，比如博主的信息，文章分类信息，文章标签信息等等。</li><li>在浏览文章时，侧栏的信息要加以精简，仅仅保留博主信息以及文章目录，同时在滚动页面时侧栏要固定在头部。</li></ul><p>由于NexT主题默认在任何页面侧栏在滚动到页面顶部时都固定不动。但我想在主页面时侧栏能够显示更多的信息，于是我的想法就是写一个<code>if</code>语句判断当前页面是否为浏览文章页面，若不是，则侧栏随页面滚动，并且尽可能多的显示信息。若是，则仅仅显示博主信息以及文章目录。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>首先打开<code>/next/layout/_macro</code>文件夹下的<code>sidebar.swig</code>文件，这就是我们博客侧栏的源代码了，我们开始进行魔改</p><p>首先开头第一段<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line-wrap"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-first"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-middle"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"sidebar-toggle-line sidebar-toggle-line-last"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>我们保持不动，然后接下来<code>aside</code>标签下的内容全部删除，是的你没听错，全部删除，我们重新实现。<br>首先先写个<code>if</code>语句来判断当前的页面<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;%- <span class="keyword">set</span> display_toc = theme.toc.enable and display_toc %&#125;</span><br><span class="line">&#123;%- <span class="keyword">if</span> not display_toc or toc(page.content).length &lt;= <span class="number">1</span> %&#125;</span><br><span class="line">...</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure></p><p>接着我们在if语句内部协商我们想在主页面侧栏现实的内容</p><ol><li><p>博主信息。包括头像，自我介绍以及附加博客的文章数，类别数以及标签数。这段代码在<code>next/layout/_partials/sidebar/</code>的<code>site-overview.swig</code>文件内有现成的，我们只要保留我们想显示的部分就好了</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"author-overview"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author motion-element"</span> itemprop=<span class="string">"author"</span> itemscope itemtype=<span class="string">"http://schema.org/Person"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> theme.avatar.url %&#125;</span><br><span class="line">        &lt;img <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-image"</span> itemprop=<span class="string">"image"</span></span><br><span class="line">            src=<span class="string">"&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span></span><br><span class="line">            alt=<span class="string">"&#123;&#123; author &#125;&#125;"</span>&gt;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">        &lt;p <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-name"</span> itemprop=<span class="string">"name"</span>&gt;&#123;&#123; author &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="site-description motion-element" itemprop="description"&gt;&#123;&#123; description &#125;&#125;&lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">    &#123;%- if theme.site_state %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;nav class="site-state motion-element"&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- if config.archive_dir != '/</span><span class="string">' and site.posts.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-posts"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.menu.archives %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.archives).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">            &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.archive_dir) &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.posts<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if site.categories.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set categoriesPageQuery = site.pages.find(&#123;type: '</span>categories<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasCategoriesPage = categoriesPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-categories"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasCategoriesPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.categories %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.categories).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.category_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set visibleCategories = 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- for cat in site.categories %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if cat.length %&#125;&#123;%- set visibleCategories += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleCategories &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.categories<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasCategoriesPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if site.tags.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set tagsPageQuery = site.pages.find(&#123;type: '</span>tags<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasTagsPage = tagsPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-tags"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasTagsPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.tags %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.tags).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.tag_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set visibleTags = 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- for tag in site.tags %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if tag.length %&#125;&#123;%- set visibleTags += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleTags &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.tags<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- if hasTagsPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &lt;/nav&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.social %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="links-of-author motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- for name, link in theme.social %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="links-of-author-item"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarURL = link.split('</span>||<span class="string">')[0] | trim %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if not (theme.social_icons.enable) or (not theme.social_icons.icons_only) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarText = name %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.social_icons.enable %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarIcon = '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-fw fa-' + link.split('||')[1] | trim | default('globe') + '"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;&#123; next_url(sidebarURL, sidebarIcon + sidebarText, &#123;title: name + '</span> &amp;rarr; <span class="string">' + sidebarURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="string">            &lt;/span&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.rss %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="feed-link motion-element"&gt;</span></span><br><span class="line"><span class="string">        &lt;a href="&#123;&#123; url_for(theme.rss) &#125;&#125;" rel="alternate"&gt;</span></span><br><span class="line"><span class="string">            &lt;i class="fa fa-rss"&gt;&lt;/i&gt;RSS</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.chat.enable and theme.chat.service !== '</span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="chat motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>chatra<span class="string">' and theme.chatra.enable %&#125;</span></span><br><span class="line"><span class="string">        &lt;a onclick="Chatra('</span>openChat<span class="string">', true)"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>tidio<span class="string">' and theme.tidio.enable %&#125;</span></span><br><span class="line"><span class="string">        &lt;a onclick="tidioChatApi.open();"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.icon %&#125;&lt;i class="fa fa-&#123;&#123; theme.chat.icon &#125;&#125;"&gt;&lt;/i&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;&#123; theme.chat.text &#125;&#125;</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="cc-license motion-element" itemprop="license"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- set ccLanguage = theme.creative_commons.language %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license === '</span>zero<span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccType = '</span>publicdomain/zero/<span class="number">1.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;% else %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccType = '</span>licenses/<span class="string">' + theme.creative_commons.license + '</span>/<span class="number">4.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccURL = '</span>https:<span class="comment">//creativecommons.org/' + ccType %&#125;</span></span><br><span class="line">        &#123;%- <span class="keyword">set</span> ccImage = '&lt;img src="' + url_for(theme.images + '/cc-' + theme.creative_commons.license + '.svg') + '" alt="Creative Commons"&gt;' %&#125;</span><br><span class="line">        &#123;&#123; next_url(ccURL, ccImage, &#123;<span class="attr">class</span>: <span class="string">'cc-opacity'</span>&#125;) &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>博客公告。内容就自由发挥了。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;card-announcement&quot;&gt;</span><br><span class="line">    &lt;div class=&quot;card-content&quot;&gt;</span><br><span class="line">        &lt;div class=&quot;item_headline&quot;&gt;</span><br><span class="line">        &lt;i class=&quot;fa fa-bullhorn card-announcement-animation&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;</span><br><span class="line">        &lt;span&gt;公告&lt;/span&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">        &lt;div class=&quot;announcement_content&quot;&gt;</span><br><span class="line">        谢谢你这么帅，这么漂亮还来看我的博客，如果喜欢的话记得收藏哦</span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure></li><li><p>博客信息。我主要就显示文章数量，博客运行天数，访问人数以及点击量四条信息。</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-overview"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;<span class="xml"><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-line-chart"</span> <span class="attr">aria-hidden</span>=<span class="string">"true"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="xml"><span class="tag">&lt;<span class="name">span</span>&gt;</span>网站资讯<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><span class="xml"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info"</span>&gt;</span><br><span class="line">        &lt;script&#123;&#123; pjax &#125;&#125; <span class="keyword">async</span> src=<span class="string">"https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-name"</span>&gt;文章数目&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="blogs-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/li&gt;</span></span><br><span class="line"><span class="regexp">        &lt;li class="blog-info-list"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="blog-info-name"&gt;运行天数&lt;/</span>span&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"days-count"</span> id=<span class="string">"sitedays"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span></span><br><span class="line">        &lt;script language=javascript&gt;</span><br><span class="line">            <span class="function"><span class="keyword">function</span> <span class="title">siteTime</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">                <span class="built_in">window</span>.setTimeout(<span class="string">"siteTime()"</span>, <span class="number">1000</span>);</span><br><span class="line">                <span class="keyword">var</span> seconds = <span class="number">1000</span>;</span><br><span class="line">                <span class="keyword">var</span> minutes = seconds * <span class="number">60</span>;</span><br><span class="line">                <span class="keyword">var</span> hours = minutes * <span class="number">60</span>;</span><br><span class="line">                <span class="keyword">var</span> days = hours * <span class="number">24</span>;</span><br><span class="line">                <span class="keyword">var</span> years = days * <span class="number">365</span>;</span><br><span class="line">                <span class="keyword">var</span> today = <span class="keyword">new</span> <span class="built_in">Date</span>();</span><br><span class="line">                <span class="keyword">var</span> todayYear = today.getFullYear();</span><br><span class="line">                <span class="keyword">var</span> todayMonth = today.getMonth()+<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">var</span> todayDate = today.getDate();</span><br><span class="line">                <span class="keyword">var</span> todayHour = today.getHours();</span><br><span class="line">                <span class="keyword">var</span> todayMinute = today.getMinutes();</span><br><span class="line">                <span class="keyword">var</span> todaySecond = today.getSeconds();</span><br><span class="line">                <span class="keyword">var</span> t1 = <span class="built_in">Date</span>.UTC(<span class="number">2019</span>,<span class="number">07</span>,<span class="number">26</span>,<span class="number">00</span>,<span class="number">00</span>,<span class="number">00</span>); <span class="comment">//北京时间2018-2-13 00:00:00</span></span><br><span class="line">                <span class="keyword">var</span> t2 = <span class="built_in">Date</span>.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);</span><br><span class="line">                <span class="keyword">var</span> diff = t2-t1;</span><br><span class="line">                <span class="keyword">var</span> diffYears = <span class="built_in">Math</span>.floor(diff/years);</span><br><span class="line">                <span class="keyword">var</span> diffDays = <span class="built_in">Math</span>.floor((diff/days)-diffYears*<span class="number">365</span>);</span><br><span class="line">                <span class="built_in">document</span>.getElementById(<span class="string">"sitedays"</span>).innerHTML=diffDays+<span class="string">" 天 "</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            siteTime();</span><br><span class="line">        &lt;<span class="regexp">/script&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;访问数&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="site-uv" title="&#123;&#123; __('footer.total_visitors') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">            &lt;span class="busuanzi-value" id="busuanzi_value_site_uv"&gt;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">        &lt;li <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-list"</span>&gt;</span><br><span class="line">        &lt;span <span class="class"><span class="keyword">class</span></span>=<span class="string">"blog-info-name"</span>&gt;点击量&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;span class="site-pv" title="&#123;&#123; __('footer.total_views') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">            &lt;span class="busuanzi-value" id="busuanzi_value_site_pv"&gt;&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>li&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>分类信息。</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-categoreus"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-folder-open"</span> aria-hidden=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">    &lt;span&gt;分类&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &lt;div&gt;</span><br><span class="line">    &#123;&#123; list_categories() &#125;&#125; </span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>标签云。GitHub地址：<a href="https://github.com/MikeCoder/hexo-tag-cloud" target="_blank" rel="noopener">https://github.com/MikeCoder/hexo-tag-cloud</a></p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-tags"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-tags"</span> aria-hidden=<span class="string">"true"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span></span><br><span class="line">    &lt;span&gt;标签云&lt;<span class="regexp">/span&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &#123;% <span class="keyword">if</span> site.tags.length &gt; <span class="number">1</span> %&#125;</span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span> charset=<span class="string">"utf-8"</span> src=<span class="string">"&#123;&#123; url_for('/js/tagcloud.js') &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">    &lt;script type=<span class="string">"text/javascript"</span> charset=<span class="string">"utf-8"</span> src=<span class="string">"&#123;&#123; url_for('/js/tagcanvas.js') &#125;&#125;"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"widget-wrap"</span>&gt;</span><br><span class="line">        &lt;div id=<span class="string">"myCanvasContainer"</span> <span class="class"><span class="keyword">class</span></span>=<span class="string">"widget tagcloud"</span>&gt;</span><br><span class="line">        &lt;canvas width=<span class="string">"250"</span> height=<span class="string">"250"</span> id=<span class="string">"resCanvas"</span> style=<span class="string">"width=100%"</span>&gt;</span><br><span class="line">            &#123;&#123; list_tags() &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/canvas&gt;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;% endif %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></li><li><p>友情链接</p> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"link-of-blogroll"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">    &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-handshake-o"</span> aria-hidden=<span class="string">"true"</span>&gt;</span><br><span class="line">    &lt;<span class="regexp">/i&gt;&lt;span&gt;友情链接&lt;/</span>span&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- if theme.links %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;div class="links-of-blogroll motion-element &#123;&#123; "links-of-blogroll-" + theme.links_layout | default('inline') &#125;&#125;"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="links-of-blogroll-title"&gt;</span></span><br><span class="line"><span class="regexp">        &lt;i class="fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;"&gt;&lt;/i</span>&gt;</span><br><span class="line">        &#123;&#123; theme.links_title &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &lt;ul class="links-of-blogroll-list"&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- for blogrollText, blogrollURL in theme.links %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;li class="links-of-blogroll-item"&gt;</span></span><br><span class="line"><span class="regexp">            &#123;&#123; next_url(blogrollURL, blogrollText, &#123;title: blogrollURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="regexp">            &lt;/</span>li&gt;</span><br><span class="line">        &#123;%- endfor %&#125;</span><br><span class="line">        &lt;<span class="regexp">/ul&gt;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>主页的侧栏就设置完成了。</p><p>接下来就是浏览文章时的侧栏的设计，这里就直接上我的代码了<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"sidebar-inner"</span>&gt; </span><br><span class="line">    &#123;%- <span class="keyword">if</span> display_toc and toc(page.content).length &gt; <span class="number">1</span> %&#125;</span><br><span class="line">    &lt;!--noindex--&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"author-overview"</span>&gt;</span><br><span class="line">        &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author motion-element"</span> itemprop=<span class="string">"author"</span> itemscope itemtype=<span class="string">"http://schema.org/Person"</span>&gt;</span><br><span class="line">            &#123;%- <span class="keyword">if</span> theme.avatar.url %&#125;</span><br><span class="line">            &lt;img <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-image"</span> itemprop=<span class="string">"image"</span></span><br><span class="line">                src=<span class="string">"&#123;&#123; url_for( theme.avatar.url | default(theme.images + '/avatar.gif') ) &#125;&#125;"</span></span><br><span class="line">                alt=<span class="string">"&#123;&#123; author &#125;&#125;"</span>&gt;</span><br><span class="line">            &#123;%- endif %&#125;</span><br><span class="line">            &lt;p <span class="class"><span class="keyword">class</span></span>=<span class="string">"site-author-name"</span> itemprop=<span class="string">"name"</span>&gt;&#123;&#123; author &#125;&#125;&lt;<span class="regexp">/p&gt;</span></span><br><span class="line"><span class="regexp">            &lt;div class="site-description motion-element" itemprop="description"&gt;&#123;&#123; description &#125;&#125;&lt;/</span>div&gt;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- if theme.site_state %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;nav class="site-state motion-element"&gt;</span></span><br><span class="line"><span class="regexp">            &#123;%- if config.archive_dir != '/</span><span class="string">' and site.posts.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-posts"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.archives %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(theme.menu.archives).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                &lt;a href="&#123;&#123; url_for(config.archive_dir) &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; site.posts.length &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.posts<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            &#123;%- if site.categories.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set categoriesPageQuery = site.pages.find(&#123;type: '</span>categories<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasCategoriesPage = categoriesPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-categories"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasCategoriesPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.categories %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(theme.menu.categories).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(config.category_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set visibleCategories = 0 %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- for cat in site.categories %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if cat.length %&#125;&#123;%- set visibleCategories += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleCategories &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.categories<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasCategoriesPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            &#123;%- if site.tags.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set tagsPageQuery = site.pages.find(&#123;type: '</span>tags<span class="string">'&#125;, &#123;lean: true&#125;) %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set hasTagsPage = tagsPageQuery.length &gt; 0 %&#125;</span></span><br><span class="line"><span class="string">            &lt;div class="site-state-item site-state-tags"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasTagsPage %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if theme.menu.tags %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(theme.menu.tags).split('</span>||<span class="string">')[0] | trim &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;% else %&#125;</span></span><br><span class="line"><span class="string">                    &lt;a href="&#123;&#123; url_for(config.tag_dir) + '</span>/<span class="string">' &#125;&#125;"&gt;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set visibleTags = 0 %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- for tag in site.tags %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- if tag.length %&#125;&#123;%- set visibleTags += 1 %&#125;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-count"&gt;&#123;&#123; visibleTags &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &lt;span class="site-state-item-name"&gt;&#123;&#123; __('</span>state.tags<span class="string">') &#125;&#125;&lt;/span&gt;</span></span><br><span class="line"><span class="string">                &#123;%- if hasTagsPage %&#125;&lt;/a&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &lt;/div&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &lt;/nav&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.social %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="links-of-author motion-element"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- for name, link in theme.social %&#125;</span></span><br><span class="line"><span class="string">            &lt;span class="links-of-author-item"&gt;</span></span><br><span class="line"><span class="string">            &#123;%- set sidebarURL = link.split('</span>||<span class="string">')[0] | trim %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if not (theme.social_icons.enable) or (not theme.social_icons.icons_only) %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set sidebarText = name %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- if theme.social_icons.enable %&#125;</span></span><br><span class="line"><span class="string">                &#123;%- set sidebarIcon = '</span>&lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-fw fa-' + link.split('||')[1] | trim | default('globe') + '"</span>&gt;<span class="xml"><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">                &#123;&#123; next_url(sidebarURL, sidebarIcon + sidebarText, &#123;title: name + '</span> &amp;rarr; <span class="string">' + sidebarURL&#125;) &#125;&#125;</span></span><br><span class="line"><span class="string">            &lt;/span&gt;</span></span><br><span class="line"><span class="string">            &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.rss %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="feed-link motion-element"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="&#123;&#123; url_for(theme.rss) &#125;&#125;" rel="alternate"&gt;</span></span><br><span class="line"><span class="string">            &lt;i class="fa fa-rss"&gt;&lt;/i&gt;RSS</span></span><br><span class="line"><span class="string">            &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.enable and theme.chat.service !== '</span><span class="string">' %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="chat motion-element"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>chatra<span class="string">' and theme.chatra.enable %&#125;</span></span><br><span class="line"><span class="string">            &lt;a onclick="Chatra('</span>openChat<span class="string">', true)"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.service == '</span>tidio<span class="string">' and theme.tidio.enable %&#125;</span></span><br><span class="line"><span class="string">            &lt;a onclick="tidioChatApi.open();"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.chat.icon %&#125;&lt;i class="fa fa-&#123;&#123; theme.chat.icon &#125;&#125;"&gt;&lt;/i&gt;&#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">            &#123;&#123; theme.chat.text &#125;&#125;</span></span><br><span class="line"><span class="string">        &lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license and theme.creative_commons.sidebar %&#125;</span></span><br><span class="line"><span class="string">        &lt;div class="cc-license motion-element" itemprop="license"&gt;</span></span><br><span class="line"><span class="string">        &#123;%- set ccLanguage = theme.creative_commons.language %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- if theme.creative_commons.license === '</span>zero<span class="string">' %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set ccType = '</span>publicdomain/zero/<span class="number">1.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;% else %&#125;</span></span><br><span class="line"><span class="string">            &#123;%- set ccType = '</span>licenses/<span class="string">' + theme.creative_commons.license + '</span>/<span class="number">4.0</span>/<span class="string">' + ccLanguage %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="string">        &#123;%- set ccURL = '</span>https:<span class="comment">//creativecommons.org/' + ccType %&#125;</span></span><br><span class="line">        &#123;%- <span class="keyword">set</span> ccImage = '&lt;img src="' + url_for(theme.images + '/cc-' + theme.creative_commons.license + '.svg') + '" alt="Creative Commons"&gt;' %&#125;</span><br><span class="line">            &#123;&#123; next_url(ccURL, ccImage, &#123;<span class="attr">class</span>: <span class="string">'cc-opacity'</span>&#125;) &#125;&#125;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;/</span>div&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-toc-wrap motion-element sidebar-panel sidebar-panel-active"</span> id=<span class="string">"post-toc-wrap"</span>&gt;</span><br><span class="line">        &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"item_headline"</span>&gt;</span><br><span class="line">        &lt;i <span class="class"><span class="keyword">class</span></span>=<span class="string">"fa fa-list"</span> aria-hidden=<span class="string">"true"</span>&gt;</span><br><span class="line">        &lt;<span class="regexp">/i&gt;&lt;span&gt;目录&lt;/</span>span&gt;</span><br><span class="line">        &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &lt;div class="post-toc"&gt;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;%- set next_toc_number = theme.toc.number %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- if page.toc_number !== undefined %&#125;</span></span><br><span class="line"><span class="regexp">            &#123;%- set next_toc_number = page.toc_number %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- set next_toc_max_depth = page.toc_max_depth|default(theme.toc.max_depth)|default(6) %&#125;</span></span><br><span class="line"><span class="regexp">        &#123;%- set toc = toc(page.content, &#123; "class": "nav", list_number: next_toc_number, max_depth: next_toc_max_depth &#125;) %&#125;</span></span><br><span class="line"><span class="regexp"></span></span><br><span class="line"><span class="regexp">        &#123;%- if toc.length &lt;= 1 %&#125;</span></span><br><span class="line"><span class="regexp">            &lt;p class="post-toc-empty"&gt;&#123;&#123; __('post.toc_empty') &#125;&#125;&lt;/</span>p&gt;</span><br><span class="line">        &#123;% <span class="keyword">else</span> %&#125;</span><br><span class="line">            &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-toc-content"</span>&gt;&#123;&#123; toc &#125;&#125;&lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">        &#123;%- endif %&#125;</span></span><br><span class="line"><span class="regexp">        &lt;/</span>div&gt;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">    &lt;!--/</span>noindex--&gt;</span><br><span class="line">    &#123;%- endif %&#125;</span><br><span class="line">&lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></p><p>之后就可以自由发挥添加CSS样式了。</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
            <tag> JavaScript </tag>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo折腾日记之标签篇</title>
      <link href="/2019/08/20/hexo%E5%BD%A9%E8%89%B2%E6%A0%87%E7%AD%BE/"/>
      <url>/2019/08/20/hexo%E5%BD%A9%E8%89%B2%E6%A0%87%E7%AD%BE/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天又耗费一个上午给我的博客折腾出了彩色标签，接下来记录我的设置过程</p><a id="more"></a><h1 id="page-swig文件设置"><a href="#page-swig文件设置" class="headerlink" title="page.swig文件设置"></a>page.swig文件设置</h1><p>首先先打开<code>/next/layout</code>文件夹下的page.swig文件，在文件中找到以下内容<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud"</span>&gt;</span><br><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud-title"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">set</span> visibleTags = 0 %&#125;</span><br><span class="line">        &#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> site.tags %&#125;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> tag.length %&#125;</span><br><span class="line">            &#123;%- <span class="keyword">set</span> visibleTags += 1 %&#125;</span><br><span class="line">        &#123;%- endif %&#125;</span><br><span class="line">        &#123;%- endfor %&#125;</span><br><span class="line">        &#123;&#123; _p(<span class="string">'counter.tag_cloud'</span>, visibleTags) &#125;&#125;</span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br></pre></td></tr></table></figure></p><p>接下来的就是我们标签页面的主要代码<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    &lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"tag-cloud-tags"</span> id=<span class="string">"tags"</span>&gt;</span><br><span class="line">        &#123;%- <span class="keyword">if</span> not theme.tagcloud %&#125;</span><br><span class="line">        &#123;&#123; tagcloud(&#123;<span class="attr">min_font</span>: <span class="number">12</span>, <span class="attr">max_font</span>: <span class="number">30</span>, <span class="attr">amount</span>: <span class="number">200</span>, <span class="attr">color</span>: <span class="literal">true</span>, <span class="attr">start_color</span>: <span class="string">'#fff'</span>, <span class="attr">end_color</span>: <span class="string">'#fff'</span>&#125;) &#125;&#125;</span><br><span class="line">        &#123;% <span class="keyword">else</span> %&#125;</span><br><span class="line">        &#123;&#123; tagcloud(&#123;<span class="attr">min_font</span>: theme.tagcloud.min, <span class="attr">max_font</span>: theme.tagcloud.max, <span class="attr">amount</span>: theme.tagcloud.amount, <span class="attr">color</span>: <span class="literal">true</span>, <span class="attr">start_color</span>: theme.tagcloud.start, <span class="attr">end_color</span>: theme.tagcloud.end&#125;) &#125;&#125;</span><br><span class="line">        &#123;%- endif %&#125;             </span><br><span class="line">    &lt;<span class="regexp">/div&gt;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></p><p>这里我们已经将起始颜色和终止颜色都换成了<code>#fff</code>即纯白色，接下来我们就开始随机设置每个标签的背景颜色。<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line">    <span class="keyword">var</span> alltags=<span class="built_in">document</span>.getElementById(<span class="string">'tags'</span>);</span><br><span class="line">    <span class="keyword">var</span> tags=alltags.getElementsByTagName(<span class="string">'a'</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> i = tags.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">var</span> r=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">130</span>);</span><br><span class="line">        <span class="keyword">var</span> g=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">var</span> b=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">80</span>);</span><br><span class="line">        tags[i].style.background = <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure></p><p>然后我们再调整标签的CSS样式即可，这里附上我的CSS样式<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.tag-cloud</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">0px</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0px</span> <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">10px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="post-swig文件设置"><a href="#post-swig文件设置" class="headerlink" title="post.swig文件设置"></a>post.swig文件设置</h1><p>接下来我们设置主页的标签，首先NexT主题首页的文章预览里没有标签，所以我们要先在文章预览里添加标签。打开<code>/home/zyj/blog/themes/next/layout/_macro</code>下的post.swig文件，找到<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;% elif post.excerpt %&#125;</span><br><span class="line">    &#123;&#123; post.excerpt &#125;&#125;</span><br></pre></td></tr></table></figure></p><p>在其后面添加<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;%- <span class="keyword">if</span> post.tags and post.tags.length %&#125;</span><br><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"home-post-tags"</span>&gt;</span><br><span class="line">    &#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> post.tags %&#125;</span><br><span class="line">    &lt;a href=<span class="string">"&#123;&#123; url_for(tag.path) &#125;&#125;"</span> rel=<span class="string">"tag"</span>&gt;&#123;&#123; tag.name &#125;&#125;&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">    &#123;%- endfor %&#125;</span></span><br><span class="line"><span class="regexp">    &lt;script type="text/</span>javascript<span class="string">"&gt;</span></span><br><span class="line"><span class="string">    var tagsall=document.getElementsByClassName("</span>home-post-tags<span class="string">")</span></span><br><span class="line"><span class="string">    for (var i = tagsall.length - 1; i &gt;= 0; i--)&#123;</span></span><br><span class="line"><span class="string">        var tags=tagsall[i].getElementsByTagName("</span>a<span class="string">");</span></span><br><span class="line"><span class="string">        for (var j = tags.length - 1; j &gt;= 0; j--) &#123;</span></span><br><span class="line"><span class="string">            var r=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            var g=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            var b=Math.floor(Math.random()*255);</span></span><br><span class="line"><span class="string">            tags[j].style.background = "</span>rgb(<span class="string">"+r+"</span>,<span class="string">"+g+"</span>,<span class="string">"+b+"</span>)<span class="string">";</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &lt;/script&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#123;%- endif %&#125;</span></span><br></pre></td></tr></table></figure></p><p>即可在文章预览中添加彩色标签</p><p>接下来将文章主页面末尾的标签换成彩色标签，同样在post.swig中找到：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;div <span class="class"><span class="keyword">class</span></span>=<span class="string">"post-tags"</span> id=<span class="string">"post-tags"</span>&gt;</span><br><span class="line">&#123;%- <span class="keyword">for</span> tag <span class="keyword">in</span> post.tags %&#125;</span><br><span class="line">    &lt;a href=<span class="string">"&#123;&#123; url_for(tag.path) &#125;&#125;"</span> rel=<span class="string">"tag"</span>&gt;&#123;&#123; tag_indicate &#125;&#125; &#123;&#123; tag.name &#125;&#125;&lt;<span class="regexp">/a&gt;</span></span><br><span class="line"><span class="regexp">&#123;%- endfor %&#125;</span></span><br><span class="line"><span class="regexp">&lt;/</span>div&gt;</span><br></pre></td></tr></table></figure></p><p>在其后添加<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;script type=<span class="string">"text/javascript"</span>&gt;</span><br><span class="line"><span class="keyword">var</span> tagsall=<span class="built_in">document</span>.getElementsByClassName(<span class="string">"post-tags"</span>)</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">var</span> i = tagsall.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">    <span class="keyword">var</span> tags=tagsall[i].getElementsByTagName(<span class="string">"a"</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> j = tags.length - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">        <span class="keyword">var</span> r=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">130</span>);</span><br><span class="line">        <span class="keyword">var</span> g=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">var</span> b=<span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random()*<span class="number">75</span>+<span class="number">80</span>);</span><br><span class="line">        tags[j].style.background = <span class="string">"rgb("</span>+r+<span class="string">","</span>+g+<span class="string">","</span>+b+<span class="string">")"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;                        </span><br><span class="line">&lt;<span class="regexp">/script&gt;</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
            <tag> JavaScript </tag>
            
            <tag> HTML </tag>
            
            <tag> CSS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo添加Valine评论功能</title>
      <link href="/2019/08/18/valine%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/"/>
      <url>/2019/08/18/valine%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一直想给自己的博客添加评论功能，比较靠谱的评论系统有GitHub上的gitment，gitalk，gitter，来必力(有点儿花哨)，以及一些国外的评论系统(需要翻墙)。偶然间了解到一款国人开发的评论系统叫Valine用的是LeanCloud作为数据库，洁面很简洁，不像来必力那样花哨只是单纯的评论，简洁到没有后台，删除和管理评论直接操作数据库，于是决定就是你了。本片博客用于记录添加Valine评论功能的完整过程。</p><a id="more"></a><h1 id="添加Valine评论"><a href="#添加Valine评论" class="headerlink" title="添加Valine评论"></a>添加Valine评论</h1><ol><li><p>注册LeanCloud</p></li><li><p>创建一个开发版应用</p></li><li><p>创建 Class<br> 在LeanCloud -&gt; 存储 -&gt; 创建Class -&gt; 无限制的Class</p></li><li>关闭服务开关<br> 在LeanCloud -&gt; 设置 -&gt; 安全中心 -&gt; 服务开关，把除数据存储其他选项都关闭。</li><li>添加域名<br> 在LeanCloud -&gt; 设置 -&gt; 安全中心 -&gt; Wwb安全域名，注意格式</li><li><p>修改主题配置文件<br> 打开主题配置文件<code>_config.yml</code>，搜索找到<code>Valine</code>部分，填写appid以及appkey</p> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Valine</span></span><br><span class="line"><span class="comment"># You can get your appid and appkey from https://leancloud.cn</span></span><br><span class="line"><span class="comment"># For more information: https://valine.js.org, https://github.com/xCss/Valine</span></span><br><span class="line"><span class="attr">valine:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span> <span class="comment"># When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version</span></span><br><span class="line"><span class="attr">appid:</span> <span class="comment"># your appid</span></span><br><span class="line"><span class="attr">appkey:</span> <span class="comment"># your appkey</span></span><br><span class="line"><span class="attr">notify:</span> <span class="literal">true</span> <span class="comment"># Mail notifier. See: https://github.com/xCss/Valine/wiki</span></span><br><span class="line"><span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line"><span class="attr">placeholder:</span> <span class="string">Just</span> <span class="string">go</span> <span class="string">go</span> <span class="comment"># Comment box placeholder</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">mm</span> <span class="comment"># Gravatar style</span></span><br><span class="line"><span class="attr">guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># Custom comment header</span></span><br><span class="line"><span class="attr">pageSize:</span> <span class="number">10</span> <span class="comment"># Pagination size</span></span><br><span class="line"><span class="attr">language:</span> <span class="comment"># Language, available values: en, zh-cn</span></span><br><span class="line"><span class="attr">visitor:</span> <span class="literal">false</span> <span class="comment"># leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors' for counter compatibility. Article reading statistic https://valine.js.org/visitor.html</span></span><br><span class="line"><span class="attr">comment_count:</span> <span class="literal">true</span> <span class="comment"># If false, comment count will only be displayed in post page, not in home page</span></span><br></pre></td></tr></table></figure></li><li><p>开启email通知<br> 若要开启邮件通知，首先修改主题配置文件：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">notify: true # Mail notifier. See: https://github.com/xCss/Valine/wiki</span><br></pre></td></tr></table></figure><p> 然后在LeanCloud -&gt; 设置 -&gt; 邮件模板<br> 用于重置密码的邮件主题修改为：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你在&#123;&#123;appname&#125;&#125; 的评论收到了新的回复</span><br></pre></td></tr></table></figure><p> 内容修改为：</p> <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>Hi, &#123;&#123;username&#125;&#125;<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line">你在 &#123;&#123;appname&#125;&#125; 的评论收到了新的回复，请点击查看：</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"你的网址首页链接"</span> <span class="attr">style</span>=<span class="string">"display: inline-block; padding: 10px 20px; border-radius: 4px; background-color: #3090e4; color: #fff; text-decoration: none;"</span>&gt;</span>马上查看<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>最后欢迎大家在下方评论区测试评论</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
            <tag> Valine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客部署到阿里云服务器</title>
      <link href="/2019/08/17/%E9%83%A8%E7%BD%B2%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2019/08/17/%E9%83%A8%E7%BD%B2%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>这几天一折腾起来就发现完全停不下来了，前几天我把我的博客部署到了Coding平台，前天又手痒在阿里云上购买了一个域名。今早又没忍住购买了阿里云的学生服务器。于是我最终决定把我的博客部署到阿里云服务器上。在折腾了一个上午后，我终于成功的完成了博客的部署，这篇博客记录下我的整个部署流程。</p><a id="more"></a><h1 id="服务器环境搭建"><a href="#服务器环境搭建" class="headerlink" title="服务器环境搭建"></a>服务器环境搭建</h1><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><p>这没啥好说的：</p><pre><code>apt-get install git</code></pre><h2 id="配置Nginx"><a href="#配置Nginx" class="headerlink" title="配置Nginx"></a>配置Nginx</h2><ul><li><p>安装nginx</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install nginx</span><br></pre></td></tr></table></figure></li><li><p>nginx</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service start nginx</span><br></pre></td></tr></table></figure><p>  然后在浏览器输入服务器的公网地址，就可以看见nginx的默认页面。</p><p>  <strong>注意:</strong> 这里有个超级大坑，一定要在服务器的安全组规则中添加80端口，否则不会有任何输出。这里耗费了我一个多小时的时间，说多了都是泪。</p></li><li><p>默认配置<br>  <code>cd</code>进入nginx的配置文件目录，然后使用<code>vim</code>打开文件</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/nginx/sites-available</span><br><span class="line">sudo vim default</span><br></pre></td></tr></table></figure><p>  其中<code>server_name</code>修改为自己的域名，没有域名则无需修改。<br>  <code>root</code>修改为<code>/var/www/html/blog</code>，这是我们博客网站的根目录。</p></li></ul><h2 id="创建git用户"><a href="#创建git用户" class="headerlink" title="创建git用户"></a>创建git用户</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">adduser git</span><br><span class="line">chmod 740 /etc/sudoers</span><br><span class="line">vim /etc/sudoers</span><br></pre></td></tr></table></figure><p>找到以下内容<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">## Allow root to run any commands anywhere</span><br><span class="line">root    ALL=(ALL)    ALL</span><br></pre></td></tr></table></figure></p><p>在下面添加一行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git ALL=(ALL) ALL</span><br></pre></td></tr></table></figure></p><p>获得root权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd git</span><br></pre></td></tr></table></figure></p><h2 id="设置SSH"><a href="#设置SSH" class="headerlink" title="设置SSH"></a>设置SSH</h2><p>切换为git用户，创建 ~/.ssh 文件夹和 ~/.ssh/authorized_keys 文件，并赋予相应的权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su git</span><br><span class="line">mkdir ~/.ssh</span><br><span class="line">vim ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>然后将客户端<code>.ssh</code>文件夹下的<code>id_rsa.pub</code>文件里的内容复制到<code>authorized_keys</code>中，接着赋予相应的权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 600 ~/.ssh/authorzied_keys</span><br><span class="line">chmod 700 ~/.ssh</span><br></pre></td></tr></table></figure></p><p>然后我们在客户端终端上输入<code>ssh -v git@ip地址</code>就可以免密登录了</p><h2 id="git仓库设置"><a href="#git仓库设置" class="headerlink" title="git仓库设置"></a>git仓库设置</h2><p>切换到git用户，然后再服务器上初始化一个git裸库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su git</span><br><span class="line">cd ~</span><br><span class="line">git init --bare blog.git</span><br></pre></td></tr></table></figure></p><p>接着新建一个post-receive文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/blog.git/hooks/post-receive</span><br></pre></td></tr></table></figure></p><p>在文件中输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#！/bin/sh</span><br><span class="line">git --work-tree=/var/www/html/blog --git-dir=/home/git/blog.git checkout -f</span><br></pre></td></tr></table></figure></p><p>保存退出后再赋予该文件执行权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x ~/blog.git/hooks/post-receive</span><br></pre></td></tr></table></figure></p><h1 id="本地设置"><a href="#本地设置" class="headerlink" title="本地设置"></a>本地设置</h1><p>这里nodejs,npm,git,hexo等的安装就不再叙述了。本地的设置就很简单，只需要修改博客配置文件即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">    aliyun: git@ip地址:/home/git/blog.git,master</span><br></pre></td></tr></table></figure></p><p>然后我们在终端执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><p>博客文件就会上传到我们在服务器上的git仓库，然后再部署到我们创建的博客根目录。我们在浏览器中访问服务器地址，就可以看到我们的博客了。</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
          <category> Ubuntu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Ubuntu </tag>
            
            <tag> SSH </tag>
            
            <tag> 阿里云 </tag>
            
            <tag> Git </tag>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu深度学习环境搭建</title>
      <link href="/2019/08/16/ubuntu%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/"/>
      <url>/2019/08/16/ubuntu%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>上次搭建深度学习环境已经是很久以前了，但我还记得被其支配的恐惧，各种出错，各种版本对应不上，搞得我头皮发麻。这次因为系统奔溃需要重装系统，也就意味着我需要重新搭建Ubuntu的深度学习环境，我心里是一百个不愿意的。但也没有办法，只能硬着头皮上。但出乎意料，这次搭建过程还算顺利，只有在tensorflow运行出了点小状况(后面会提到)。本篇博客会详细记录我的整个搭建过程，说不定那天我的系统又被我玩崩溃了。先附上我的深度学习环境基本信息：</p><ul><li>Ubuntu: 16.04</li><li>NVIDIA: GeForce GTX 1080TI</li><li>Driver Version: 430.40</li><li>CUDA: 10.0</li><li>cuDNN: 7.5.1</li><li>TensorFlow: 1.13.1</li></ul><a id="more"></a><h1 id="安装显卡驱动"><a href="#安装显卡驱动" class="headerlink" title="安装显卡驱动"></a>安装显卡驱动</h1><h2 id="禁用nouveau"><a href="#禁用nouveau" class="headerlink" title="禁用nouveau"></a>禁用nouveau</h2><p>打开终端输入：</p><pre><code>sudo vim /etc/modprobe.d/blacklist.conf</code></pre><p>在文件最后加上：</p><pre><code>blacklist nouveauoptions nouveau modeset=0</code></pre><p>保存退出后输入命令：</p><pre><code>sudo update-initramfs -u</code></pre><p>然后重启电脑，接着输入命令：</p><pre><code>lsmod | grep nouveau</code></pre><p>若没有输出信息，说明禁用成功</p><h2 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h2><p>首先去NVIDIA官网上下载对应的Linux版本的显卡驱动。</p><p>然后按Ctrl+Alt+F1进入命令行模式，输入用户名和密码</p><p>接着关闭图形界面：</p><pre><code>sudo service lightdm stop</code></pre><p>然后<code>cd</code>进去驱动所在的文件夹，获取权限：</p><pre><code>sudo chmod a+x NVIDIA-Linux-x86_64-xxx.run</code></pre><p>安装驱动：</p><pre><code>sudo ./NVIDIA-Linux-x86_64-410.78.run –no-x-check –no-nouveau-check –no-opengl-files</code></pre><ul><li>–no-x-check 安装驱动时关闭X服务</li><li>–no-nouveau-check 安装驱动时禁用nouveau</li><li>–no-opengl-files 只安装驱动文件，不安装OpenGL文件；安装了，如果是双显卡的话，会出现重复登录的问题</li></ul><p>完成安装后启动图形界面：</p><pre><code>sudo service lightdm start</code></pre><p>最后在终端输入：</p><pre><code>nvidia-smi</code></pre><p>若有输出显卡信息，说明驱动安装成功</p><h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>下载CUDA10.0，下载地址：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p><code>cd</code>进入CUDA所在文件夹，赋予文件执行权限后安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod +x cuda_10.0.130_410.48_linux.run</span><br><span class="line">sudo ./cuda_10.0.130_410.48_linux.run</span><br></pre></td></tr></table></figure></p><p>开始安装后需要按空格键阅读条款，时间比较长，等不及的可以直接Ctrl+C跳过。阅读完使用条款后开始配置，一步一步慢慢来<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">accept/decline/quit: accept</span><br><span class="line"></span><br><span class="line">Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 410.48?</span><br><span class="line">(y)es/(n)o/(q)uit: n（这里不要再安装驱动了！！！）</span><br><span class="line"></span><br><span class="line">Install the CUDA 10.0 Toolkit?（是否安装CUDA 10 ，这里必须要安装）</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Enter Toolkit Location（安装路径，使用默认，直接回车就行）</span><br><span class="line"> [ default is /usr/local/cuda-10.0 ]:  </span><br><span class="line"></span><br><span class="line">Do you want to install a symbolic link at /usr/local/cuda?（同意创建软链接）</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Install the CUDA 10.0 Samples?</span><br><span class="line">(y)es/(n)o/(q)uit: y</span><br><span class="line"></span><br><span class="line">Installing the CUDA Toolkit in /usr/local/cuda-10.0 ...（开始安装）</span><br></pre></td></tr></table></figure></p><p>安装完成后加入环境变量</p><pre><code>sudo gedit ~/.bashrc</code></pre><p>最后一行加入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line">export PATH=$PATH:$CUDA_HOME/bin</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure></p><p>保存退出后输入：</p><pre><code>source .bashrc</code></pre><p>最后终端输入：</p><pre><code>nvcc --version</code></pre><p>若输出CUDA版本信息说明安装成功</p><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><p>从 <a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">https://developer.nvidia.com/cudnn</a> 上下载cudnn相应版本的压缩包（需要注册或登录）。</p><p>解压当前的.tgz格式的软件包</p><pre><code>tar -xzvf cudnn-10.0-linux-x64-v7.5.1.10.tgz</code></pre><p>解压后的文件夹名为cuda，文件夹中包含两个文件夹：一个为include，另一个为lib64。最好把这个cuda文件夹放在home目录下，方便操作。</p><p>将解压后的lib64文件夹关联到环境变量中：</p><pre><code>sudo gedit ~/.bashrc</code></pre><p>最后一行加入：</p><pre><code>export LD_LIBRARY_PATH=/home/cuda/lib64:$LD_LIBRARY_PATH</code></pre><p>保存退出后输入：</p><pre><code>source .bashrc</code></pre><p>最后一步就是将解压后的/home/cuda/include目录下的一些文件拷贝到/usr/local/cuda/include中。由于进入了系统路径，因此执行该操作时需要获取管理员权限：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p><p>完成cuDNN的配置</p><h2 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h2><p>这步比较简单，直接使用pip命令安装即可(我安装的是1.13.1版本)：</p><pre><code>pip install tensorflow-gpu==1.13.1</code></pre><p>然后进入交互式命令行看看是否安装成功：</p><pre><code>import tensorflow as tf</code></pre><p>接着就突然出现了FutureWarning警告信息，吓出我一身冷汗。我赶紧去网上查阅相关资料，看到有人说是因为numpy的版本是1.17太高了。我赶紧查看了我安装的numpy的版本，果然是1.17。于是我进行了降级安装，安装了1.16.4版本的numpy，最终问题得以完美解决。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Ubuntu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> Ubuntu </tag>
            
            <tag> CUDA </tag>
            
            <tag> cuDNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows下安装Ubuntu16.04双系统</title>
      <link href="/2019/08/16/%E5%AE%89%E8%A3%85Ubuntu%E7%B3%BB%E7%BB%9F/"/>
      <url>/2019/08/16/%E5%AE%89%E8%A3%85Ubuntu%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>昨天在更新tensorflow时我的Ubuntu系统突然崩溃了，无奈只能重装系统。但距离上次安装Ubuntu系统已经过去许久，只能去网上搜教程折腾，好在整个过程异常顺利，于是用本博客记录下我的整个安装步骤。</p><a id="more"></a><h1 id="完整步骤"><a href="#完整步骤" class="headerlink" title="完整步骤"></a>完整步骤</h1><ol><li><p><strong>空间准备</strong></p><p> 在Windows 10中打开“磁盘管理器”，找一个空闲的磁盘分区，压缩出来一部分空间给Ubuntu使用，压缩出来的硬盘应处于未分配状态。或者通过删除某个不使用的本地磁盘使其处于未分配状态。这里我给Ubuntu系统分配了100G空间</p></li><li><p><strong>制作启动盘</strong></p><ol><li>插入用来制作启动盘的U盘（会被格式化，请备份好重要文件），打开UltraISO刻录软件（免费无限期试用）。</li><li>选择“文件(F)”-&gt;“打开”，找到“Ubuntu-16.04-desktop-amd64.iso”镜像文件，然后点击“打开”。</li><li>选择“启动(B)”-&gt;“写入硬盘映像”，打开启动盘制作界面。</li><li>然后点击下方的“写入”，会弹出警告提示，确定后，就会开始制作启动盘。写入完成后关闭UltraISO软件即可。</li></ol></li><li><p><strong>BIOS设置</strong></p><p> 我的主板为微星主板，重启电脑猛按DEL键即可进入BIOS界面，然后设置U盘为第一启动项。最后按F10保存设置并重启。</p></li><li><p><strong>安装Ubuntu系统</strong></p><ol><li>重启后便进入安装界面，通过上下键选择则第二项：<code>Install Ubuntu</code></li><li>选择语言</li><li><strong>不要</strong>选择为图形或者无线硬件，以及Mp3和其他媒体安装第三方软件。</li><li>选择最后一项：其它选项</li><li>选中我们预留好的100G空间，然后点击+号新建分区</li><li>新建分区：<ol><li>大小一般为8G</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于交换空间</li><li>挂载点/swap</li></ol></li><li>新建分区：<ol><li>剩余空间的一半分给它，我分了50G</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于EXT4日志文件系统</li><li>挂载点/home</li></ol></li><li>新建分区：<ol><li>大小剩下的空间都给它</li><li>新分区类型为主分区</li><li>新分区位置为空间起始位置</li><li>用于EXT4日志文件系统</li><li>挂载点/</li></ol></li><li>设置安装引导的启动设置，在下拉中选择Windows Boot Manager所在的整块硬盘（不带数字标号）</li><li>点击继续</li><li>选择Shanghai，定位时区</li><li>键盘布局中英都行，以后可以改</li><li>设置用户名以及密码</li><li>等待进度条结束即可完成安装</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> Ubuntu </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> 系统安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo部署博客到Coding</title>
      <link href="/2019/08/13/%E9%83%A8%E7%BD%B2%E5%88%B0coding/"/>
      <url>/2019/08/13/%E9%83%A8%E7%BD%B2%E5%88%B0coding/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前我的博客部署在GitHub上，访问速度有点揪心，于是萌生了把我的博客部署在Coding上的想法。正好今天下午想偷个懒，于是马上查资料开始捣鼓。在耗费了我快一个半小时的时间后，终于成功的将我的博客部署在Coding上。用这篇博客记录下我整个折腾过程。</p><a id="more"></a><h1 id="Coding平台介绍"><a href="#Coding平台介绍" class="headerlink" title="Coding平台介绍"></a>Coding平台介绍</h1><p>Coding 是基于云计算技术的软件开发平台，集项目管理、代码托管、运行空间、质量控制为一体。在云计算时代，Coding推动软件开发的云端化，使开发者能用一个浏览器完成开发的各个环节。开发人员可以专心构建业务问题的解决方案，而非管理运营或发布堆栈，确保应用满足产品层目标服务等级，同时更为企业层级的项目应用提供了代码质量检验以及项目质量把控的渠道和标准。在保证私有项目的数据安全和稳定的同时，Coding 还结合了冒泡及评论、公开项目发布与讨论等一系列社交化协作功能，打造具有技术支撑的开发者社区。——摘自百度百科</p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>关于git,node.js,hexo等安装步骤这里不再赘述。</p><h2 id="Coding平台设置"><a href="#Coding平台设置" class="headerlink" title="Coding平台设置"></a>Coding平台设置</h2><ol><li>注册Coding<br> <strong>注意：</strong> 一定要注册Coding个人版，而不要注册Coding企业版，说多了都是泪。</li><li><p>新建项目<br> 项目名称填</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yourname.coding.me</span><br></pre></td></tr></table></figure><p> 这里的yourname最好是你注册Coding时的username<br> 建议勾选启用README.MD文件初始化项目</p></li><li>开启Pages服务<br> 在开启后我们就可以通过用户名+网站后缀来访问博客，而且还可以绑定域名通过固定域名来访问。</li></ol><h2 id="SSH设置"><a href="#SSH设置" class="headerlink" title="SSH设置"></a>SSH设置</h2><ol><li><p>检查电脑是否已生成SSH Key:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.ssh</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><p> 若在目录下存在id_rsa.pub或者id_dsa.pub文件，那么直接到第三步</p></li><li><p>创建SSH Key:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p> 参数含义：</p><ul><li>-t 指定密钥类型，默认是rsa</li><li><p>-C 设置注释文字，比如用户名</p><p>此处我们直接按下回车使用默认文件名创建，那么就会生成id_rsa和id_rsa.pub两个秘钥文件。接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是Coding管理者的密码），当然，你也可以不输入密码，我直接按回车。那么push的时候就不需要输入密码，直接提交到Git服务器上了</p></li></ul></li><li><p>在Coding中配置SSH<br> 点击个人设置，再选择SSH公钥。然后点击新填公钥，用记事本打开id_rsa.pub文件，将里面的内容全部复制到公钥内容中，公约名称可以不填。最后点击添加即可。</p></li><li><p>SSH测试</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@git.coding.net</span><br></pre></td></tr></table></figure><p> 若出现以下说明则说明SSH配置成功</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello username You&apos;ve connected to Coding.net by SSH successfully!</span><br></pre></td></tr></table></figure></li></ol><h2 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h2><ol><li><p>修改配置文件<br> 打开博客根目录配置文件_config.yml，找到<code>deploy</code>，填写以下内容</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo:</span><br><span class="line">    github: git@github.com:yourname/yourname.github.io.git,master</span><br><span class="line">    coding: git@git.coding.net:yourname/yourname.git,master</span><br></pre></td></tr></table></figure></li><li><p>部署博客<br> <code>cd</code>进博客根目录，输入以下命令</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g #生成静态网页</span><br><span class="line">hexo d #部署博客</span><br></pre></td></tr></table></figure><p> 最后打开 <a href="http://yourname.coding.me" target="_blank" rel="noopener">http://yourname.coding.me</a> 即可看到你的博客主页</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Coding </tag>
            
            <tag> git </tag>
            
            <tag> SSH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Non-local PyTorch部分源码解读</title>
      <link href="/2019/08/13/non-local%E7%9A%84pytorch%E6%BA%90%E7%A0%81/"/>
      <url>/2019/08/13/non-local%E7%9A%84pytorch%E6%BA%90%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<blockquote><p>代码地址：<a href="https://github.com/AlexHex7/Non-local_pytorch" target="_blank" rel="noopener">https://github.com/AlexHex7/Non-local_pytorch</a></p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我只看了non-local_embedded_gaussian.py文件下的源码，以下为我的解读</p><a id="more"></a><h1 id="结构图示"><a href="#结构图示" class="headerlink" title="结构图示"></a>结构图示</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/P6cAD8NTqf3a.png?imageslim" alt="mark"></p><h1 id="部分代码解读"><a href="#部分代码解读" class="headerlink" title="部分代码解读"></a>部分代码解读</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_NonLocalBlockND</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    in_channels为输入的通道数</span></span><br><span class="line"><span class="string">    inter_channels为中间过程的通道数</span></span><br><span class="line"><span class="string">    dimension为维度数</span></span><br><span class="line"><span class="string">    sub_sample标志是否进行下采样(subsampled)</span></span><br><span class="line"><span class="string">    bn_layer标示是否进行Batch Norm</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, inter_channels=None, dimension=<span class="number">3</span>, sub_sample=True, bn_layer=True)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># assert用来检查条件，不符合就终止</span></span><br><span class="line">        <span class="comment"># 只能处理一维，二维以及三维的输入数据</span></span><br><span class="line">        <span class="keyword">assert</span> dimension <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        self.dimension = dimension</span><br><span class="line">        self.sub_sample = sub_sample</span><br><span class="line"></span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.inter_channels = inter_channels</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若没有指定中间过程的通道数，则指定为输入通道数的一半</span></span><br><span class="line">        <span class="keyword">if</span> self.inter_channels <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.inter_channels = in_channels // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> self.inter_channels == <span class="number">0</span>:</span><br><span class="line">                self.inter_channels = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据输入的维数来指定对应的卷积函数，池化函数以及归一化函数</span></span><br><span class="line">        <span class="keyword">if</span> dimension == <span class="number">3</span>:</span><br><span class="line">            conv_nd = nn.Conv3d</span><br><span class="line">            max_pool_layer = nn.MaxPool3d(kernel_size=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm3d</span><br><span class="line">        <span class="keyword">elif</span> dimension == <span class="number">2</span>:</span><br><span class="line">            conv_nd = nn.Conv2d</span><br><span class="line">            max_pool_layer = nn.MaxPool2d(kernel_size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv_nd = nn.Conv1d</span><br><span class="line">            max_pool_layer = nn.MaxPool1d(kernel_size=(<span class="number">2</span>))</span><br><span class="line">            bn = nn.BatchNorm1d</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 指定g函数</span></span><br><span class="line">        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                         kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否需要进行归一化操作</span></span><br><span class="line">        <span class="keyword">if</span> bn_layer:</span><br><span class="line">            self.W = nn.Sequential(</span><br><span class="line">                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,</span><br><span class="line">                        kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">                bn(self.in_channels)</span><br><span class="line">            )</span><br><span class="line">            nn.init.constant(self.W[<span class="number">1</span>].weight, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant(self.W[<span class="number">1</span>].bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,</span><br><span class="line">                             kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 初始化为0</span></span><br><span class="line">            nn.init.constant(self.W.weight, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant(self.W.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                             kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,</span><br><span class="line">                           kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否需要进行下采样</span></span><br><span class="line">        <span class="keyword">if</span> sub_sample:</span><br><span class="line">            self.g = nn.Sequential(self.g, max_pool_layer)</span><br><span class="line">            self.phi = nn.Sequential(self.phi, max_pool_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param x: (b, c, t, h, w)</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获得batch的大小</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># g(x)的size为batch_size*inter_channels*W*H</span></span><br><span class="line">        <span class="comment"># view类似于resize，使得个g_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        g_x = self.g(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 维度换位，g_x的size变成batch_size*(W*H)*inter_channels</span></span><br><span class="line">        g_x = g_x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        theta_x = self.theta(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># theta_x的size为batch_size*(W*H)*inter_channels</span></span><br><span class="line">        theta_x = theta_x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># phi_x的size为batch_size*inter_channels*(W*H)</span></span><br><span class="line">        phi_x = self.phi(x).view(batch_size, self.inter_channels, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># f的size为batch_size*(W*H)*(W*H)</span></span><br><span class="line">        f = torch.matmul(theta_x, phi_x)</span><br><span class="line"></span><br><span class="line">        f_div_C = F.softmax(f, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># y的size为batch_size*(H*W)*inter_channels</span></span><br><span class="line">        y = torch.matmul(f_div_C, g_x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，</span></span><br><span class="line">        <span class="comment"># 需要用contiguous()来返回一个contiguous copy。</span></span><br><span class="line">        <span class="comment"># y的size为batch_size*inter_channels*(H*W)</span></span><br><span class="line">        y = y.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># y的size为batch_size*inter_channels*H*W</span></span><br><span class="line">        y = y.view(batch_size, self.inter_channels, *x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># W_y的size为batch_size*out_channels*W*H</span></span><br><span class="line">        W_y = self.W(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到最终输出</span></span><br><span class="line">        z = W_y + x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> PyTorch </tag>
            
            <tag> 源码解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>空洞卷积(Dilated Convolution)学习笔记</title>
      <link href="/2019/08/13/dilated_conv/"/>
      <url>/2019/08/13/dilated_conv/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>引入空洞卷积不得不提的是感受野，感受野就是卷积神经网络的每一层输出的特征图(feature map)上的像素点在原图像上映射的区域大小。空洞卷积主要为了解决图像分割中的一些问题而提出的，在FCN中通过pooling增大感受野缩小图像尺寸，然后通过upsampling还原图像尺寸，但是这个过程中造成了精度的损失，那么为了减小这种损失理所当然想到的是去掉pooling层，然而这样就导致特征图感受野太小，因此空洞卷积应运而生。</p><a id="more"></a><h1 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>Dilated Convolutions，翻译为扩张卷积或空洞卷积。扩张卷积与普通的卷积相比，除了卷积核的大小以外，还有一个扩张率(dilation rate)参数，主要用来表示扩张的大小。扩张卷积与普通卷积的相同点在于，卷积核的大小是一样的，在神经网络中即参数数量不变，区别在于扩张卷积具有更大的感受野。</p><h2 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/yljmAFjJxXBF.png?imageslim" alt="mark"></p><ul><li>图(a)为1-dilated conv，感受野为3×3</li><li>图(b)为2-dilated conv，跟在1-dilated conv后面，感受野扩大为为7×7</li><li>图(c)为4-dilated conv，同样跟在1-dilated conv以及1-dilated conv后面，感受野扩大为为15×15</li><li>相比之下，使用stride为1的普通卷积，三层后的感受野仅为7×7</li></ul><h2 id="空洞卷积的作用"><a href="#空洞卷积的作用" class="headerlink" title="空洞卷积的作用"></a>空洞卷积的作用</h2><ul><li><p><strong>扩大感受野</strong><br>  在deep net中为了增加感受野且降低计算量，总要进行降采样(pooling或s2/conv)，这样虽然可以增加感受野，但空间分辨率降低了。为了能不丢失分辨率，且仍然扩大感受野，可以使用空洞卷积。这在检测，分割任务中十分有用。一方面感受野大了可以检测分割大目标，另一方面分辨率高了可以精确定位目标。</p></li><li><p><strong>捕获多尺度上下文信息</strong><br>  空洞卷积有一个参数可以设置dilation rate，具体含义就是在卷积核中填充dilation rate-1个0，因此，当设置不同dilation rate时，感受野就会不一样，也即获取了多尺度信息。</p></li></ul><h2 id="空洞卷积存在的问题"><a href="#空洞卷积存在的问题" class="headerlink" title="空洞卷积存在的问题"></a>空洞卷积存在的问题</h2><ul><li><p><strong>The Gridding Effect</strong><br>  假设我们仅仅多次叠加 dilation rate 2 的 3 x 3 kernel 的话，则会出现这个问题：</p><p>  <img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/9kvcJLQc3gW6.png?imageslim" alt="mark"></p><p>  很明显，感受野不连续（我们上一小结的例子就没这个问题，所以空洞卷积依赖网络设计）。</p></li><li><p><strong>Long-ranged information might be not relevant</strong><br>  我们从 dilated convolution 的设计背景来看就能推测出这样的设计是用来获取 long-ranged information。然而光采用大 dilation rate 的信息或许只对一些大物体分割有效果，而对小物体来说可能则有弊无利了。如何同时处理不同大小的物体的关系，则是设计好 dilated convolution 网络的关键。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> 空洞卷积 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记11：ResNet</title>
      <link href="/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/"/>
      <url>/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B011/</url>
      
        <content type="html"><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><p>ResNet是由Kaiming He等4名华人提出，通过使用Residual Unit成功训练了152层的深度神经网络，在ILSVRC 2015比赛中获得冠军，取得了3.57%的top-5错误率，同时参数却比VGGNet少。ResNet的结构可以极快的加速超深神经网络的训练，模型的准确率也有较大的提升。之后很多方法都建立在ResNet的基础上完成的，例如检测，分割，识别等领域都纷纷使用ResNet。在ResNet推出不久，Google就借鉴了ResNet的精髓，提出了Inception V4和Inception-ResNet-V2，并通过融合这两个模型，在ILSVRC数据集上取得了惊人的3.08%的错误率。所以可见ResNet确实很好用。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/z6ycUHr6A6NC.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><h2 id="导入包并设计Block模块组"><a href="#导入包并设计Block模块组" class="headerlink" title="导入包并设计Block模块组"></a>导入包并设计Block模块组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">slim = tf.contrib.slim</span><br></pre></td></tr></table></figure><p>我们使用collections.namedtuple设计ResNet基本Block模块组的named tuple<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">scope为生成的Block的名称</span></span><br><span class="line"><span class="string">unit_fn为残差学习元生成函数</span></span><br><span class="line"><span class="string">args是一个长度等于Block中单元数目的序列，序列中每个元素</span></span><br><span class="line"><span class="string">包含第三层通道数，前两层通道数以及中间层步长(depth,depth_bottleneck,stride)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">'Block'</span>,[<span class="string">'scope'</span>,<span class="string">'unit_fn'</span>,<span class="string">'args'</span>])</span>)</span>:</span></span><br><span class="line">    <span class="string">'A named tuple describing a ResNet block'</span></span><br></pre></td></tr></table></figure></p><p>以下面Block(‘block1’, bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)])为例</p><ul><li>block1: 是这个Block的名称</li><li>bottleneck: 前面定义的残差学习单元（有三层）</li><li>[(256, 64, 1)] * 2 + [(256, 64, 2)]: 是一个列表，其中每个元素都对应一个bottleneck残差学习单元，前面两个元素都是(256, 64, 1),最后一个是(256, 64, 2)。每个元素都时一个3元组，即（depth, depth_bottleneck, stride）,代表构建的bottleneck残差学习单元中，第三层的输出通道为256（depth），前两层的输出通道数为64（depth_bottleneck）且中间那层的步长stride为1（stride）</li></ul><h2 id="定义部分方法"><a href="#定义部分方法" class="headerlink" title="定义部分方法"></a>定义部分方法</h2><ul><li><p>定义一个降采样subsample的方法</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">降采样函数   </span></span><br><span class="line"><span class="string">input为输入</span></span><br><span class="line"><span class="string">factor为采样因子</span></span><br><span class="line"><span class="string">使用slim.max_pool2d来实现</span></span><br><span class="line"><span class="string">'''</span>    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsample</span><span class="params">(inputs,factor,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> factor==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> slim.max_pool2d(inputs,[<span class="number">1</span>,<span class="number">1</span>],stride=factor,scope=scope)</span><br></pre></td></tr></table></figure></li><li><p>定义一个conv2d_same函数创建卷积层</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_same</span><span class="params">(inputs,num_outputs,kernel_size,stride,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> stride==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> slim.conv2d(inputs,num_outputs,kernel_size,stride=<span class="number">1</span>,padding=<span class="string">'SAME'</span>,scope=scope)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 显式地pad zero</span></span><br><span class="line">        pad_total=kernel_size - <span class="number">1</span></span><br><span class="line">        pad_beg=pad_total // <span class="number">2</span></span><br><span class="line">        pad_end=pad_total-pad_beg</span><br><span class="line">        <span class="comment">#使用tf.pad对图像进行填充</span></span><br><span class="line">        inputs = tf.pad(inputs, [[<span class="number">0</span>, <span class="number">0</span>], [pad_beg, pad_end], [pad_beg, pad_end], [<span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">        <span class="keyword">return</span> slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,</span><br><span class="line">                        padding=<span class="string">'VALID'</span>, scope=scope)</span><br></pre></td></tr></table></figure></li><li><p>定义堆叠Blocks的函数</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">net为输入</span></span><br><span class="line"><span class="string">blocks为定义的Block的class的列表</span></span><br><span class="line"><span class="string">outputs_collections是用来收集各个end_points的collections</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="meta">@slim.add_arg_scope</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stack_blocks_dense</span><span class="params">(net,blocks,outputs_collections=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> blocks:</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(block.scope,<span class="string">'block'</span>,[net])<span class="keyword">as</span> sc:</span><br><span class="line">            <span class="comment"># 拿到每个残差学习单元的args</span></span><br><span class="line">            <span class="keyword">for</span> i,unit <span class="keyword">in</span> enumerate(block.args):</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">'unit_%d'</span>%(i+<span class="number">1</span>),values=[net]):</span><br><span class="line">                    <span class="comment">#获取每个Block中的参数，包括第三层通道数，前两层通道数以及中间层步长</span></span><br><span class="line">                    unit_depth,unit_depth_bottleneck,unit_stride=unit    </span><br><span class="line">                    <span class="comment">#unit_fn是Block类的残差神经元生成函数，它按顺序创建残差学习元并进行连接</span></span><br><span class="line">                    net=block.unit_fn(net,</span><br><span class="line">                                    depth=unit_depth,</span><br><span class="line">                                    depth_bottleneck=unit_depth_bottleneck,</span><br><span class="line">                                    stride=unit_stride)</span><br><span class="line">            <span class="comment"># 将输出的net添加到collection中</span></span><br><span class="line">            net=slim.utils.collect_named_outputs(outputs_collections,sc.name,net)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure></li><li><p>创建ResNet通用的arg_scope</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_arg_scope</span><span class="params">(is_training=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                    weight_decay=<span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_decay=<span class="number">0.997</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_epsilon=<span class="number">1e-5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_norm_scale=True)</span>:</span></span><br><span class="line">    batch_norm_params = &#123;</span><br><span class="line">    <span class="string">'decay'</span>: batch_norm_decay,</span><br><span class="line">    <span class="string">'epsilon'</span>: batch_norm_epsilon,</span><br><span class="line">    <span class="string">'scale'</span>: batch_norm_scale,</span><br><span class="line">    <span class="string">'updates_collections'</span>: tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">    <span class="string">'fused'</span>: <span class="literal">None</span>,  <span class="comment"># Use fused batch norm if possible.</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置slim.conv2d函数的默认参数</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">    [slim.conv2d],</span><br><span class="line">    weights_regularizer=slim.l2_regularizer(weight_decay),</span><br><span class="line">    weights_initializer=slim.variance_scaling_initializer(),</span><br><span class="line">    activation_fn=tf.nn.relu,</span><br><span class="line">    normalizer_fn=slim.batch_norm,</span><br><span class="line">    normalizer_params=batch_norm_params):</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm], **batch_norm_params):</span><br><span class="line">            <span class="keyword">with</span> slim.arg_scope([slim.max_pool2d], padding=<span class="string">'SAME'</span>) <span class="keyword">as</span> arg_sc:</span><br><span class="line">                <span class="keyword">return</span> arg_sc</span><br></pre></td></tr></table></figure></li><li><p>定义核心的bottleneck残差学习单元</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用add_arg_scope使bottleneck函数能够直接使用slim.arg_scope设置默认参数</span></span><br><span class="line"><span class="meta">@slim.add_arg_scope</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bottleneck</span><span class="params">(inputs,depth,depth_bottleneck,stride,</span></span></span><br><span class="line"><span class="function"><span class="params">            outputs_collections=None,scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'bottleneck_v2'</span>,[inputs])<span class="keyword">as</span> sc:</span><br><span class="line">        <span class="comment"># 获取输入的最后一个维度，即输入的通道数目</span></span><br><span class="line">        <span class="comment"># 参数min_rank=4可以限定最少为4个维度</span></span><br><span class="line">        depth_in=slim.utils.last_dimension(inputs.get_shape(),min_rank=<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 先对输入进行Batch Normalization，再进行非线性激活</span></span><br><span class="line">        preact=slim.batch_norm(inputs,activation_fn=tf.nn.relu,scope=<span class="string">'preact'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果残差神经元的输出通道数目和输入的通道数目相同，那么直接对图像进行降采样，以保证shortcut尺寸和经历三个卷积层后的输出的此存相同        </span></span><br><span class="line">        <span class="keyword">if</span> depth==depth_in:</span><br><span class="line">            shortcut=subsample(inputs,stride,<span class="string">'shortcut'</span>)</span><br><span class="line">        <span class="comment"># 如果残差神经元的输出通道数目和输入的通道数目不同，利用尺寸为1x1的卷积核对输入进行卷积，使输入通道数相同；</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shortcut=slim.conv2d(preact,depth,[<span class="number">1</span>,<span class="number">1</span>],stride=stride,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                activation_fn=<span class="literal">None</span>,scope=<span class="string">'shortcut'</span>)</span><br><span class="line">        <span class="comment"># 然后，定义残差，即三个卷积层           </span></span><br><span class="line">        residual=slim.conv2d(preact,depth_bottleneck,[<span class="number">1</span>,<span class="number">1</span>],stride=<span class="number">1</span>,scope=<span class="string">'conv1'</span>)</span><br><span class="line">        residual=conv2d_same(residual,depth_bottleneck,<span class="number">3</span>,stride,scope=<span class="string">'conv2'</span>)</span><br><span class="line">        residual=slim.conv2d(residual,depth,[<span class="number">1</span>,<span class="number">1</span>],stride=<span class="number">1</span>,normalizer_fn=<span class="literal">None</span>,activation_fn=<span class="literal">None</span>,scope=<span class="string">'conv3'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将shortcut和residual相加，作为输出        </span></span><br><span class="line">        output=shortcut+residual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> slim.utils.collect_named_outputs(outputs_collections,sc.name,output)</span><br></pre></td></tr></table></figure></li></ul><h2 id="定义生成ResNet-V2的主函数"><a href="#定义生成ResNet-V2的主函数" class="headerlink" title="定义生成ResNet V2的主函数"></a>定义生成ResNet V2的主函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">input是输入</span></span><br><span class="line"><span class="string">blocks包含残差学习元的参数</span></span><br><span class="line"><span class="string">num_classes是输出分类数</span></span><br><span class="line"><span class="string">global_pool标志是否加上最后一层全局平均年池化</span></span><br><span class="line"><span class="string">include_root_block标志是否加上ResNet网络最前面通常使用的7×7卷积和最大池化</span></span><br><span class="line"><span class="string">reuse标志是否重用</span></span><br><span class="line"><span class="string">scope是整个网络的名称 </span></span><br><span class="line"><span class="string">'''</span>     </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2</span><span class="params">(inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">              blocks,</span></span></span><br><span class="line"><span class="function"><span class="params">              num_classes=None,</span></span></span><br><span class="line"><span class="function"><span class="params">              global_pool=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              include_root_block=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              reuse=None,</span></span></span><br><span class="line"><span class="function"><span class="params">              scope=None)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">'resnet_v2'</span>,[inputs],reuse=reuse) <span class="keyword">as</span> sc:</span><br><span class="line">        end_points_collection=sc.original_name_scope+<span class="string">'_end_points'</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d,bottleneck,stack_blocks_dense],</span><br><span class="line">                            outputs_collections=end_points_collection):</span><br><span class="line">            net=inputs</span><br><span class="line">            <span class="keyword">if</span> include_root_block:</span><br><span class="line">                <span class="keyword">with</span> slim.arg_scope([slim.conv2d],activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>):</span><br><span class="line">                    <span class="comment">#卷积核为7x7步长为2的卷积层</span></span><br><span class="line">                    net=conv2d_same(net,<span class="number">64</span>,<span class="number">7</span>,stride=<span class="number">2</span>,scope=<span class="string">'conv1'</span>)</span><br><span class="line">                <span class="comment">#最大值池化</span></span><br><span class="line">                net=slim.max_pool2d(net,[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">'pool1'</span>)</span><br><span class="line">            <span class="comment">#调用stack_blocks_dense堆叠残差学习元，每个有三个卷积层</span></span><br><span class="line">            net=stack_blocks_dense(net,blocks)</span><br><span class="line">            <span class="comment">#先做batch norm然后使用relu激活</span></span><br><span class="line">            net=slim.batch_norm(net,activation_fn=tf.nn.relu,scope=<span class="string">'postnorm'</span>)</span><br><span class="line">            <span class="keyword">if</span> global_pool:     </span><br><span class="line">                <span class="comment">#进行全局平均池化</span></span><br><span class="line">                net=tf.reduce_mean(net,[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">'pool5'</span>,keep_dims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment">#一个输出为num_classes的卷积层，不进行激活也不归一正则化。</span></span><br><span class="line">            <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                net=slim.conv2d(net,num_classes,[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,scope=<span class="string">'logits'</span>)</span><br><span class="line">            <span class="comment"># 将collection转化为Python的dict</span></span><br><span class="line">            end_points=slim.utils.convert_collection_to_dict(end_points_collection)</span><br><span class="line"></span><br><span class="line">            <span class="comment">#使用softmax进行分类         </span></span><br><span class="line">            <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                end_points[<span class="string">'predictions'</span>]=slim.softmax(net,scope=<span class="string">'predictions'</span>)</span><br><span class="line">            <span class="keyword">return</span> net,end_points</span><br></pre></td></tr></table></figure><h2 id="定义不同深度的ResNet网络结构"><a href="#定义不同深度的ResNet网络结构" class="headerlink" title="定义不同深度的ResNet网络结构"></a>定义不同深度的ResNet网络结构</h2><ul><li><p>50层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_50</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_50'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">5</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>101层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 101层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_101</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_101'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span>+ [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">3</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">22</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>152层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 152层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_152</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_152'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">7</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li><li><p>200层深度的ResNet</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 200层深度的ResNet网络配置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_v2_200</span><span class="params">(inputs, num_classes = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                global_pool = True,</span></span></span><br><span class="line"><span class="function"><span class="params">                reuse = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                scope = <span class="string">'resnet_v2_200'</span>)</span>:</span></span><br><span class="line">    blocks = [</span><br><span class="line">        Block(<span class="string">'block1'</span>, bottleneck, [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">1</span>)] * <span class="number">2</span> + [(<span class="number">256</span>, <span class="number">64</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block2'</span>, bottleneck, [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">1</span>)] * <span class="number">23</span> + [(<span class="number">512</span>, <span class="number">128</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block3'</span>, bottleneck, [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>)] * <span class="number">35</span> + [(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">2</span>)]),</span><br><span class="line">        Block(<span class="string">'block4'</span>, bottleneck, [(<span class="number">2048</span>, <span class="number">512</span>, <span class="number">1</span>)] * <span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">return</span> resnet_v2(inputs, blocks, num_classes, global_pool,</span><br><span class="line">                    include_root_block = <span class="literal">True</span>, reuse = reuse,</span><br><span class="line">                    scope = scope)</span><br></pre></td></tr></table></figure></li></ul><h2 id="耗时测试"><a href="#耗时测试" class="headerlink" title="耗时测试"></a>耗时测试</h2><ul><li><p>定义测试耗时函数</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session, target, info_string)</span>:</span></span><br><span class="line">    num_steps_burn_in = <span class="number">10</span>  <span class="comment"># 打印阈值</span></span><br><span class="line">    total_duration = <span class="number">0.0</span>    <span class="comment"># 每一轮所需要的迭代时间</span></span><br><span class="line">    total_duration_aquared = <span class="number">0.0</span>  <span class="comment"># 每一轮所需要的迭代时间的平方</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time    <span class="comment"># 计算耗时</span></span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_steps_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</span><br><span class="line">                print(<span class="string">"%s : step %d, duration = %.3f"</span> % (datetime.now(), i - num_steps_burn_in, duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_aquared += duration * duration</span><br><span class="line">    mn = total_duration / num_batches   <span class="comment"># 计算均值</span></span><br><span class="line">    vr = total_duration_aquared / num_batches - mn * mn  <span class="comment"># 计算方差</span></span><br><span class="line">    sd = math.sqrt(vr) <span class="comment"># 计算标准差</span></span><br><span class="line">    print(<span class="string">"%s : %s across %d steps, %.3f +/- %.3f sec/batch"</span> % (datetime.now(), info_string, num_batches, mn, sd))</span><br></pre></td></tr></table></figure></li><li><p>耗时测试</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">height, width = <span class="number">224</span>, <span class="number">224</span></span><br><span class="line">inputs = tf.random_uniform((batch_size, height, width, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope(resnet_arg_scope(is_training = <span class="literal">False</span>)):</span><br><span class="line">    net, end_points = resnet_v2_152(inputs, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    num_batches = <span class="number">100</span></span><br><span class="line">    time_tensorflow_run(sess, net, <span class="string">"Forward"</span>)</span><br></pre></td></tr></table></figure></li><li><p>测试结果</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2019-01-26 08:10:51.879413 : step 0, duration = 0.486</span><br><span class="line">2019-01-26 08:10:56.748640 : step 10, duration = 0.487</span><br><span class="line">2019-01-26 08:11:01.628659 : step 20, duration = 0.489</span><br><span class="line">2019-01-26 08:11:06.511324 : step 30, duration = 0.489</span><br><span class="line">2019-01-26 08:11:11.410210 : step 40, duration = 0.490</span><br><span class="line">2019-01-26 08:11:16.311633 : step 50, duration = 0.491</span><br><span class="line">2019-01-26 08:11:21.219118 : step 60, duration = 0.493</span><br><span class="line">2019-01-26 08:11:26.133231 : step 70, duration = 0.492</span><br><span class="line">2019-01-26 08:11:31.054586 : step 80, duration = 0.493</span><br><span class="line">2019-01-26 08:11:35.984226 : step 90, duration = 0.494</span><br><span class="line">2019-01-26 08:11:40.435636 : Forward across 100 steps, 0.490 +/- 0.002 sec/batch</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记10：Inception V3</title>
      <link href="/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/"/>
      <url>/2019/08/12/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B010/</url>
      
        <content type="html"><![CDATA[<h1 id="Inception-V3结构"><a href="#Inception-V3结构" class="headerlink" title="Inception V3结构"></a>Inception V3结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/OtRKyOfuqBS3.png?imageslim" alt="mark"></p><a id="more"></a><p>值得借鉴的设计CNN的思想和Trick：</p><ul><li>Factorization into small convolutions很有效，可以降低参数量，减轻过拟合，增加网络非线性的表达能力。</li><li>卷积网络从输入到输出，应该让图片尺寸逐渐减少，输出通道逐渐增加，即让空间结构简化，将空间信息转化为高阶抽象的特征信息。</li><li>Inception Module用多个分支提取不同抽象程度的高阶特征的思路很有效，可以丰富网络的表达能力。</li></ul><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><h2 id="定义函数-inception-v3-arg-scope"><a href="#定义函数-inception-v3-arg-scope" class="headerlink" title="定义函数 inception_v3_arg_scope"></a>定义函数 inception_v3_arg_scope</h2><p>函数 inception_v3_arg_scope 用来生成网络中经常用到的函数的默认参数，比如卷记的激活函数，权重初始化方式，标准化器等等。接下来嵌套一个<code>slim.arg_scope</code>，对卷积层生成函数<code>slim.conv2d</code>的几个人参数赋予默认值。最后返回定义好的scope。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br><span class="line"><span class="comment">#定义简单的函数产生截断的正态分布</span></span><br><span class="line">trunc_normal = <span class="keyword">lambda</span> stddev:tf.truncated_normal_initializer(<span class="number">0.0</span>,stddev)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义函数 inception_v3_arg_scope 用来生成网络中经常用到的函数的默认参数</span></span><br><span class="line"><span class="string">L2正则的Weight_decay默认值为0.0004</span></span><br><span class="line"><span class="string">标准差stddev默认值为0.1</span></span><br><span class="line"><span class="string">参数batch_norm_var_collection默认值为moving_vars</span></span><br><span class="line"><span class="string">batch normalization参数字典：</span></span><br><span class="line"><span class="string">    衰减系数decay为0.9997</span></span><br><span class="line"><span class="string">    epsilon为0.001</span></span><br><span class="line"><span class="string">    updates_collections为tf.GraphKeys.UPDATE_OPS</span></span><br><span class="line"><span class="string">    字典variables_collections:</span></span><br><span class="line"><span class="string">        beta和gamma设置为None</span></span><br><span class="line"><span class="string">        moving_mean和moving_variance设置为batch_norm_var_collection</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3_arg_scope</span><span class="params">(weight_decay=<span class="number">0.00004</span>,stddev=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        batch_norm_var_collection=<span class="string">"moving_vars"</span>)</span>:</span></span><br><span class="line">    batch_norm_params = &#123;</span><br><span class="line">        <span class="string">"decay"</span>:<span class="number">0.9997</span>,<span class="string">"epsilon"</span>:<span class="number">0.001</span>,<span class="string">"updates_collections"</span>:tf.GraphKeys.UPDATE_OPS,</span><br><span class="line">        <span class="string">"variables_collections"</span>:&#123;</span><br><span class="line">            <span class="string">"beta"</span>:<span class="literal">None</span>,<span class="string">"gamma"</span>:<span class="literal">None</span>,<span class="string">"moving_mean"</span>:[batch_norm_var_collection],</span><br><span class="line">            <span class="string">"moving_variance"</span>:[batch_norm_var_collection]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    嵌套slim.arg_scope</span></span><br><span class="line"><span class="string">    权重初始化器weight_initializer设置为trunc_normal(stddev)</span></span><br><span class="line"><span class="string">    激活函数设置为ReLU</span></span><br><span class="line"><span class="string">    标准化器设置为slim.batch_norm</span></span><br><span class="line"><span class="string">    标准化器的参数设置为batch_norm_patams</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.fully_connected],</span><br><span class="line">                        weights_regularizer=slim.l2_regularizer(weight_decay)):</span><br><span class="line">        <span class="comment">#对卷积层生成函数的几个参数赋予默认值</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d],</span><br><span class="line">                            weights_regularizer = tf.truncated_normal_initializer(stddev=stddev),</span><br><span class="line">                            activation_fc = tf.nn.relu,</span><br><span class="line">                            normalizer_fc = slim.batch_norm,</span><br><span class="line">                            normalizer_params = batch_norm_params) <span class="keyword">as</span> scope:</span><br><span class="line">            <span class="keyword">return</span> scope</span><br></pre></td></tr></table></figure></p><h2 id="定义Inception-V3的卷积部分"><a href="#定义Inception-V3的卷积部分" class="headerlink" title="定义Inception V3的卷积部分"></a>定义Inception V3的卷积部分</h2><p>它可以生成Inception V3网络的卷积部分。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义Inception V3的卷积部分</span></span><br><span class="line"><span class="string">inputs为输入图片数据的tensor</span></span><br><span class="line"><span class="string">scope为包含了函数默认参数的环境</span></span><br><span class="line"><span class="string">用字典表end_points来保存某些关键节点供之后使用</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3_base</span><span class="params">(inputs,scope=None)</span>:</span></span><br><span class="line">    end_points = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">"InceptionV3"</span>,[inputs]):</span><br><span class="line">        <span class="comment">#对slim.conv2d,slim.max_pool2d,slim.avg_pool2d三个函数的参数设置默认值</span></span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],stride = <span class="number">1</span>,padding = <span class="string">"VALID"</span>):                               </span><br><span class="line">            <span class="comment">#各参数分别为输入的tensor,输出的通道,卷积核尺寸,步长stride，padding模式</span></span><br><span class="line">            net = slim.conv2d(inputs,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_2a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],padding=<span class="string">"SAME"</span>,scope=<span class="string">"Conv2d_2b_3x3"</span>)</span><br><span class="line">            net = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"MaxPool_3a_3x3"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">80</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_3b_1x1"</span>)</span><br><span class="line">            net = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_4a_3x3"</span>)</span><br><span class="line">            net = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,scope=<span class="string">"MaxPool_5a_3x3"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="定义第一个Inception模块"><a href="#定义第一个Inception模块" class="headerlink" title="定义第一个Inception模块"></a>定义第一个Inception模块</h3><ul><li><p><strong>第一个Inception Module</strong><br>  第一个Inception Module名称为Mixed_5b。这个Inception Module中有4个分支：</p><ol><li>第一个分支有64输出通道的1×1卷积</li><li>第二个分支有48输出通道的1×1卷积，连接有64输出通道的5×5卷积</li><li>第三个分支有64输出通道的1×1卷积，再连续连接两个有96通道的3×3卷积</li><li><p>第四个分支有为3×3的平均池化，连接有32输出通道的1×1卷积</p><p>最后使用<code>tf.concat</code>将四个分支的输出合并在一起，生成这个Inception Module的最终输出。4个分支的输出通道数之和为64+64+96+32=256，即最终输出的图片尺寸为35×35×256。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义第一个Inception模块组</span></span><br><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],</span><br><span class="line">                    stride = <span class="number">1</span>,padding = <span class="string">"SAME"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5b"</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">            batch_0 = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">            batch_1 = slim.conv2d(net,num_outputs=<span class="number">48</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_1 = slim.conv2d(batch_1,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],scope=<span class="string">"Conv2d_0b_5x5"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">            batch_2 = slim.conv2d(net,num_outputs=<span class="number">64</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">96</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">96</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">            batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">            batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">32</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"> </span><br><span class="line">        net = tf.concat([batch_0,batch_1,batch_2,batch_3],<span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  第二个Inception Module的名称为Mixed_5c。它同样也有四个分支，唯一不同的是第四个分支最后接的是64输出通道的1×1卷积。因此我们输出的tensor的最终尺寸为35×5×288。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5c"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">48</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">5</span>, <span class="number">5</span>], scope=<span class="string">"Conv2d_0c_5x5"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">    </span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2, batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第三个Inception Module</strong><br>  第三个Inception Module的名称为Mixed_5d，和上一个Inception Module完全相同。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_5d"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">48</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">5</span>, <span class="number">5</span>], scope=<span class="string">"Conv2d_0c_5x5"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0c_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2, batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="定义第二个Inception模块组"><a href="#定义第二个Inception模块组" class="headerlink" title="定义第二个Inception模块组"></a>定义第二个Inception模块组</h3><p>第二个Inception模块组是一个非常大的模块组，包含了5个Inception Module。其中第二个到第五个Inception Module的结构非常相似。</p><ul><li><p><strong>第一个Inception Module</strong><br>  第一个Inception Module的名称为Mixed_6a，包含三个分支：</p><ol><li>第一个分支为384通道的3×3卷积，步长为2</li><li>第二个分支有三层，分别为64输出通道的1×1卷积，和两个96输出通道的3×3卷积，最后一层的步长为2。</li><li>第三个分支为3×3的池化层，步长为2<br>最后用<code>tf.concat</code>将三个分支在输出通道上合并，最后的输出尺寸为17×17×(384+96+256)=17×17×768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6a"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">                                stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">64</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">96</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">                                stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.max_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],stride=<span class="number">2</span>,padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                    scope=<span class="string">"MaxPool_1a_3x3"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  名称为Mixed_6b，它有四个分支：</p><ol><li>第一个分支为193输出通道的1×1卷积</li><li>第二个分支有三个卷积层，分别为128输出通道的1×1卷积，128输出通道的1×7卷积，以及192输出通道的7×1卷积，这里用到了Factorization into small convolutions思想，串联的1×7卷积和7×1卷积相当于合成一个7×7卷积。大大减少了参数，减轻了过拟合，同事多了一个激活函数增加了非线性特征变换。</li><li>第三个分支有五个卷积层，分别为128输出通道的1×1卷积，128输出通道的7×1卷积，128输出通道的1×1卷积，128输出通道的1×7卷积，128输出通道的7×1卷积和192输出通道的1×7卷积</li><li>第四个分支为3×3的平均池化层，再连接192输出通道的1×1卷积。<br>最后四个分支合并，输出的tensor尺寸为17×17×(192+192+192+192)=17×17×768<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6b"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">128</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第三个Inception Module</strong><br>  名称为Mixed_6c，和前一个Inception Module非常相似，唯一不同的地方就是第二和第三分支中前几个卷积层的输出通道从128变成了160。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第三个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6c"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第四个Inception Module</strong><br>  名称为Mixed_d，和Mixed_6c完全一致。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第四个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6d"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>第五个Inception Module</strong><br>  名称为Mixed_6e，和前两个Inception Module也完全一致。这是第二个Inception模块组的最后一个Inception Module。我们将Mixed_6e存储于end_points中，作为Auxiliary Classifier辅助模型的分类。</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第二个Inception模块组,第五个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_6e"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0c_1x7"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">160</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0d_7x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">7</span>], scope=<span class="string">"Conv2d_0e_1x7"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br><span class="line"><span class="comment">#第二个模块组的最后一个Inception模块，将Mixed_6e存储于end_points中</span></span><br><span class="line">end_points[<span class="string">"Mixed_6e"</span>] = net</span><br></pre></td></tr></table></figure></li></ul><h3 id="定义第三个Inception模块组"><a href="#定义第三个Inception模块组" class="headerlink" title="定义第三个Inception模块组"></a>定义第三个Inception模块组</h3><p>第三个Inception模块组包含了3个Inception Module。其中后两个Inception Module的结构非常相似。</p><ul><li><p><strong>第一个Inception Module</strong><br>  名称为Mixed_7a，包含了三个分支：</p><ol><li>第一个分支为192输出通道的1×1卷积，再接320输出通道的3×3卷积，步长为2</li><li>第二个分支有四个卷积层，分别为192输出通道的1×1卷积，192输出通道的1×7卷积，192输出通道的7×1卷积和192输出通道的3×3卷积。最后一个卷积层步长为2，padding为VALID</li><li>第三个分支为一个3×3最大池化层，步长为2，padding为VALID。<br>最后合并得到的tensor尺寸为为3×3×(320+192+768)=8×8×1280。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第三个Inception模块组,第一个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7a"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>],stride=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">1</span>,<span class="number">7</span>], scope=<span class="string">"Conv2d_0b_1x7"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">7</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0c_7x1"</span>)</span><br><span class="line">        batch_1 = slim.conv2d(batch_1, num_outputs=<span class="number">192</span>, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">"VALID"</span>,scope=<span class="string">"Conv2d_1a_3x3"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.max_pool2d(net, kernel_size=[<span class="number">3</span>, <span class="number">3</span>], stride=<span class="number">2</span>, padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                    scope=<span class="string">"MaxPool_1a_3x3"</span>)</span><br><span class="line"></span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><strong>第二个Inception Module</strong><br>  名称为Mixed_7b，它有四个分支：</p><ol><li>第一个分支为320输出通道的1×1卷积</li><li>第二个分支先是一个384输出通道的1×1卷积，随后在分支内开两个分支，分别为384输出通道的1×3卷积和384输出通道的3×1卷积，然后用<code>tf.concat</code>合并，得到的tensor的尺寸为8×8×(384+384)=8×8×768</li><li>第三个分支先是48输出通道的1×1卷积，然后是384输出通道的3×3卷积，然后同样在分支内拆成两个分支，分别为384输出通道的1×3卷积和384输出通道的3×1卷积</li><li>第四个分支为一个3×3的平均池化层后接一个192输出通道的1×1卷积。<br>最后合并得到的tensor的尺寸为8×8×(320+768+768+192)=8×8×2048<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义第三个Inception模块组,第二个Inception模块</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7b"</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">        batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">        batch_1 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_1 = tf.concat([</span><br><span class="line">            slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_1x3"</span>),</span><br><span class="line">            slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">        batch_2 = slim.conv2d(net,num_outputs=<span class="number">448</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">        batch_2 = tf.concat([</span><br><span class="line">            slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_1x3"</span>),</span><br><span class="line">            slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0d_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">        batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">        batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"></span><br><span class="line">net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br></pre></td></tr></table></figure></li></ol></li><li><p><em>第三个Inception Module*</em><br>  名称为Mixed_7c，它和前一个Inception Module完全一致。最后我们返回这个Inception Module的结果，作为inception_v3_base函数的最终输出</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    <span class="comment"># 定义第三个Inception模块组,第三个Inception模块</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"Mixed_7c"</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_0"</span>):</span><br><span class="line">            batch_0 = slim.conv2d(net, num_outputs=<span class="number">320</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_1"</span>):</span><br><span class="line">            batch_1 = slim.conv2d(net, num_outputs=<span class="number">384</span>, kernel_size=[<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_1 = tf.concat([</span><br><span class="line">                slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_1x3"</span>),</span><br><span class="line">                slim.conv2d(batch_1,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_2"</span>):</span><br><span class="line">            batch_2 = slim.conv2d(net,num_outputs=<span class="number">448</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0a_1x1"</span>)</span><br><span class="line">            batch_2 = slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0b_3x3"</span>)</span><br><span class="line">            batch_2 = tf.concat([</span><br><span class="line">                slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">1</span>,<span class="number">3</span>],scope=<span class="string">"Conv2d_0c_1x3"</span>),</span><br><span class="line">                slim.conv2d(batch_2,num_outputs=<span class="number">384</span>,kernel_size=[<span class="number">3</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0d_3x1"</span>)],axis=<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"Branch_3"</span>):</span><br><span class="line">            batch_3 = slim.avg_pool2d(net,kernel_size=[<span class="number">3</span>,<span class="number">3</span>],scope=<span class="string">"AvgPool_0a_3x3"</span>)</span><br><span class="line">            batch_3 = slim.conv2d(batch_3,num_outputs=<span class="number">192</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_0b_1x1"</span>)</span><br><span class="line"> </span><br><span class="line">    net = tf.concat([batch_0, batch_1, batch_2,batch_3], <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">return</span> net,end_points</span><br></pre></td></tr></table></figure></li></ul><h2 id="实现Inception-v3函数"><a href="#实现Inception-v3函数" class="headerlink" title="实现Inception_v3函数"></a>实现Inception_v3函数</h2><p>实现Inception V3网络的最后一部分——全局平均池化，Softmax和Auxiliary Logits。</p><ul><li><p><strong>函数Inception_v3的输入参数</strong></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3</span><span class="params">(inputs,num_classes=<span class="number">1000</span>,is_training=True,droupot_keep_prob = <span class="number">0.8</span>,prediction_fn = slim.softmax,spatial_squeeze = True,reuse = None, scope=<span class="string">"InceptionV3"</span>)</span>:</span> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">InceptionV3整个网络的构建</span></span><br><span class="line"><span class="string">param :</span></span><br><span class="line"><span class="string">inputs -- 输入tensor</span></span><br><span class="line"><span class="string">num_classes -- 最后分类数目</span></span><br><span class="line"><span class="string">is_training -- 是否是训练过程</span></span><br><span class="line"><span class="string">droupot_keep_prob -- dropout保留节点比例</span></span><br><span class="line"><span class="string">prediction_fn -- 最后分类函数，默认为softmax</span></span><br><span class="line"><span class="string">patial_squeeze -- 是否对输出去除维度为1的维度</span></span><br><span class="line"><span class="string">reuse -- 是否对网络和Variable重复使用</span></span><br><span class="line"><span class="string">scope -- 函数默认参数环境</span></span><br><span class="line"><span class="string">return:</span></span><br><span class="line"><span class="string">logits -- 最后输出结果</span></span><br><span class="line"><span class="string">end_points -- 包含辅助节点的重要节点字典表</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Auxiliary Logits部分的逻辑</strong><br>  Auxiliary Logits作为辅助分类的节点，对分类结果预测有很大的帮助。通过end_points得到Mixed_6e后：</p><ul><li>连接一个5×5的平均池化，步长为3，padding设为VALID，这样输出的尺寸就从17×17×768变成5×5×768。</li><li>接着连接一个128输出通道的1×1卷积和一个768输出通道的5×5卷积，这里权重初始化方式重设为标准差为0.01的正态分布，padding设置为VALID，输出尺寸变为1×1×768</li><li>然后再接一个输出通道为num_classes的1×1卷积，不设激活函数和规范化函数权重初始方式重设为标准差为0.001的正态分布，这样输出就变成了1×1×1000</li><li>最后使用<code>tf.squeeze</code>函数消除输出tensor中前两个为1的维度。将辅助分类节点的输出aux_logits储存到字典表end_points中。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">aux_logits = end_points[<span class="string">"Mixed_6e"</span>]</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"AuxLogits"</span>):</span><br><span class="line">    aux_logits = slim.avg_pool2d(aux_logits,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],stride=<span class="number">3</span>,</span><br><span class="line">                                    padding=<span class="string">"VALID"</span>,scope=<span class="string">"Avgpool_1a_5x5"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">128</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">768</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],</span><br><span class="line">                                weights_initializer=trunc_normal(<span class="number">0.01</span>),padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                scope=<span class="string">"Conv2d_2a_5x5"</span>)</span><br><span class="line">    aux_logits = slim.conv2d(aux_logits,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                                activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                weights_initializer=trunc_normal(<span class="number">0.001</span>),scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">    <span class="comment">#消除tensor中前两个维度为1的维度</span></span><br><span class="line">    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">        aux_logits = tf.squeeze(aux_logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#将辅助节点分类的输出aux_logits存到end_points中</span></span><br><span class="line">    end_points[<span class="string">"AuxLogits"</span>] = aux_logits</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>分类预测部分逻辑</strong><br>  得到Mixed_7e即最后一个卷积层的输出后：</p><ul><li>接一个8×8全局平均池化，padding设置为VALID，tensor的尺寸就变成了1×1×2048</li><li>然后接一个Dropout层，节点保留率为dropout_keep_prob</li><li>接着连接一个输出通道为1000的1×1卷积，激活函数和规范化函数设为空</li><li>然后用<code>tf.squeeze</code>去除输出tensor中维数为1的维度</li><li>最后连接一个Softmax对结果进行分类预测，输出的结果存储到end_points中<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正常分类预测</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Logits"</span>):</span><br><span class="line">    net = slim.avg_pool2d(net,kernel_size=[<span class="number">8</span>,<span class="number">8</span>],padding=<span class="string">"VALID"</span>,</span><br><span class="line">                            scope=<span class="string">"Avgpool_1a_8x8"</span>)</span><br><span class="line">    net = slim.dropout(net,keep_prob=droupot_keep_prob,scope=<span class="string">"Dropout_1b"</span>)</span><br><span class="line">    end_points[<span class="string">"Logits"</span>] = net</span><br><span class="line"></span><br><span class="line">    logits = slim.conv2d(net,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,</span><br><span class="line">                            normalizer_fn=<span class="literal">None</span>,scope=<span class="string">"Conv2d_1c_1x1"</span>)</span><br><span class="line">    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">        logits = tf.squeeze(logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line"></span><br><span class="line">end_points[<span class="string">"Logits"</span>] = logits</span><br><span class="line">end_points[<span class="string">"Predictions"</span>] = prediction_fn(logits,scope=<span class="string">"Predictions"</span>)</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>inception_v3函数实现代码汇总：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inception_v3</span><span class="params">(inputs,num_classes=<span class="number">1000</span>,is_training=True,droupot_keep_prob = <span class="number">0.8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 prediction_fn = slim.softmax,spatial_squeeze = True,reuse = None,scope=<span class="string">"InceptionV3"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    InceptionV3整个网络的构建</span></span><br><span class="line"><span class="string">    param :</span></span><br><span class="line"><span class="string">    inputs -- 输入tensor</span></span><br><span class="line"><span class="string">    num_classes -- 最后分类数目</span></span><br><span class="line"><span class="string">    is_training -- 是否是训练过程</span></span><br><span class="line"><span class="string">    droupot_keep_prob -- dropout保留节点比例</span></span><br><span class="line"><span class="string">    prediction_fn -- 最后分类函数，默认为softmax</span></span><br><span class="line"><span class="string">    patial_squeeze -- 是否对输出去除维度为1的维度</span></span><br><span class="line"><span class="string">    reuse -- 是否对网络和Variable重复使用</span></span><br><span class="line"><span class="string">    scope -- 函数默认参数环境</span></span><br><span class="line"><span class="string">    return:</span></span><br><span class="line"><span class="string">    logits -- 最后输出结果</span></span><br><span class="line"><span class="string">    end_points -- 包含辅助节点的重要节点字典表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope,<span class="string">"InceptionV3"</span>,[inputs,num_classes],</span><br><span class="line">                           reuse=reuse) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.batch_norm,slim.dropout],</span><br><span class="line">                            is_training = is_training):</span><br><span class="line">            net,end_points = inception_v3_base(inputs,scope=scope)     <span class="comment">#前面定义的整个卷积网络部分</span></span><br><span class="line"> </span><br><span class="line">            <span class="comment">#辅助分类节点部分</span></span><br><span class="line">            <span class="keyword">with</span> slim.arg_scope([slim.conv2d,slim.max_pool2d,slim.avg_pool2d],</span><br><span class="line">                                stride = <span class="number">1</span>,padding = <span class="string">"SAME"</span>):</span><br><span class="line">                <span class="comment">#通过end_points取到Mixed_6e</span></span><br><span class="line">                aux_logits = end_points[<span class="string">"Mixed_6e"</span>]</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">"AuxLogits"</span>):</span><br><span class="line">                    aux_logits = slim.avg_pool2d(aux_logits,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],stride=<span class="number">3</span>,</span><br><span class="line">                                                 padding=<span class="string">"VALID"</span>,scope=<span class="string">"Avgpool_1a_5x5"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">128</span>,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=<span class="number">768</span>,kernel_size=[<span class="number">5</span>,<span class="number">5</span>],</span><br><span class="line">                                             weights_initializer=trunc_normal(<span class="number">0.01</span>),padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                             scope=<span class="string">"Conv2d_2a_5x5"</span>)</span><br><span class="line">                    aux_logits = slim.conv2d(aux_logits,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                                             activation_fn=<span class="literal">None</span>,normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">                                             weights_initializer=trunc_normal(<span class="number">0.001</span>),scope=<span class="string">"Conv2d_1b_1x1"</span>)</span><br><span class="line">                    <span class="comment">#消除tensor中前两个维度为1的维度</span></span><br><span class="line">                    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                        aux_logits = tf.squeeze(aux_logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line"> </span><br><span class="line">                    end_points[<span class="string">"AuxLogits"</span>] = aux_logits    <span class="comment">#将辅助节点分类的输出aux_logits存到end_points中</span></span><br><span class="line"> </span><br><span class="line">                <span class="comment">#正常分类预测</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">"Logits"</span>):</span><br><span class="line">                    net = slim.avg_pool2d(net,kernel_size=[<span class="number">8</span>,<span class="number">8</span>],padding=<span class="string">"VALID"</span>,</span><br><span class="line">                                          scope=<span class="string">"Avgpool_1a_8x8"</span>)</span><br><span class="line">                    net = slim.dropout(net,keep_prob=droupot_keep_prob,scope=<span class="string">"Dropout_1b"</span>)</span><br><span class="line">                    end_points[<span class="string">"Logits"</span>] = net</span><br><span class="line"> </span><br><span class="line">                    logits = slim.conv2d(net,num_outputs=num_classes,kernel_size=[<span class="number">1</span>,<span class="number">1</span>],activation_fn=<span class="literal">None</span>,</span><br><span class="line">                                         normalizer_fn=<span class="literal">None</span>,scope=<span class="string">"Conv2d_1c_1x1"</span>)</span><br><span class="line">                    <span class="keyword">if</span> spatial_squeeze:</span><br><span class="line">                        logits = tf.squeeze(logits,axis=[<span class="number">1</span>,<span class="number">2</span>],name=<span class="string">"SpatialSqueeze"</span>)</span><br><span class="line">                </span><br><span class="line">                end_points[<span class="string">"Logits"</span>] = logits</span><br><span class="line">                end_points[<span class="string">"Predictions"</span>] = prediction_fn(logits,scope=<span class="string">"Predictions"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> logits,end_points</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> InceptionNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记10：FCOS:Fully Convolutional One-Stage Object Detection</title>
      <link href="/2019/08/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B010/"/>
      <url>/2019/08/09/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B010/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1904.01355.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.01355.pdf</a><br>代码地址：<a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">https://github.com/tianzhi0549/FCOS</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一种全卷积的 one-stage 目标检测器（FCOS, Fully Convolutional One Stage）, 它以对每个像素进行预测的方式来解决目标检测问题，类似于语义分割。几乎所有的 SOTA 物体检测器，如 RetinaNet，SSD，YOLOv3 和 Faster R-CNN 都依赖于预定义的 anchor box。相比之下，我们提出的 FCOS 不需要 anchor box，同时也不需要 proposals (即 One-Stage)。通过消除对预定义 anchor 的依赖，FCOS 完全避免了与 anchor box 相关的复杂计算，例如在训练期间计算 overlapping 并显着减少 training memory footprint。更重要的是，我们还避免了与 anchor 相关的所有超参数，这些参数通常对最终检测性能非常敏感。凭借唯一的后处理操作非最大抑制（NMS），我们的 FCOS 优于之前的 anchor-based one-stage detectors，并且结构更简单。我们首次展示了一种更加简单灵活的检测框架，可以提高检测精度。我们希望 FCOS 框架可以作为许多其他实例级任务简单而强大的替代方案。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Anchor-based检测器存在的缺点：</p><ul><li>如 Faster R-CNN 和 Focal Loss 所示，检测性能对于尺寸，宽高比和 anchor 数量非常敏感。 例如，在RetinaNet 中，根据 COCO 的 benchmark 上，仅仅改变这些超参数就会影响AP的性能提升4％[13]。 因此，对于 anchor-based 检测器需要仔细调整这些超参数。</li><li>即使经过精心设计，由于 anchor box 的比例和宽高比保持固定，detectors 在处理具有较大形状变化的物体集合时会遇到困难，特别是对于小物体。 预定义的 anchor box 也妨碍了探测器的泛化能力，因为它们需要在具有不同物体尺寸或宽高比的新探测任务上进行重新设定。</li><li>为了实现高召回率，anchor-based 检测器需要将 anchor box 密集地放置在输入图像上（例如，在特征金字塔网络(FPN)中, 对于短边像素为 800 的输入图像, 会产生超过 180K 的 anchor box）。 大多数这些 anchor box 在训练期间会被标记为 negative samples。 过多的 negative samples 加剧了 training 过程中正负样本之间的不平衡性。</li><li>当在训练期间计算所有 anchor box 和 GT box 之间的 IOU 时，过多数量的 anchor box 也显著增加了计算量和存储器占用量。</li></ul><p>基于 anchor-based 的检测方法偏离全卷积预测的框架，而本文尝试类似于语义分割的像素级预测应用至目标检测任务中。因此，目标检测，语义分割，关键点检测等几种任务可以糅合到一个模型中。一些工作如 Dense-Box，UnitBox 曾利用基于 FCN-based 的框架进行目标检测。但这些方法在某一层的 feature map 上直接预测4D坐标及类别，如下图左侧所示，4D向量表示了边框的边距离该像素点的偏移。这些方法与语义分割的全卷积相类似，但每个位置需要回归一个连续的4D向量。为了处理不同大小的边框，DenseBox 将训练图片调整为固定尺寸。DenseBox 必须要在图像金字塔上进行检测，违反了全卷积对所有卷积只计算一次的思想。而且，这些方法大多应用在目标检测的特殊场景中，比如文本检测或者人脸检测。如下图右侧所示，较多的重叠框造成了模糊，无法确定重叠区域应该对哪个框进行回归。本文证明通过FPN结构可以消除这种模糊。本文发现在距离目标中心较远的位置会预测一定数量低质量的边界框。为了打压这些边框，本文设计了一个新的分支”center-ness”，用于预测一个像素与对应边框中心的偏差。所得的分数用于 down-weight 低质量的检测框，最后通过NMS将检测结果进行融合。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/IgTSrBOLa2Sx.png?imageslim" alt="mark"></p><p>这个新的检测框架拥有以下优点：</p><ul><li>检测任务现在与许多其他 FCN 可解决的任务（例如语义分割）相统一，从而可以更轻松地重复使用这些任务中的想法。</li><li>检测变为 proposal free 和 anchor free，这显著减少了超参数的数量。 超参数通常需要启发式调整，并且涉及许多技巧才能获得良好的性能。 而我们的新检测框架使检测器，特别是使它的 training 阶段变得相当简单。 此外，通过消除 anchor box，我们的新探测器完全避免了复杂的 IOU 计算以及训练期间 anchor box 和 GT box 之间的匹配，并将总的训练内存占用(training memory footprint)减少了2倍左右。</li><li>我们在 One-Stage Detectors 中实现了 SOTA 的结果。 我们的实验还表明，本文所提出的 FCOS 可以用作 Two-Stage Detectors 中的 RPN，并且可以实现比基于 anchor 的 RPN 更好的性能。 鉴于更简单的 anchor free Detectors 具有更好的性能，我们鼓励大家重新考虑物体检测中 anchor 的必要性，虽然目前这被认为是检测任务的事实标准(defacto standard for detection)。</li><li>我们所提出的 detector 只需做很小的修改就可以立即扩展到其他视觉任务，包括实例分割和关键点检测。 我们相信这种新方法可以成为许多实例级预测问题的新 baseline。</li></ul><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><p><strong>Anchor-based Detectors</strong></p></li><li><p><strong>Anchor-free Detectors</strong></p></li></ul><h1 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h1><p>在本节中，我们首先以逐像素预测的方式重新构造目标检测任务。 接下来，我们展示了我们如何利用多级预测(multi-level prediction)来改善召回率并解决训练中重叠边界框导致的模糊性。 最后，我们展示了我们提出的 “center-ness” 分支，它有助于抑制低质量的检测边界框并大幅提高整体性能.</p><h2 id="Fully-Convolutional-One-Stage-Object-Detector"><a href="#Fully-Convolutional-One-Stage-Object-Detector" class="headerlink" title="Fully Convolutional One-Stage Object Detector"></a>Fully Convolutional One-Stage Object Detector</h2><p>设$F_i \in R^{H×W×C}$是 backbone CNN 第 $i$ 层的 feature maps，$s$ 为该层之前的总步长。输入图片的真实框定义为$\lbrace B_i \rbrace$，$B_i=(x_0^{(i)},y_0^{(i)},x_1^{(i)},y_1^{(i)},c^{(i)})\in R^4×\lbrace1,2…C\rbrace$。其中$(x_0^{(i)},y_0^{(i)})$和$(x_1^{(i)},y_1^{(i)})$分别为边界框左上角点和右下角点的坐标，$c^{(i)}$为边界框内的object的类别。</p><p>对于 feature maps $F_i$ 上的的每一个位置$(x,y)$，我们都可以将其映射回输入图像的坐标$(\lfloor\frac{s}{2}\rfloor+xs,\lfloor\frac{s}{2}\rfloor+ys)$，它差不多刚好位于位置$(x,y)$的感受野中心附近。与 anchor based detectors 将输入图像上的位置视为 anchor box 的中心并对这些 anchor box 的目标边界框进行回归不同，我们直接回归每个位置的目标边界框。 换句话说，我们的 Detector 直接将 location 视为训练样本而不是将 anchor box 视为训练样本，这与用于语义分割的FCN相同。</p><p>具体而言，如果位置$(x,y)$落入到任何 GT Box 内部, 那么久将其视为正样本, 并且该位置的类标签$c^\star$就是$B_i$的类标签，否则它就是负样本并且$c^\star=0$（背景类）。除了用于分类的标签之外，我们还有一个 4D 的实数向量$t^\star=(l^\star,t^\star,r^\star,b^\star)$, 该向量是每个样本的回归目标。这里$l^\star,t^\star,r^\star,b^\star$是从 location 到 bbox 四条边的距离，如图1（左）所示。如果某个位置属于多个边界框，则会将其视为模糊样本。现在，我们只选择具有最小面积的边界框作为其回归目标(最简单的策略)。 在下一节中，我们将 展示通过多级预测，可以显著减少模糊样本的数量。 形式上，如果位置$(x,y)$与边界框$B_i$相关联，则该位置的训练回归目标可以表示为:</p><script type="math/tex; mode=display">\left\{  \begin{array}{lr}    l^\star=x-x_0^{(i)}, & t^\star=y-Y_0^{(i)} \\    r^\star=x_1^{(i)}-x, & b^\star=y_1^{(i)}-y  \end{array}\right.</script><p>值得注意的是，FCOS 可以利用尽可能多的前景样本来训练回归量。(GT box 内的每个像素点都是正样本) 它与基于 anchor 的探测器不同，anchor-based detectors 仅仅将与 GT box 具有足够 IOU 的anchor box 作为正样本。我们认为，这可能是 FCOS 优于 anchor-based 的原因之一。</p><p><strong>Network Outputs</strong><br>对应于 training targets，我们网络的最后一层会预测用于分类的 80D 向量$\vec{p}$和 bounding box 坐标 4D 向量$\vec{t}=(l，t，r，b)$。跟随 R-CNN 的做法，我们不是训练多类分类器，而是训练 C 个二元分类器。与 R-CNN 类似，我们在 backbone 网络的特征图谱之后分别为分类和回归分支添加了 四个卷积层。此外，由于回归目标总是正的，我们使用$exp(x)$将任意的实数都映射到回归分支顶部的$(0,\infty)$。值得注意的是，FCOS 的网络输出变量比常用的 anchor based detectors 少 9 倍，其中每个位置有 9 个 anchor boxes。</p><p><strong>Loss Function</strong><br>我们定义我们的训练损失函数如下：</p><script type="math/tex; mode=display">L(\lbrace p_{x,y}\rbrace,\lbrace t_{x,y}\rbrace)=\frac{1}{N_{pos}}\sum_{x,y}L_cls(p_{x,y},c_{x,y}^\star)+\frac{\lambda}{N_pos}\sum_{x,y}I_{\lbrace c_{x,y}^\star >0\rbrace}L_{reg}(t_{x,y},t_{x,y}^\star)</script><p>其中$L_{cls}$为 focal loss，$L_{reg}$为 IOU loss。$N_{pos}$为正样本数，$\lambda$在本文中为1来平衡$L_{reg}$的权重。求和是通过对feature maps $F_i$ 上的所有点进行计算得到的。$I_{\lbrace c_{x,y}^\star &gt;0\rbrace}$是一个indicator function，当$c_i^\star&gt;0$时为1，否则为0。</p><p><strong>Inference</strong><br>FCOS 的 Inference 很简单。给定输入图像，我们将其放入网络进行一次 forward 计算, 并获得 feature map $F_i$ 上的每个位置的分类分数$p_{x,y}$和回归预测值$t_{x,y}$。 跟随 R-CNN 的设定，我们选择$p_{x,y}&gt;0.0$的位置作为正样本并通过上述公式来获得预测的边界框。</p><h2 id="Multi-level-Prediction-with-FPN-for-FCOS"><a href="#Multi-level-Prediction-with-FPN-for-FCOS" class="headerlink" title="Multi-level Prediction with FPN for FCOS"></a>Multi-level Prediction with FPN for FCOS</h2><p>在这里，我们展示了如何通过 FPN 的多级预测来解决所提出的 FCOS 存在的两个可能问题。</p><ol><li>CNN 中最后的 feature maps 的大步幅（例如，16）可能回导致相对较低的 best possible recall (BPR)。对于基于 anchor 的检测器，由于大步幅导致的低召回率可以通过降低 positive anchor boxes 所需的 IOU 分数来在一定程度上得到缓解。而对于 FCOS，乍一看可能认为其 BPR 会远低于基于 anchor 的检测器，因为 网络无法召回由于大步幅而最终在 feature map 上没有位置编码的对象。在这里，我们凭经验证明，即使步幅很大，基于 FCN 的 FCOS 仍然能够产生良好的BPR，它甚至可以比官方实现的 Detectron 中基于 anchor 的检测器 RetinaNet 的 BPR 更好。（参见表1）。因此，BPR 实际上不是 FCOS 无法解决的问题。此外，利用多级 FPN 预测，可以进一步改进 BPR 以匹配基于 anchor 的 RetinaNet 最佳BPR。</li><li>与 GT box 的多个重叠会导致在训练期间产生难以理解的模糊性，即哪个边界框应该在重叠位置进行回归？这种模糊性导致基于 FCN 的检测器的性能下降。在本文中，我们表明，使用多级预测可以极大地解决模糊性，并且与基于 anchor 的检测器相比，基于 FCN 的检测器可以获得相同的，有时甚至更好的性能。</li></ol><p>我们在不同级别的特征图上检测到不同大小的对象。具体来说，我们使用定义为 $\lbrace P_3,P_4,P_5,P_6,P_7\rbrace$ 的五个级别的 feature map。$P_3$，$P_4$ 和 $P_5$ 由 backbone CNN 的特征图 $C_3$，$C_4$ 和 $C_5$ 和具有横向连接的1×1卷积层产生，如下图所示. $P_6$ 和 $P_7$ 通过分别在 $P_5$ 和 $P_6$ 上使用一个步长为 2 的卷积层产生. 最终，特征层级 $P_3$，$P_4$，$P_5$，$P_6$ 和 $P_7$ 具有的步幅分别为 8,16,32,64和128。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/sNtrzUOWkbhi.png?imageslim" alt="mark"></p><p>与 anchor based detectors 将不同大小的 anchor 分配给不同的特征级别不一样的是，我们直接限制边界框回归的范围。 更具体地说，我们首先计算所有特征级别上每个位置的回归目标$l^\star$，$t^\star$，$r^\star$和$b^\star$。如果位置满足 $max(l^\star,t^\star,r^\star,b^\star)&gt; m_i$或$max(l^\star,t^\star,r^\star,b^\star)&lt; m_{i-1}$，我们就将其设置为负样本并且再也不会对该位置进行回归操作。这里的 $m_i$ 是特征层级 $i$ 需要回归的最大距离。在本文中，$m_2$，$m_3$，$m_4$，$m_5$，$m_6$和$m_7$分别设置为0,64,128,256,512和$\infty$。 由于 具有不同大小的对象被分配给不同的特征级别(这里是与 FoveaBox 的一处重要区别) 并且 大多数重叠发生在具有显著不同大小的对象之间，因此多级预测可以在很大程度上减轻上述模糊性并且将基于 FCN 的检测器提升到与基于 anchor 的检测器相同的检测性能，如我们后面的实验所示。</p><p>最后，跟随 R-CNN 和 Fast R-CNN 的设定，我们 共享不同特征级别之间的头部(这是与其他 Detector 的不同之处, 其他的都是每个特征层级独立的执行分类和回归)，这样不仅使检测器参数高效，而且能够提高检测性能。 然而，我们观察到不同的特征水平需要回归不同的尺寸范围（例如，$P_3$ 的尺寸范围是 [0,64] 而 $P_4$ 的尺寸范围是 [64,128]），因此 在不同的特征层使用相同的回归 heads 是不合理的。 故此, 我们不使用标准的 $exp(x)$，而是使用带有可训练标量 $s_i% 的 $exp(s_i x)$ 来自动调整特征级 $P_i$ 的指数函数的基数，从而凭经验提高检测性能。</p><h2 id="Center-ness-for-FCOS"><a href="#Center-ness-for-FCOS" class="headerlink" title="Center-ness for FCOS"></a>Center-ness for FCOS</h2><p>在 FCOS 中使用多级预测后，FCOS 和 anchor based 的检测器之间仍存在性能差距。 我们观察到这是由于远离物体中心的位置产生的许多低质量预测边界框造成的。</p><p>我们提出了一种简单而有效的策略来抑制这些低质量的检测边界框而不引入任何超参数。 具体来说，我们添加一个单层分支，与分类分支并行，以预测一个位置的“中心概率(center-ness)”（即，从该位置到该位置所负责的对象的中心的距离）如图2所示， 给定位置的回归目标$l^\star$，$t^\star$，$r^\star$和$b^\star$，center-ness target 定义为</p><script type="math/tex; mode=display">centerness^\star = \sqrt{\frac{min(l^\star,r^\star)}{max(l^\star,r^\star)}×\frac{min(t^\star,b^\star)}{max(t^\star,b^\star)}}</script><p>我们在这里使用 sqrt 来减缓中心的衰减。center-ness 从0到1，因此用 二元交叉熵（BCE）损失训练。 损失被添加到上述损失函数公式中。 在测试时，通过将预测的 center-ness 与相应的分类得分相乘来计算最终得分（用于对检测到的边界框进行排名）。因此，center-ness 可以使远离物体中心的边界框的 scores 减小。结果，这些低质量的边界框很可能被最终的非最大抑制（NMS）过程滤除，从而显著提高了检测性能。</p><p>基于 anchor 的检测器使用两个 IOU 阈值 $T_{low}$ 和 $T_{high}$ 将 anchor boxes 标记为负、忽略和正样本，center-ness 可以看作是一个 软阈值。它是在网络训练中学习的，不需要调整。此外，利用该策略，我们的检测器仍然可以将任何落在 GT Box 中的位置视为正样本，除了上述多层预测中设置为负样本的位置外，这样就可以为回归器使用尽可能多的训练样本。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>不再叙述</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><h3 id="Multi-level-Prediction-with-FPN"><a href="#Multi-level-Prediction-with-FPN" class="headerlink" title="Multi-level Prediction with FPN"></a>Multi-level Prediction with FPN</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/YBbIVpisIPb8.png?imageslim" alt="mark"></p><h3 id="With-or-Without-Center-ness"><a href="#With-or-Without-Center-ness" class="headerlink" title="With or Without Center-ness"></a>With or Without Center-ness</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/qrjP0Ygn5DxK.png?imageslim" alt="mark"></p><h3 id="FCOS-vs-Anchor-based-Detectors"><a href="#FCOS-vs-Anchor-based-Detectors" class="headerlink" title="FCOS vs. Anchor-based Detectors"></a>FCOS vs. Anchor-based Detectors</h3><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/bXD3bdM9o5df.png?imageslim" alt="mark"></p><h2 id="Comparison-with-State-of-the-art-Detectors"><a href="#Comparison-with-State-of-the-art-Detectors" class="headerlink" title="Comparison with State-of-the-art Detectors"></a>Comparison with State-of-the-art Detectors</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/s4EFKSNIFo6K.png?imageslim" alt="mark"></p><h1 id="Extensions-on-Region-Proposal-Networks"><a href="#Extensions-on-Region-Proposal-Networks" class="headerlink" title="Extensions on Region Proposal Networks"></a>Extensions on Region Proposal Networks</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/2vzzVPyj2UrS.png?imageslim" alt="mark"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种 anchor freee, proposal free 的单级探测器FCOS。 如实验所示，FCOS与流行的 anchor based 的一级探测器相比更具有优势，包括RetinaNet，YOLO和SSD，但设计复杂性要低得多。 FCOS完全避免了与 anchor 相关的所有计算和超参数，并以每像素预测方式解决了对象检测，类似于其他密集预测任务，例如语义分割。 FCOS还在一级探测器中实现了最先进的性能。 我们还表明，FCOS可用作两级探测器中的RPN，速度更快的R-CNN，并且大幅优于其RPN。 鉴于其有效性和效率，我们希望FCOS可以作为当前主流锚点探测器的强大而简单的替代方案。 我们还相信FCOS可以扩展到解决许多其他实例级识别任务。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> anchor-free </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 关键点检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow-Slim简介</title>
      <link href="/2019/08/08/tensorflow-slim/"/>
      <url>/2019/08/08/tensorflow-slim/</url>
      
        <content type="html"><![CDATA[<h1 id="Slim简介"><a href="#Slim简介" class="headerlink" title="Slim简介"></a>Slim简介</h1><p>slim是一个使构建，训练，评估神经网络变得简单的库。它可以消除原生tensorflow里面很多重复的模板性的代码，让代码更紧凑，更具备可读性。另外slim提供了很多计算机视觉方面的著名模型（VGG, AlexNet等），我们不仅可以直接使用，甚至能以各种方式进行扩展。</p><a id="more"></a><ul><li>导入Slim<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></table></figure></li></ul><h1 id="TF-Slim的优点"><a href="#TF-Slim的优点" class="headerlink" title="TF-Slim的优点"></a>TF-Slim的优点</h1><ul><li>允许用户通过减少模板代码使得模型更加简洁。这个可以通过使用argument scoping和大量的高层layers、variables来实现</li><li>通过使用常用的正则化（ regularizers）使得建立模型更加简单；</li><li>一些广泛使用的计算机视觉相关的模型（比如VGG，AlexNet）已经在slim中定义好了，用户可以很方便的使用；这些既可以当成黑盒使用，也可以被扩展使用，比如添加一些“multiple heads”到不同的内部的层；</li><li>Slim使得扩展复杂模型变得容易，可以使用已经存在的模型的checkpoints来开始训练算法。</li></ul><h1 id="TF-Slim的主要组件"><a href="#TF-Slim的主要组件" class="headerlink" title="TF-Slim的主要组件"></a>TF-Slim的主要组件</h1><p>TF-Slim由几个独立存在的组件组成，主要包括以下几个：</p><ul><li><p><strong>arg_scope</strong><br>  提供一个新的作用域（scope），称为arg_scope，在该作用域（scope）中，用户可以定义一些默认的参 数，用于特定的操作；<br>  如果你的网络中有大量的相同的参数：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], <span class="number">4</span>, padding=<span class="string">'SAME'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'SAME'</span>,</span><br><span class="line">              weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">              weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>), scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure><p>  用arg_scope处理一下：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> slim.arg_scope([slim.conv2d], padding=<span class="string">'SAME'</span>,</span><br><span class="line">                  weights_initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">                  weights_regularizer=slim.l2_regularizer(<span class="number">0.0005</span>)):</span><br><span class="line">    net = slim.conv2d(inputs, <span class="number">64</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv1'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">128</span>, [<span class="number">11</span>, <span class="number">11</span>], padding=<span class="string">'VALID'</span>, scope=<span class="string">'conv2'</span>)</span><br><span class="line">    net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">11</span>, <span class="number">11</span>], scope=<span class="string">'conv3'</span>)</span><br></pre></td></tr></table></figure><p>  arg_scope在作用范围内定义了指定层的默认参数，若想特别指定某些层的参数，可以重新赋值（相当于重写）</p></li><li><p><strong>data</strong><br>  包含TF-Slim的dataset定义，data providers，parallel_reader，和 decoding utilities；</p></li><li><strong>evaluation</strong><br>  包含用于模型评估的常规函数；</li><li><p><strong>layers</strong><br>  包含用于建立模型的高级layers；<br>  比如分别用TensorFlow和Slim实现一个卷积层的案例：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tensorflow实现卷积层</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1_1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    kernel = tf.Variable(tf.truncated_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], dtype=tf.float32,</span><br><span class="line">                                       stddev=<span class="number">1e-1</span>), name=<span class="string">'weights'</span>)</span><br><span class="line">    conv = tf.nn.conv2d(input, kernel, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>, shape=[<span class="number">128</span>], dtype=tf.float32),</span><br><span class="line">                   trainable=<span class="literal">True</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">    conv1 = tf.nn.relu(bias, name=scope)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#slim实现卷积层</span></span><br><span class="line">net = slim.conv2d(input, <span class="number">128</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv1_1'</span>)</span><br></pre></td></tr></table></figure><p>  另外，比较吸引人的是slim中的repeat和stack操作，假设定义三个相同的卷积层，</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_1'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_2'</span>)</span><br><span class="line">net = slim.conv2d(net, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3_3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure><p>  在slim中的repeat操作可减少代码量：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = slim.repeat(net, <span class="number">3</span>, slim.conv2d, <span class="number">256</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'conv3'</span>)</span><br><span class="line">net = slim.max_pool2d(net, [<span class="number">2</span>, <span class="number">2</span>], scope=<span class="string">'pool2'</span>)</span><br></pre></td></tr></table></figure><p>  stack是处理卷积核或者输出不一样的情况：假设定义三层FC：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Verbose way:</span></span><br><span class="line">x = slim.fully_connected(x, <span class="number">32</span>, scope=<span class="string">'fc/fc_1'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">64</span>, scope=<span class="string">'fc/fc_2'</span>)</span><br><span class="line">x = slim.fully_connected(x, <span class="number">128</span>, scope=<span class="string">'fc/fc_3'</span>)</span><br></pre></td></tr></table></figure><p>  使用stack操作：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slim.stack(x, slim.fully_connected, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>], scope=<span class="string">'fc'</span>)</span><br></pre></td></tr></table></figure><p>  卷积层使用stack操作：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通方法:</span></span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_1'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_2'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>], scope=<span class="string">'core/core_3'</span>)</span><br><span class="line">x = slim.conv2d(x, <span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>], scope=<span class="string">'core/core_4'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简便方法:</span></span><br><span class="line">slim.stack(x, slim.conv2d, [(<span class="number">32</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">32</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="number">64</span>, [<span class="number">3</span>, <span class="number">3</span>]), (<span class="number">64</span>, [<span class="number">1</span>, <span class="number">1</span>])], scope=<span class="string">'core'</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>learning</strong><br>  包含一些用于训练模型的常规函数；</p></li><li><strong>losses</strong><br>  包含一些用于loss function的函数；</li><li><strong>metrics</strong><br>  包含一些热门的评价标准；</li><li><strong>nets</strong><br>  包含一些热门的网络定义，如VGG，AlexNet等模型；</li><li><strong>queues</strong><br>  提供一个内容管理者，使得可以很容易、很安全地启动和关闭QueueRunners；</li><li><strong>regularizers</strong><br>  包含权重正则化；</li><li><strong>variables</strong><br>  提供一个方便的封装，用于变量创建和使用。<br>  变量分为两类：模型变量和局部变量。局部变量是不作为模型参数保存的，而模型变量会再save的时候保存下来。诸如global_step之类的就是局部变量。slim中可以写明变量存放的设备，正则和初始化规则。还有获取变量的函数也需要注意一下，get_variables是返回所有的变量。<br>  slim中定义一个变量的实例：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Model Variables</span></span><br><span class="line">weights = slim.model_variable(<span class="string">'weights'</span>,</span><br><span class="line">                            shape=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">3</span> , <span class="number">3</span>],</span><br><span class="line">                            initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>),</span><br><span class="line">                            regularizer=slim.l2_regularizer(<span class="number">0.05</span>),</span><br><span class="line">                            device=<span class="string">'/CPU:0'</span>)</span><br><span class="line">model_variables = slim.get_model_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Regular variables</span></span><br><span class="line">my_var = slim.variable(<span class="string">'my_var'</span>,</span><br><span class="line">                    shape=[<span class="number">20</span>, <span class="number">1</span>],</span><br><span class="line">                    initializer=tf.zeros_initializer())</span><br><span class="line">regular_variables_and_model_variables = slim.get_variables()</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记9：VGGNet-16</title>
      <link href="/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/"/>
      <url>/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B09/</url>
      
        <content type="html"><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><p>VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind公司一起研发的深度卷积神经网络。VGGNet探索了卷积神经网络的深度和其性能之间的关系，通过反复堆叠3×3的小型卷积核和2×2的最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络。到目前为止，VGGNet还经常被用来提取图像的特征。VGGNet训练后的模型参数在其官方网站上开源了，可用来在domain specific的图像分类任务上进行再训练(相当于提供了非常好的初始化权重)。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/a259vKoJSqwF.png?imageslim" alt="mark"></p><p>VGGNet有5段卷积，每一段内有2~3个卷积层，每段卷积的尾部会连接一个最大池化层来缩小图片的尺寸。每段内的卷积核数量一样，越靠后的段的卷积核数量越多，卷积核数量核段的关系:64-128-256-512-512。在段内有多个完全一样的3×3的卷积层堆叠在一起的情况，在卷积神经网络中这其实是一种非常有用的设计。两个3×3的卷积层串联相当于1个5×5的卷积层，即一个像素会和周围5×5的像素产生关联，也就是感受野为5×5。而3个3×3的卷积层串联的效果则相当于1个7×7的卷积层。同时，3个3×3卷积层比1个7×7的卷积层有着更少的参数，(3<em>3</em>3)/(7*7)=55%。最重要的是，3个3×3的卷积层拥有比1个7×7的卷积层更多的非线性变换，3个3×3的卷积层使用了3次RELU激活函数，而1个7×7的卷积层只使用了1次，这样可以让卷积神经网络对特征的学习能力更强。</p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>卷积层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义卷积层函数</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name：该层的名称</span></span><br><span class="line"><span class="string">kh:卷积核的高</span></span><br><span class="line"><span class="string">kw:卷积核的宽</span></span><br><span class="line"><span class="string">n_out:卷积核的数量(输出通道数)</span></span><br><span class="line"><span class="string">dh:步长的高</span></span><br><span class="line"><span class="string">dw:步长的宽</span></span><br><span class="line"><span class="string">p:参数列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_op</span><span class="params">(input_op,name,kh,kw,n_out,dh,dw,p)</span>:</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel = tf.get_variable(scope+<span class="string">"w"</span>,shape=[kh,kw,n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer_conv2d())</span><br><span class="line">                                 </span><br><span class="line">        <span class="comment">#卷积</span></span><br><span class="line">        conv = tf.nn.conv2d(input_op,kernel,(<span class="number">1</span>,dh,dw,<span class="number">1</span>),padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        bias_init_val = tf.constant(<span class="number">0.0</span>,shape=[n_out],dtype=tf.float32)</span><br><span class="line">        biases = tf.Variable(bias_init_val,trainable=<span class="literal">True</span>,name=<span class="string">"b"</span>)</span><br><span class="line">        z = tf.nn.bias_add(conv,biases)</span><br><span class="line">        activation = tf.nn.relu(z,name=scope)</span><br><span class="line">        <span class="comment">#保存参数</span></span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">    <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure></li><li><p>全连接层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义全连接层函数</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name:该层的名称</span></span><br><span class="line"><span class="string">n_out:输出的通道数</span></span><br><span class="line"><span class="string">p:参数列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_op</span><span class="params">(input_op,name,n_out,p)</span>:</span></span><br><span class="line">    n_in = input_op.get_shape()[<span class="number">-1</span>].value</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(name) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化全连接的权重</span></span><br><span class="line">        kernel = tf.get_variable(scope+<span class="string">"w"</span>,shape=[n_in,n_out],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer())              </span><br><span class="line">        <span class="comment">#初始化全连接层的偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.1</span>,shape=[n_out],dtype=tf.float32),name=<span class="string">"b"</span>)</span><br><span class="line">        <span class="comment">#将输入与权重的乘法和偏置的加法合并</span></span><br><span class="line">        activation = tf.nn.relu_layer(input_op,kernel,biases,name=scope)</span><br><span class="line">        <span class="comment">#保存参数</span></span><br><span class="line">        p += [kernel,biases]</span><br><span class="line">        <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure></li><li><p>最大池化层函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">定义最大池化层</span></span><br><span class="line"><span class="string">input_op:输入的tensor</span></span><br><span class="line"><span class="string">name:该层的名称</span></span><br><span class="line"><span class="string">kh:池化层的高</span></span><br><span class="line"><span class="string">kw:池化层的宽</span></span><br><span class="line"><span class="string">dh:步长的高</span></span><br><span class="line"><span class="string">dw:步长的宽</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(input_op,name,kh,kw,dh,dw)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(input_op,ksize=[<span class="number">1</span>,kh,kw,<span class="number">1</span>],strides=[<span class="number">1</span>,dh,dw,<span class="number">1</span>],padding=<span class="string">"SAME"</span>,name=name)</span><br></pre></td></tr></table></figure></li><li><p>VGG实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">VGG16</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_op</span><span class="params">(input_op,keep_prob)</span>:</span></span><br><span class="line">    p = []</span><br><span class="line">    <span class="comment">#第一层的第一层卷积</span></span><br><span class="line">    conv1_1 = conv_op(input_op,name=<span class="string">"conv1_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第一层的第二层卷积</span></span><br><span class="line">    conv1_2 = conv_op(conv1_1,name=<span class="string">"conv1_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">64</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#最大池化层</span></span><br><span class="line">    pool1 = max_pool(conv1_2,name=<span class="string">"pool1"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dw=<span class="number">2</span>,dh=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第二层的第一层卷积</span></span><br><span class="line">    conv2_1 = conv_op(pool1,name=<span class="string">"conv2_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第二层的第二层卷积</span></span><br><span class="line">    conv2_2 = conv_op(conv2_1,name=<span class="string">"conv2_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">128</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    <span class="comment">#第二层的最大池化</span></span><br><span class="line">    pool2 = max_pool(conv2_2,name=<span class="string">"pool2"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第三层</span></span><br><span class="line">    conv3_1 = conv_op(pool2,name=<span class="string">"conv3_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv3_2 = conv_op(conv3_1,name=<span class="string">"conv3_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv3_3 = conv_op(conv3_2,name=<span class="string">"conv3_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">256</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool3 = max_pool(conv3_3,name=<span class="string">"pool3"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第四层</span></span><br><span class="line">    conv4_1 = conv_op(pool3,name=<span class="string">"conv4_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv4_2 = conv_op(conv4_1,name=<span class="string">"conv4_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv4_3 = conv_op(conv4_2,name=<span class="string">"conv4_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool4 = max_pool(conv4_3,name=<span class="string">"pool4"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第五层</span></span><br><span class="line">    conv5_1 = conv_op(pool4,name=<span class="string">"conv5_1"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv5_2 = conv_op(conv5_1,name=<span class="string">"conv5_2"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    conv5_3 = conv_op(conv5_2,name=<span class="string">"conv5_3"</span>,kh=<span class="number">3</span>,kw=<span class="number">3</span>,n_out=<span class="number">512</span>,dh=<span class="number">1</span>,dw=<span class="number">1</span>,p=p)</span><br><span class="line">    pool5 = max_pool(conv5_3,name=<span class="string">"pool5"</span>,kh=<span class="number">2</span>,kw=<span class="number">2</span>,dh=<span class="number">2</span>,dw=<span class="number">2</span>)</span><br><span class="line">    <span class="comment">#将pool5展平</span></span><br><span class="line">    pool5_shape = pool5.get_shape()</span><br><span class="line">    flattened_shape = pool5_shape[<span class="number">1</span>].value * pool5_shape[<span class="number">2</span>].value * pool5_shape[<span class="number">3</span>].value</span><br><span class="line">    resh1 = tf.reshape(pool5,[<span class="number">-1</span>,flattened_shape],name=<span class="string">"resh1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    fc6 = fc_op(resh1,name=<span class="string">"fc6"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc6_drop = tf.nn.dropout(fc6,keep_prob,name=<span class="string">"fc6_drop"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#全连接层</span></span><br><span class="line">    fc7 = fc_op(fc6_drop,name=<span class="string">"fc7"</span>,n_out=<span class="number">4096</span>,p=p)</span><br><span class="line">    fc7_drop = tf.nn.dropout(fc7,keep_prob,name=<span class="string">"fc7_drop"</span>)</span><br><span class="line"></span><br><span class="line">    fc8 = fc_op(fc7_drop,name=<span class="string">"fc8"</span>,n_out=<span class="number">1000</span>,p=p)</span><br><span class="line">    softmax = tf.nn.softmax(fc8)</span><br><span class="line">    predictions = tf.argmax(softmax,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions,softmax,fc8,p</span><br></pre></td></tr></table></figure></li><li><p>性能统计 性能统计模块主要统计网络迭代一次所需时间，由于刚开始运行程序的时候GPU需要加载内存会比较慢，所以统计通10次迭代以后才开始。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_batches = <span class="number">100</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,feed,info_string)</span>:</span></span><br><span class="line">    num_steps_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches + num_steps_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _= session.run(target,feed_dict=feed)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt; num_steps_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span>:</span><br><span class="line">                print(<span class="string">"%s：step:%d,duration:%.3f"</span>%(datetime.now(),i-num_steps_burn_in,duration))</span><br><span class="line">                total_duration += duration</span><br><span class="line">                total_duration_squared += duration * duration</span><br><span class="line">    mn = total_duration / num_batches</span><br><span class="line">    vr = total_duration_squared / num_batches - mn * mn</span><br><span class="line">    sd = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s：%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_batches,mn,sd))</span><br></pre></td></tr></table></figure></li><li><p>训练过程 通过使用random_normal来随机产生224×224的图片，进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_benchmark</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        image_size = <span class="number">224</span></span><br><span class="line">        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,<span class="number">3</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>))</span><br><span class="line">        keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">        predictions,softmax,fc8,p=inference_op(images,keep_prob)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        time_tensorflow_run(sess,predictions,&#123;keep_prob:<span class="number">1.0</span>&#125;,<span class="string">"Forward"</span>)</span><br><span class="line">        objective = tf.nn.l2_loss(fc8)</span><br><span class="line">        grad = tf.gradients(objective,p)</span><br><span class="line">        time_tensorflow_run(sess,grad,&#123;keep_prob:<span class="number">0.5</span>&#125;,<span class="string">"Forward-backward"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    run_benchmark()</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> VGGNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记8：AlexNet</title>
      <link href="/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/"/>
      <url>/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B08/</url>
      
        <content type="html"><![CDATA[<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>AlexNet是Hinton的学生Alex Krizhevsky在2012年提出的深度卷积神经网络，它是LeNet一种更深更宽的版本。在AlexNet上首次应用了几个trick，ReLU、Dropout和LRN。AlexNet包含了6亿3000万个连接，6000万个参数和65万个神经元，有5个卷积层，3个全连接层。在ILSVRC 2012比赛中，AlexNet以top-5的错误率为16.4%的显著优势夺得冠军，第二名的成绩是26.2%。AlexNet的trick主要包括：</p><a id="more"></a><ol><li>成功使用RELU作为CNN的激活函数，并验证其效果在较深的网络中的效果超过了sigmoid，解决了sigmoid在深层的网络中的梯度弥散的问题。</li><li>使用Dropout来随机使得一部分神经元失活，来避免模型的过拟合，在AlexNet中，dropout主要应用在全连接层。</li><li>使用重叠的最大池化，以前在卷积神经网络中大部分都采用平均池化，在AlexNet中都是使用最大池化，最大池化可以避免平均池化的模糊化效果。重叠的最大池化是指卷积核的尺寸要大于步长，这样池化层的输出之间会有重叠和覆盖，提升特征的丰富性。在AlexNet中使用的卷积核大小为3×3，横向和纵向的步长都为2。</li><li>使用LRN层，对局部神经元的活动创建有竞争机制，让响应较大的值变得相对更大，并抑制反馈较小的神经元，来增强模型的泛化能力。</li><li>使用了CUDA来加速深度神经网络的训练。</li><li>数据增强，随机从256×256的原始图像中截取224×224的图像以及随机翻转。如果没有数据增强，在参数众多的情况下，卷积神经网络会陷入到过拟合中，使用数据增强可以减缓过拟合，提升泛化能力。进行预测的时候，提取图片的四个角加中间位置，并进行左右翻转，一共10张图片，对它们进行预测并取10次结果的平均值。在AlexNet论文中也提到了，对图像的RGB数据进行PCA处理，并做一个标准差为0.1的高斯扰动，增加一些噪声，可以降低1%的错误率。</li></ol><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/oqz082I49HMR.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>第一层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv1"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#设置卷积核11×11,3通道,64个卷积核</span></span><br><span class="line">    kernel1 = tf.Variable(tf.truncated_normal([<span class="number">11</span>,<span class="number">11</span>,<span class="number">3</span>,<span class="number">64</span>],mean=<span class="number">0</span>,stddev=<span class="number">0.1</span>,</span><br><span class="line">                                              dtype=tf.float32),name=<span class="string">"weights"</span>)</span><br><span class="line">    <span class="comment">#卷积,卷积的横向步长和竖向补偿都为4</span></span><br><span class="line">    conv = tf.nn.conv2d(images,kernel1,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0</span>,shape=[<span class="number">64</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活函数</span></span><br><span class="line">    conv1 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    <span class="comment">#输出该层的信息</span></span><br><span class="line">    print_tensor_info(conv1)</span><br><span class="line">    <span class="comment">#统计参数</span></span><br><span class="line">    parameters += [kernel1,biases]</span><br><span class="line">    <span class="comment">#lrn处理</span></span><br><span class="line">    lrn1 = tf.nn.lrn(conv1,<span class="number">4</span>,bias=<span class="number">1</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn1"</span>)</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool1 = tf.nn.max_pool(lrn1,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool1"</span>)</span><br><span class="line">    print_tensor_info(pool1)</span><br></pre></td></tr></table></figure></li><li><p>第二层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv2"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel2 = tf.Variable(tf.truncated_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                          ,name=<span class="string">"weights"</span>)</span><br><span class="line">    conv = tf.nn.conv2d(pool1,kernel2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    <span class="comment">#初始化偏置</span></span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0</span>,dtype=tf.float32,shape=[<span class="number">192</span>])</span><br><span class="line">                         ,trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活</span></span><br><span class="line">    conv2 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    print_tensor_info(conv2)</span><br><span class="line">    parameters += [kernel2,biases]</span><br><span class="line">    <span class="comment">#LRN</span></span><br><span class="line">    lrn2 = tf.nn.lrn(conv2,<span class="number">4</span>,<span class="number">1.0</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn2"</span>)</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool2 = tf.nn.max_pool(lrn2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool2"</span>)</span><br><span class="line">    print_tensor_info(pool2)</span><br></pre></td></tr></table></figure></li><li><p>第三层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv3"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">       <span class="comment">#初始化权重</span></span><br><span class="line">       kernel3 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">192</span>,<span class="number">384</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                             ,name=<span class="string">"weights"</span>)</span><br><span class="line">       conv = tf.nn.conv2d(pool2,kernel3,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">       biases = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">384</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">       bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">       <span class="comment">#RELU激活层</span></span><br><span class="line">       conv3 = tf.nn.relu(bias,name=scope)</span><br><span class="line">       parameters += [kernel3,biases]</span><br><span class="line">       print_tensor_info(conv3)</span><br></pre></td></tr></table></figure></li><li><p>第四层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv4"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel4 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">384</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weights"</span>)</span><br><span class="line">    <span class="comment">#卷积</span></span><br><span class="line">    conv = tf.nn.conv2d(conv3,kernel4,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#RELU激活</span></span><br><span class="line">    conv4 = tf.nn.relu(bias,name=scope)</span><br><span class="line">    parameters += [kernel4,biases]</span><br><span class="line">    print_tensor_info(conv4)</span><br></pre></td></tr></table></figure></li><li><p>第五层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"conv5"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    <span class="comment">#初始化权重</span></span><br><span class="line">    kernel5 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weights"</span>)</span><br><span class="line">    conv = tf.nn.conv2d(conv4,kernel5,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">    biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),name=<span class="string">"biases"</span>)</span><br><span class="line">    bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">    <span class="comment">#REUL激活层</span></span><br><span class="line">    conv5 = tf.nn.relu(bias)</span><br><span class="line">    parameters += [kernel5,bias]</span><br><span class="line">    <span class="comment">#最大池化</span></span><br><span class="line">    pool5 = tf.nn.max_pool(conv5,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool5"</span>)</span><br><span class="line">    print_tensor_info(pool5)</span><br></pre></td></tr></table></figure></li><li><p>最后三层全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第六层全连接层</span></span><br><span class="line">pool5 = tf.reshape(pool5,(<span class="number">-1</span>,<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>))</span><br><span class="line">weight6 = tf.Variable(tf.truncated_normal([<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                       name=<span class="string">"weight6"</span>)</span><br><span class="line">ful_bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias1"</span>)</span><br><span class="line">ful_con1 = tf.nn.relu(tf.add(tf.matmul(pool5,weight6),ful_bias1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第七层第二层全连接层</span></span><br><span class="line">weight7 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                      name=<span class="string">"weight7"</span>)</span><br><span class="line">ful_bias2 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias2"</span>)</span><br><span class="line">ful_con2 = tf.nn.relu(tf.add(tf.matmul(ful_con1,weight7),ful_bias2))</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#第八层第三层全连接层</span></span><br><span class="line">weight8 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                      name=<span class="string">"weight8"</span>)</span><br><span class="line">ful_bias3 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">1000</span>]),name=<span class="string">"ful_bias3"</span>)</span><br><span class="line">ful_con3 = tf.nn.relu(tf.add(tf.matmul(ful_con2,weight8),ful_bias3))</span><br></pre></td></tr></table></figure></li><li><p>softmax层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weight9 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>),dtype=tf.float32,name=<span class="string">"weight9"</span>)</span><br><span class="line">bias9 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>]),dtype=tf.float32,name=<span class="string">"bias9"</span>)</span><br><span class="line">output_softmax = tf.nn.softmax(tf.matmul(ful_con3,weight9)+bias9)</span><br></pre></td></tr></table></figure></li><li><p>评估模型性能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,info_string)</span>:</span></span><br><span class="line">    <span class="comment">#前10次迭代不计入时间消耗</span></span><br><span class="line">    num_step_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_bathes + num_step_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_step_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span> :</span><br><span class="line">                print(<span class="string">"%s:step %d,duration=%.3f"</span>%(datetime.now(),i-num_step_burn_in,duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_squared += duration * duration</span><br><span class="line">    <span class="comment">#计算消耗时间的平均差</span></span><br><span class="line">    mn = total_duration / num_bathes</span><br><span class="line">    <span class="comment">#计算消耗时间的标准差</span></span><br><span class="line">    vr = total_duration_squared / num_bathes - mn * mn</span><br><span class="line">    std = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s:%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_bathes,mn,std))</span><br></pre></td></tr></table></figure></li></ol><h1 id="具体代码"><a href="#具体代码" class="headerlink" title="具体代码"></a>具体代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> math,time</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_bathes = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">获取tensor信息</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_tensor_info</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    print(<span class="string">"tensor name:"</span>,tensor.op.name,<span class="string">"-tensor shape:"</span>,tensor.get_shape().as_list())</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">计算每次迭代消耗时间</span></span><br><span class="line"><span class="string">session:TensorFlow的Session</span></span><br><span class="line"><span class="string">target:需要评测的运算算子</span></span><br><span class="line"><span class="string">info_string:测试的名称</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_tensorflow_run</span><span class="params">(session,target,info_string)</span>:</span></span><br><span class="line">    <span class="comment">#前10次迭代不计入时间消耗</span></span><br><span class="line">    num_step_burn_in = <span class="number">10</span></span><br><span class="line">    total_duration = <span class="number">0.0</span></span><br><span class="line">    total_duration_squared = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_bathes + num_step_burn_in):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        _ = session.run(target)</span><br><span class="line">        duration = time.time() - start_time</span><br><span class="line">        <span class="keyword">if</span> i &gt;= num_step_burn_in:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> i % <span class="number">10</span> :</span><br><span class="line">                print(<span class="string">"%s:step %d,duration=%.3f"</span>%(datetime.now(),i-num_step_burn_in,duration))</span><br><span class="line">            total_duration += duration</span><br><span class="line">            total_duration_squared += duration * duration</span><br><span class="line">    <span class="comment">#计算消耗时间的平均差</span></span><br><span class="line">    mn = total_duration / num_bathes</span><br><span class="line">    <span class="comment">#计算消耗时间的标准差</span></span><br><span class="line">    vr = total_duration_squared / num_bathes - mn * mn</span><br><span class="line">    std = math.sqrt(vr)</span><br><span class="line">    print(<span class="string">"%s:%s across %d steps,%.3f +/- %.3f sec / batch"</span>%(datetime.now(),info_string,num_bathes,</span><br><span class="line">                                                             mn,std))</span><br><span class="line"><span class="comment">#主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_bechmark</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">        image_size = <span class="number">224</span></span><br><span class="line">        <span class="comment">#以高斯分布产生一些图片</span></span><br><span class="line">        images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,<span class="number">3</span>],</span><br><span class="line">                                              dtype=tf.float32,stddev=<span class="number">0.1</span>))</span><br><span class="line">        output,parameters = inference(images)</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess = tf.Session()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        time_tensorflow_run(sess,output,<span class="string">"Forward"</span>)</span><br><span class="line">        objective = tf.nn.l2_loss(output)</span><br><span class="line">        grad = tf.gradients(objective,parameters)</span><br><span class="line">        time_tensorflow_run(sess,grad,<span class="string">"Forward-backward"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(images)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#定义参数</span></span><br><span class="line">    parameters = []</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第一层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv1"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#设置卷积核11×11,3通道,64个卷积核</span></span><br><span class="line">        kernel1 = tf.Variable(tf.truncated_normal([<span class="number">11</span>,<span class="number">11</span>,<span class="number">3</span>,<span class="number">64</span>],mean=<span class="number">0</span>,stddev=<span class="number">0.1</span>,</span><br><span class="line">                                                  dtype=tf.float32),name=<span class="string">"weights"</span>)</span><br><span class="line">        <span class="comment">#卷积,卷积的横向步长和竖向补偿都为4</span></span><br><span class="line">        conv = tf.nn.conv2d(images,kernel1,[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0</span>,shape=[<span class="number">64</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活函数</span></span><br><span class="line">        conv1 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        <span class="comment">#输出该层的信息</span></span><br><span class="line">        print_tensor_info(conv1)</span><br><span class="line">        <span class="comment">#统计参数</span></span><br><span class="line">        parameters += [kernel1,biases]</span><br><span class="line">        <span class="comment">#lrn处理</span></span><br><span class="line">        lrn1 = tf.nn.lrn(conv1,<span class="number">4</span>,bias=<span class="number">1</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn1"</span>)</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool1 = tf.nn.max_pool(lrn1,ksize=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool1"</span>)</span><br><span class="line">        print_tensor_info(pool1)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第二层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv2"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel2 = tf.Variable(tf.truncated_normal([<span class="number">5</span>,<span class="number">5</span>,<span class="number">64</span>,<span class="number">192</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                              ,name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(pool1,kernel2,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        <span class="comment">#初始化偏置</span></span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0</span>,dtype=tf.float32,shape=[<span class="number">192</span>])</span><br><span class="line">                             ,trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活</span></span><br><span class="line">        conv2 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        print_tensor_info(conv2)</span><br><span class="line">        parameters += [kernel2,biases]</span><br><span class="line">        <span class="comment">#LRN</span></span><br><span class="line">        lrn2 = tf.nn.lrn(conv2,<span class="number">4</span>,<span class="number">1.0</span>,alpha=<span class="number">1e-3</span>/<span class="number">9</span>,beta=<span class="number">0.75</span>,name=<span class="string">"lrn2"</span>)</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool2 = tf.nn.max_pool(lrn2,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool2"</span>)</span><br><span class="line">        print_tensor_info(pool2)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第三层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv3"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel3 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">192</span>,<span class="number">384</span>],dtype=tf.float32,stddev=<span class="number">0.1</span>)</span><br><span class="line">                              ,name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(pool2,kernel3,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">384</span>],dtype=tf.float32),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活层</span></span><br><span class="line">        conv3 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        parameters += [kernel3,biases]</span><br><span class="line">        print_tensor_info(conv3)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第四层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv4"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel4 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">384</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                              name=<span class="string">"weights"</span>)</span><br><span class="line">        <span class="comment">#卷积</span></span><br><span class="line">        conv = tf.nn.conv2d(conv3,kernel4,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),trainable=<span class="literal">True</span>,name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#RELU激活</span></span><br><span class="line">        conv4 = tf.nn.relu(bias,name=scope)</span><br><span class="line">        parameters += [kernel4,biases]</span><br><span class="line">        print_tensor_info(conv4)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第五层卷积层</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"conv5"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">        <span class="comment">#初始化权重</span></span><br><span class="line">        kernel5 = tf.Variable(tf.truncated_normal([<span class="number">3</span>,<span class="number">3</span>,<span class="number">256</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                              name=<span class="string">"weights"</span>)</span><br><span class="line">        conv = tf.nn.conv2d(conv4,kernel5,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line">        biases = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">256</span>]),name=<span class="string">"biases"</span>)</span><br><span class="line">        bias = tf.nn.bias_add(conv,biases)</span><br><span class="line">        <span class="comment">#REUL激活层</span></span><br><span class="line">        conv5 = tf.nn.relu(bias)</span><br><span class="line">        parameters += [kernel5,bias]</span><br><span class="line">        <span class="comment">#最大池化</span></span><br><span class="line">        pool5 = tf.nn.max_pool(conv5,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"VALID"</span>,name=<span class="string">"pool5"</span>)</span><br><span class="line">        print_tensor_info(pool5)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第六层全连接层</span></span><br><span class="line">    pool5 = tf.reshape(pool5,(<span class="number">-1</span>,<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>))</span><br><span class="line">    weight6 = tf.Variable(tf.truncated_normal([<span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                           name=<span class="string">"weight6"</span>)</span><br><span class="line">    ful_bias1 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias1"</span>)</span><br><span class="line">    ful_con1 = tf.nn.relu(tf.add(tf.matmul(pool5,weight6),ful_bias1))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#第七层第二层全连接层</span></span><br><span class="line">    weight7 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">4096</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weight7"</span>)</span><br><span class="line">    ful_bias2 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">4096</span>]),name=<span class="string">"ful_bias2"</span>)</span><br><span class="line">    ful_con2 = tf.nn.relu(tf.add(tf.matmul(ful_con1,weight7),ful_bias2))</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#第八层第三层全连接层</span></span><br><span class="line">    weight8 = tf.Variable(tf.truncated_normal([<span class="number">4096</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>,dtype=tf.float32),</span><br><span class="line">                          name=<span class="string">"weight8"</span>)</span><br><span class="line">    ful_bias3 = tf.Variable(tf.constant(<span class="number">0.0</span>,dtype=tf.float32,shape=[<span class="number">1000</span>]),name=<span class="string">"ful_bias3"</span>)</span><br><span class="line">    ful_con3 = tf.nn.relu(tf.add(tf.matmul(ful_con2,weight8),ful_bias3))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#softmax层</span></span><br><span class="line">    weight9 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>),dtype=tf.float32,name=<span class="string">"weight9"</span>)</span><br><span class="line">    bias9 = tf.Variable(tf.constant(<span class="number">0.0</span>,shape=[<span class="number">10</span>]),dtype=tf.float32,name=<span class="string">"bias9"</span>)</span><br><span class="line">    output_softmax = tf.nn.softmax(tf.matmul(ful_con3,weight9)+bias9)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_softmax,parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    run_bechmark()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> AlexNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记7：卷积神经网络</title>
      <link href="/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/"/>
      <url>/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B07/</url>
      
        <content type="html"><![CDATA[<h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>用Tensorflow实现一个完整的卷积神经网络，用这个卷积神经网络来识别手写数字数据集（MNIST）。</p><a id="more"></a><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/894wgqHtxSmc.png?imageslim" alt="mark"></p><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><ol><li><p>导入模块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></li><li><p>导入MINIST数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># number 1 to 10 data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="定义Weight和Bias参数"><a href="#定义Weight和Bias参数" class="headerlink" title="定义Weight和Bias参数"></a>定义Weight和Bias参数</h2><ol><li><p>定义产生Weight参数的函数，传入shape，返回Weight参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></li><li><p>定义产生Bias参数的函数，传入shape，返回Bias参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure></li></ol><h2 id="定义卷积和池化操作"><a href="#定义卷积和池化操作" class="headerlink" title="定义卷积和池化操作"></a>定义卷积和池化操作</h2><ol><li><p>定义卷积操作。tf.nn.conv2d函数是Tensorflow里面的二维的卷积函数，x是图片的所有参数，W是卷积层的权重，然后定义步长strides=[1,1,1,1]值。strides[0]和strides[3]的两个1是默认值，意思是不对样本个数和channel进行卷积，中间两个1代表padding是在x方向运动一步，y方向运动一步，padding采用的方式实“SAME”就是0填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="comment"># stride[1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="comment"># Must have strides[0] = strides[3] =1</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)  <span class="comment"># padding="SAME"用零填充边界</span></span><br></pre></td></tr></table></figure></li><li><p>定义池化操作。为了得到更多的图片信息，卷积时我们选择的是一次一步，也就是strides[1]=strides[2]=1,这样得到的图片尺寸没有变化，而我们希望压缩一下图片也就是参数能少一些从而减少系统的复杂度，因此我们采用pooling来稀疏化参数，也就是卷积神经网络中所谓的下采样层。pooling有两种，一种是最大值池化，一种是平均值池化，我采用的是最大值池化tf.max_pool()。池化的核函数大小为2*2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="输入处理"><a href="#输入处理" class="headerlink" title="输入处理"></a>输入处理</h2><ol><li><p>定义输入的placeholder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>]) <span class="comment"># 28*28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br></pre></td></tr></table></figure></li><li><p>定义dropout的placeholder，它是解决过拟合的有效手段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义dropout的输入，解决过拟合问题</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure></li><li><p>处理xs输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理xs，把xs的形状变成[-1,28,28,1]</span></span><br><span class="line"><span class="comment"># -1代表先不考虑输入的图片例子多少这个维度。</span></span><br><span class="line"><span class="comment"># 后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1。如果是RGB图像，那么channel就是3.</span></span><br><span class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li></ol><h2 id="建立卷积层"><a href="#建立卷积层" class="headerlink" title="建立卷积层"></a>建立卷积层</h2><ol><li><p>第一层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>]) <span class="comment"># kernel 5*5, channel is 1, out size 32</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)  <span class="comment"># output size 28*28*32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)                          <span class="comment"># output size 14*14*32</span></span><br></pre></td></tr></table></figure></li><li><p>第二层卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>]) <span class="comment"># kernel 5*5, in size 32, out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  <span class="comment"># output size 14*14*64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)                          <span class="comment"># output size 7*7*64</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="建立全连接层"><a href="#建立全连接层" class="headerlink" title="建立全连接层"></a>建立全连接层</h2><ol><li><p>展平输出。我们通过tf.reshape()将h_pool2的输出值从一个三维的变为一个一维的数据，-1表示先不考虑输入图片例子维度，将上一个输出结果展平。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [n_samples,7,7,64]-&gt;&gt;[n_samples, 7*7*64]</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br></pre></td></tr></table></figure></li><li><p>全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure></li><li><p>同时考虑了过拟合的问题，可以加一个dropout的处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure></li></ol><h2 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h2><ol><li><p>输出层参数，输入为1024，输出为10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br></pre></td></tr></table></figure></li><li><p>softmax分类器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)</span><br></pre></td></tr></table></figure></li></ol><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><ol><li><p>交叉熵损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li><li><p>优化器</p></li></ol><ul><li><p>tf.train.AdamOptimizer()优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></li><li><p>tf.train.GradientDescentOptimizer()优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></li></ul><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><ol><li><p>定义session，初始化变量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess =tf.Session()</span><br><span class="line"><span class="comment"># important step</span></span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br></pre></td></tr></table></figure></li><li><p>训练1000次，每50次检查模型精度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs,batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:batch_xs,ys:batch_ys, keep_prob:<span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># print(sess.run(prediction,feed_dict=&#123;xs:batch_xs&#125;))</span></span><br><span class="line">        print(compute_accuracy(mnist.test.images,mnist.test.labels))</span><br></pre></td></tr></table></figure></li></ol><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment"># 导入本次需要的模块</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># number 1 to 10 data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs,v_ys)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs:v_xs, keep_prob:<span class="number">1</span>&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>),tf.argmax(v_ys,<span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs:v_xs,ys:v_ys,keep_prob:<span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="comment"># stride[1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="comment"># Must have strides[0] = strides[3] =1</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)  <span class="comment"># padding="SAME"用零填充边界</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">"SAME"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># define placeholder for inputs to network</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">784</span>]) <span class="comment"># 28*28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>,<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 定义dropout的输入，解决过拟合问题</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment"># 处理xs，把xs的形状变成[-1,28,28,1]</span></span><br><span class="line"><span class="comment"># -1代表先不考虑输入的图片例子多少这个维度。</span></span><br><span class="line"><span class="comment"># 后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1。如果是RGB图像，那么channel就是3.</span></span><br><span class="line">x_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># print(x_image.shape) #[n_samples, 28,28,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## convl layer ##</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>]) <span class="comment"># kernel 5*5, channel is 1, out size 32</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)  <span class="comment"># output size 28*28*32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)                          <span class="comment"># output size 14*14*32</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## conv2 layer ##</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>]) <span class="comment"># kernel 5*5, in size 32, out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  <span class="comment"># output size 14*14*64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)                          <span class="comment"># output size 7*7*64</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## funcl layer ##</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># [n_samples,7,7,64]-&gt;&gt;[n_samples, 7*7*64]</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"> </span><br><span class="line"><span class="comment">## func2 layer ##</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># the error between prediction and real data</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[<span class="number">1</span>])) <span class="comment">#loss</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span></span><br><span class="line"> </span><br><span class="line">sess =tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs,batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs:batch_xs,ys:batch_ys, keep_prob:<span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># print(sess.run(prediction,feed_dict=&#123;xs:batch_xs&#125;))</span></span><br><span class="line">        print(compute_accuracy(mnist.test.images,mnist.test.labels))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> MNIST </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记6：逻辑回归</title>
      <link href="/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/"/>
      <url>/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B06/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。</p><a id="more"></a><h1 id="MINIST数据集"><a href="#MINIST数据集" class="headerlink" title="MINIST数据集"></a>MINIST数据集</h1><p>MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成，是一个非常有名的手写体数字识别数据集，在很多资料中，这个数据集都会被用作深度学习的入门样例。 </p><ul><li><p>存储形式<br>共有四个压缩文件<br>train-images-idx3-ubyte.gz: training set images (9912422 bytes)<br>train-labels-idx1-ubyte.gz: training set labels (28881 bytes)<br>t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)<br>t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes) </p></li><li><p>样本个数<br>训练样本：共55000个<br>验证样本：共5000个<br>测试样本：共10000个 </p></li></ul><h1 id="TensorFlow实现"><a href="#TensorFlow实现" class="headerlink" title="TensorFlow实现"></a>TensorFlow实现</h1><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载MINIST数据集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定参数</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输入,784为MINIST数据集的图片大小28*28</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出,10为预测的类别数</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定模型的权重和偏移量</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型的结构</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cross entropy来作为损失函数</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Gradient Descent</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 执行初始化</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练循环</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># 循环每一个batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># 执行optimizer,获得cost</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># 计算平均损失</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># 显示每一轮的结果</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试模型</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算精确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0001 cost= 1.183872078</span><br><span class="line">Epoch: 0002 cost= 0.665350118</span><br><span class="line">Epoch: 0003 cost= 0.552830602</span><br><span class="line">Epoch: 0004 cost= 0.498699041</span><br><span class="line">Epoch: 0005 cost= 0.465488806</span><br><span class="line">Epoch: 0006 cost= 0.442619649</span><br><span class="line">Epoch: 0007 cost= 0.425471577</span><br><span class="line">Epoch: 0008 cost= 0.412201005</span><br><span class="line">Epoch: 0009 cost= 0.401415385</span><br><span class="line">Epoch: 0010 cost= 0.392391824</span><br><span class="line">Epoch: 0011 cost= 0.384738960</span><br><span class="line">Epoch: 0012 cost= 0.378136856</span><br><span class="line">Epoch: 0013 cost= 0.372445326</span><br><span class="line">Epoch: 0014 cost= 0.367273882</span><br><span class="line">Epoch: 0015 cost= 0.362716155</span><br><span class="line">Epoch: 0016 cost= 0.358604888</span><br><span class="line">Epoch: 0017 cost= 0.354853253</span><br><span class="line">Epoch: 0018 cost= 0.351472244</span><br><span class="line">Epoch: 0019 cost= 0.348347617</span><br><span class="line">Epoch: 0020 cost= 0.345449658</span><br><span class="line">Epoch: 0021 cost= 0.342724947</span><br><span class="line">Epoch: 0022 cost= 0.340273546</span><br><span class="line">Epoch: 0023 cost= 0.337938625</span><br><span class="line">Epoch: 0024 cost= 0.335751063</span><br><span class="line">Epoch: 0025 cost= 0.333709621</span><br><span class="line">Optimization Finished!</span><br><span class="line">Accuracy: 0.9138</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensoeFlow学习笔记5：线性回归</title>
      <link href="/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/"/>
      <url>/2019/08/08/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。</p><a id="more"></a><h1 id="简单实现"><a href="#简单实现" class="headerlink" title="简单实现"></a>简单实现</h1><p>具体代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">num_points=<span class="number">1000</span>    </span><br><span class="line">vectors_set=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_points):</span><br><span class="line">    <span class="comment"># 横坐标，进行随机高斯处理化，以0为均值，以0.55为标准差</span></span><br><span class="line">    x1=np.random.normal(<span class="number">0.0</span>,<span class="number">0.55</span>)</span><br><span class="line">    <span class="comment"># 纵坐标，数据点在y1=x1*0.1+0.3上小范围浮动</span></span><br><span class="line">    y1=x1*<span class="number">0.1</span>+<span class="number">0.3</span>+np.random.normal(<span class="number">0.0</span>,<span class="number">0.03</span>)</span><br><span class="line">    vectors_set.append([x1,y1])</span><br><span class="line">    x_data=[v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors_set]</span><br><span class="line">    y_data=[v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors_set]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成1维的W矩阵，取值是[-1,1]之间的随机数</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>), name=<span class="string">'W'</span>)</span><br><span class="line"><span class="comment"># 生成1维的b矩阵，初始值是0</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]), name=<span class="string">'b'</span>)</span><br><span class="line"><span class="comment"># 经过计算得出预估值y</span></span><br><span class="line">y = W * x_data + b</span><br><span class="line"><span class="comment"># 以预估值y和实际值y_data之间的均方误差作为损失</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data), name=<span class="string">'loss'</span>)</span><br><span class="line"><span class="comment"># 采用梯度下降法来优化参数  学习率为0.5</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># 训练的过程就是最小化这个误差值</span></span><br><span class="line">train = optimizer.minimize(loss, name=<span class="string">'train'</span>)</span><br><span class="line"><span class="comment"># sess = tf.Session()</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 执行20次训练</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        sess.run(train) <span class="comment"># 输出训练好的W和b</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"W ="</span>, sess.run(W), <span class="string">"b ="</span>, sess.run(b), <span class="string">"loss ="</span>, sess.run(loss))</span><br><span class="line"></span><br><span class="line">    plt.scatter(x_data,y_data,c=<span class="string">'r'</span>)</span><br><span class="line">    plt.plot(x_data,sess.run(y))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">W = [0.41536754] b = [0.29331774] loss = 0.028788242</span><br><span class="line">W = [0.32664812] b = [0.29488927] loss = 0.015261736</span><br><span class="line">W = [0.26295245] b = [0.29606903] loss = 0.008289363</span><br><span class="line">W = [0.2172218] b = [0.29691604] loss = 0.004695383</span><br><span class="line">W = [0.18438922] b = [0.29752415] loss = 0.002842829</span><br><span class="line">W = [0.1608169] b = [0.29796076] loss = 0.0018879117</span><br><span class="line">W = [0.143893] b = [0.29827422] loss = 0.0013956894</span><br><span class="line">W = [0.1317424] b = [0.2984993] loss = 0.0011419685</span><br><span class="line">W = [0.12301882] b = [0.29866084] loss = 0.0010111856</span><br><span class="line">W = [0.11675566] b = [0.29877687] loss = 0.00094377215</span><br><span class="line">W = [0.112259] b = [0.29886016] loss = 0.0009090233</span><br><span class="line">W = [0.1090306] b = [0.29891995] loss = 0.0008911116</span><br><span class="line">W = [0.10671275] b = [0.29896286] loss = 0.00088187883</span><br><span class="line">W = [0.10504864] b = [0.2989937] loss = 0.0008771197</span><br><span class="line">W = [0.10385388] b = [0.29901582] loss = 0.00087466656</span><br><span class="line">W = [0.1029961] b = [0.2990317] loss = 0.00087340205</span><br><span class="line">W = [0.10238025] b = [0.29904312] loss = 0.0008727502</span><br><span class="line">W = [0.10193809] b = [0.2990513] loss = 0.0008724143</span><br><span class="line">W = [0.10162064] b = [0.2990572] loss = 0.0008722411</span><br><span class="line">W = [0.10139273] b = [0.29906142] loss = 0.00087215187</span><br></pre></td></tr></table></figure></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/buv6SxKq3CxD.png?imageslim" alt="mark"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记4：损失函数</title>
      <link href="/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/"/>
      <url>/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04/</url>
      
        <content type="html"><![CDATA[<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>损失函数（loss function）是机器学习中非常重要的内容，它是度量模型输出值与目标值的差异，也就是作为评估模型效果的一种重要指标，损失函数越小，表明模型的鲁棒性就越好。</p><a id="more"></a><h1 id="使用损失函数"><a href="#使用损失函数" class="headerlink" title="使用损失函数"></a>使用损失函数</h1><p>在TensorFlow中训练模型时，通过损失函数告诉TensorFlow预测结果相比目标结果是好还是坏。在多种情况下，我们会给出模型训练的样本数据和目标数据，损失函数即是比较预测值与给定的目标值之间的差异。</p><p>下面将介绍在TensorFlow中常用的损失函数。</p><ul><li><p>L1正则损失函数(即绝对值损失函数)</p><script type="math/tex; mode=display">L(Y,f(X))=|Y-f(X)|</script><p>TensorFlow实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_L1_vals = tf.abs(y_pred-y_target)</span><br></pre></td></tr></table></figure></li><li><p>L2正则损失函数(即欧拉损失函数)<br>L2正则损失函数是预测值与目标值差值的平方和，公式如下：</p><script type="math/tex; mode=display">L(Y,f(X))=\sum_{i=1}^{n}(Y-f(X))^2</script><p>TensorFlow实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_L2_vals = tf.square(y_pred-y_target)</span><br></pre></td></tr></table></figure></li><li><p>均方误差（MSE，mean squared error)<br>对L2取平均值就变成了均方误差，公式如下：</p><script type="math/tex; mode=display">MSE(y,y')=\frac{\sum_{i=1}^{n}(y_i-y_i')^2}{n}</script><p>TensorFlow实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_mse_vals = tf.reduce.mean(tf.square(y_pred-y_target))</span><br></pre></td></tr></table></figure></li><li><p>交叉熵(Cross Entropy)损失函数<br>交叉熵刻画了两个概率分布之间的距离，是分类问题中使用广泛的损失函数。给定两个概率分布p和q，交叉熵刻画的是两个概率分布之间的距离，公式如下：</p><script type="math/tex; mode=display">H(X=x)=-\sum_x p(x)logq(x)</script><p>TensorFlow实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 注意y_pred需要先经过softmax处理</span><br><span class="line">cross_entropy = -tf.reduce_mean(y_target*tf.log(tf.clip_by_value(y_pred,1e-10,1.0)))</span><br></pre></td></tr></table></figure><p>tf.clip_by_value()函数可将一个tensor的元素数值限制在指定范围内，这样可防止一些错误运算，起到数值检查作用。</p></li></ul><h1 id="TensorFlow的Cross-Entropy实现"><a href="#TensorFlow的Cross-Entropy实现" class="headerlink" title="TensorFlow的Cross_Entropy实现"></a>TensorFlow的Cross_Entropy实现</h1><ul><li><p><strong>tf.nn.softmax_cross_entropy_with_logits(_sentinel=None,labels=None, logits=None, dim=-1, name=None)</strong><br>该函数的功能是自动计算logits（未经过Softmax）与labels之间的cross_entropy交叉熵。</p><p>该操作应该施加在未经过Softmax处理的logits上，否则会产生错误结果；labels为期望输出，且必须采用labels=y_,logits=y3的形式将参数传入。</p><p>第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes</p><p>第二个参数labels：实际的标签，大小同上。<br><strong>注意：</strong> 这个函数的返回值并不是一个数，而是一个向量，如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到交叉熵，如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！</p></li><li><p><strong>tf.nn.sparse_softmax_cross_entropy_with_logits(_sentinel=None,labels=None, logits=None, name=None)</strong><br>该函数与tf.nn.softmax_cross_entropy_with_logits()十分相似，唯一的区别在于labels，该函数的标签labels要求是排他性的即只有一个正确类别，labels的形状要求是[batch_size] 而值必须是从0开始编码的int32或int64，而且值范围是[0, num_class)，对比于tf.nn.softmax_cross_entropy_with_logits的[batchsize，num_classes]格式的得分编码。</p></li><li><p><strong>tf.nn.sigmoid_cross_entropy_with_logits(_sentinel=None,labels=None, logits=None, name=None)</strong><br>sigmoid_cross_entropy_with_logits是TensorFlow最早实现的交叉熵算法。这个函数的输入是logits和labels，logits就是神经网络模型中的 W * X矩阵，注意不需要经过sigmoid，而labels的shape和logits相同，就是正确的标签值，例如这个模型一次要判断100张图是否包含10种动物，这两个输入的shape都是[100, 10]。注释中还提到这10个分类之间是独立的、不要求是互斥，这种问题我们称为多目标（多标签）分类，例如判断图片中是否包含10种动物中的一种或几种，标签值可以包含多个1或0个1。</p></li><li><p><strong>tf.nn.weighted_cross_entropy_with_logits(targets, logits, pos_weight, name=None)</strong><br>weighted_sigmoid_cross_entropy_with_logits是sigmoid_cross_entropy_with_logits的拓展版，多支持一个pos_weight参数，在传统基于sigmoid的交叉熵算法上，正样本算出的值乘以某个系数。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 损失函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记3：激励函数</title>
      <link href="/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/"/>
      <url>/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03/</url>
      
        <content type="html"><![CDATA[<h1 id="激励函数的作用"><a href="#激励函数的作用" class="headerlink" title="激励函数的作用"></a>激励函数的作用</h1><p>激励函数的作用就是将多个线性输入转换为非线性的关系。如果不使用激励函数，神经网络的每层都只是做线性变换，即使是多层输入叠加后也还是线性变换。通过使用激励函数引入非线性因素后，使神经网络的表示能力更强了。</p><a id="more"></a><h1 id="使用激励函数"><a href="#使用激励函数" class="headerlink" title="使用激励函数"></a>使用激励函数</h1><p>在TensorFlow中使用激励函数非常方便，激励函数位于神经网络库中（tensorflow.nn），下面介绍使用方法。<br>首先先创建一个会话，使用默认计算图：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure></p><ul><li><p><strong>ReLU函数</strong><br>ReLU（Rectifier linear unit，整流线性单元）是神经网络中最常用的激励函数，函数如下：</p><script type="math/tex; mode=display">max(0,x)</script><p>调用函数<code>tf.nn.relu()</code></p></li><li><p><strong>ReLU6函数</strong><br>引入ReLU6主要是为了抵消ReLU函数的线性增长部分，在ReLU的基础上再加上min，函数如下：</p><script type="math/tex; mode=display">min(max(0,x),6)</script><p>调用函数<code>tf.nn.relu6()</code></p></li><li><p><strong>Leaky ReLU函数</strong><br>引入Leaky ReLU主要是为了避免梯度消失，当神经元处于非激活状态时，允许一个非0的梯度存在，这样不会出现梯度消失，收敛速度快。函数如下：</p><script type="math/tex; mode=display">max(0.1x,x)</script><p>调用函数<code>tf.nn.leaky_relu()</code></p></li><li><p><strong>sigmoid函数</strong><br>sigmoid函数是神经网络中最常用的激励函数，它也被称为逻辑函数，它在深度学习的训练过程中会导致梯度消失，因此在深度学习中不怎么使用。函数如下：</p><script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+e^{-x}}</script><p>调用函数<code>tf.nn.sigmoid()</code></p></li><li><p><strong>tanh函数</strong><br>tanh函数即是双曲正切函数，tanh与sigmoid函数相似，但tanh的取值范围是0到1，sigmoid函数取值范围是-1到1。函数如下：</p><script type="math/tex; mode=display">tanh(x)</script><p>调用函数<code>tf.nn.tanh()</code></p></li><li><p><strong>ELU函数</strong><br>ELU在正值区间的值为x本身，而在负值区间，ELU在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性，函数如下：</p><script type="math/tex; mode=display">\left\{\begin{array}{lr}  x & x\geq 0 \\  \alpha(e^x-1) & x < 0\end{array}\right.</script><p>调用函数<code>tf.nn.elu()</code></p></li><li><p><strong>softsign函数</strong><br>softsign函数是符号函数的连续估计，定义如下：</p><script type="math/tex; mode=display">f(x)=\frac{x}{|x|+1}</script><p>调用函数<code>tf.nn.softsign()</code></p></li><li><p><strong>softplus函数</strong><br>softplus是ReLU激励函数的平滑版，定义如下：</p><script type="math/tex; mode=display">f(x)=log(e^x+1)</script><p>调用函数<code>tf.nn.softplus()</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 激励函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记2：图(Graph)与会话(Session)机制</title>
      <link href="/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/"/>
      <url>/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02/</url>
      
        <content type="html"><![CDATA[<p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/TYchJjHiKGDp.png?imageslim" alt="mark"><br>计算图是TensorFlow的核心概念，使用图（Graph）来表示计算任务，由节点和边组成。TensorFlow由前端负责构建计算图，后端负责执行计算图。<br>为了执行图的计算，图必须在会话（Session）里面启动，会话将图的操作分发到CPU、GPU等设备上执行。<br>下面将介绍如何在TensorFlow里面创建会话、图以及基本操作。</p><a id="more"></a><h1 id="图-Graph"><a href="#图-Graph" class="headerlink" title="图(Graph)"></a>图(Graph)</h1><p>TensorFlow Python库已经有一个默认图 (default graph)，如果没有创建新的计算图，则默认情况下是在这个default graph里面创建节点和边。<br>在图里面添加节点非常方便。例如现在要创建这样的计算图，两个张量相加，如下图：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/fqcuyTov5FHS.png?imageslim" alt="mark"><br>代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf </span><br><span class="line">a=tf.constant([1.0,2.0], name=&apos;a&apos;) </span><br><span class="line">b=tf.constant([3.0,4.0], name=&apos;b&apos;) </span><br><span class="line">result = tf.add(a,b)</span><br></pre></td></tr></table></figure></p><p>现在默认图就有了三个节点，两个constant()，和一个add()。<br>为了真正使两个张量相加并得到结果，就必须在会话里面启动这个图。</p><h1 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h1><h2 id="会话的创建"><a href="#会话的创建" class="headerlink" title="会话的创建"></a>会话的创建</h2><p>要启动计算图，首先要创建一个Session对象。<br>使用tf.Session()创建会话，调用run()函数执行计算图。如果没有传入任何创建参数，会话构造器将启动默认图。如果要指定某个计算图，则传入计算图参数（如g1），则创建会话方式为tf.Session(graph=g1)创建会话（Session）主要有以下三种方式：</p><ol><li><p>创建一个会话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#启动默认图</span><br><span class="line">sess=tf.Session()</span><br><span class="line">result_value = sess.run(result)</span><br><span class="line">print(result_value)</span><br><span class="line"># ==&gt; [4.0 6.0]</span><br><span class="line"></span><br><span class="line"># 任务完成, 关闭会话.</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></li><li><p>创建一个会话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    result_value = sess.run(result)</span><br><span class="line">    print(result_value)</span><br><span class="line">    # ==&gt; [4.0 6.0]</span><br></pre></td></tr></table></figure></li><li><p>创建一个默认会话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess=tf.Session()</span><br><span class="line">with sess.as_default():</span><br><span class="line">    result_value = result.eval()</span><br><span class="line">    print(result_value)</span><br></pre></td></tr></table></figure></li></ol><p>当指定默认会话后，可以通过tf.Tensor.eval函数来计算一个张量的取值。</p><ol><li>创建一个交互式会话<br>在交互式环境下（例如IPython），使用设置默认会话的方式来获取张量的取值更加方便，TensorFlow提供了一种在交互式环境下直接构建默认会话的函数：tf.InteractiveSession，该函数会自动将生成的会话注册为默认会话，使用 tf.Tensor.eval()代替 Session.run()，代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess= tf.InteractiveSession()</span><br><span class="line">result_value = result.eval()</span><br><span class="line">print(result_value)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></li></ol><h2 id="Fetch-取回"><a href="#Fetch-取回" class="headerlink" title="Fetch(取回)"></a>Fetch(取回)</h2><p>在使用sess.run( )运行图时，我们可以传入fetches，用于取回某些操作或tensor的输出内容。fetches可以是list，tuple，namedtuple，dict中的任意一个。fetches可以是一个列表，在op的一次运行中一起获得（而不是逐个去获取 tensor）多个tensor值。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = sess.run(a)            # fetches可以为单个数a</span><br><span class="line">d = sess.run([a, b])       # fetches可以为一个列表[a, b]</span><br></pre></td></tr></table></figure></p><h2 id="Feed-注入"><a href="#Feed-注入" class="headerlink" title="Feed(注入)"></a>Feed(注入)</h2><p>TensorFlow提供了feed注入机制, 它可以临时替代graph中任意op操作的输入tensor，可以对graph中任何操作提交补丁（直接插入一个tensor）。<br>feed机制只在调用它的方法内有效，方法结束，feed就会消失。最常见的用例是把某些特殊操作为feed注入的对象。你可以提供数据feed_dict，作为sess.run( )调用的参数。使用tf.placeholder( )，为某些操作的输入创建占位符。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.ones((2, 3))</span><br><span class="line">y = np.ones((3, 2)) </span><br><span class="line"></span><br><span class="line">input1 = tf.placeholder(tf.int32)</span><br><span class="line">input2 = tf.placeholder(tf.int32)</span><br><span class="line"></span><br><span class="line">output = tf.matmul(input1, input2)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(output, feed_dict = &#123;input1:x, input2:y&#125;))</span><br></pre></td></tr></table></figure></p><p>如果没有正确提供tf.placeholder( )，feed操作将产生错误。注意，feed注入的值不能是tf的tensor对象，应该是Python常量、字符串、列表、numpy ndarrays，或者TensorHandles。</p><h1 id="构建多个计算图"><a href="#构建多个计算图" class="headerlink" title="构建多个计算图"></a>构建多个计算图</h1><p>在TensorFlow中可以构建多个计算图，计算图之间的张量和运算是不会共享的，通过这种方式，可以在同个项目中构建多个网络模型，而相互之间不会受影响。<br>使用tf.Graph()函数构建图，构建多个计算图的方式如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 构建计算图g1</span><br><span class="line">g1=tf.Graph()</span><br><span class="line">with g1.as_default():</span><br><span class="line">    # 在计算图g1中定义变量&apos;v&apos;,并设置初始值为0。</span><br><span class="line">    v=tf.get_variable(&apos;v&apos;,initializer=tf.zeros_initializer()(shape = [1]))</span><br><span class="line">    </span><br><span class="line"># 构建计算图g2</span><br><span class="line">g2=tf.Graph()</span><br><span class="line">with g2.as_default():</span><br><span class="line">    # 在计算图g2中定义变量&apos;v&apos;,并设置初始值微1。</span><br><span class="line">    v=tf.get_variable(&apos;v&apos;,initializer=tf.ones_initializer()(shape = [1]))</span><br><span class="line"></span><br><span class="line"># 在计算图g1中读取变量&apos;v&apos;的取值</span><br><span class="line">with tf.Session(graph=g1) as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    with tf.variable_scope(&apos;&apos;,reuse=True):</span><br><span class="line">        print(sess.run(tf.get_variable(&apos;v&apos;)))</span><br><span class="line">        # 输出结果[0.]</span><br><span class="line"></span><br><span class="line"># 在计算图g2中读取变量&apos;v&apos;的取值</span><br><span class="line">with tf.Session(graph=g2) as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    with tf.variable_scope(&apos;&apos;,reuse=True):</span><br><span class="line">        print(sess.run(tf.get_variable(&apos;v&apos;)))</span><br><span class="line">        # 输出结果[1.]。</span><br></pre></td></tr></table></figure></p><h1 id="指定运行设备"><a href="#指定运行设备" class="headerlink" title="指定运行设备"></a>指定运行设备</h1><p>如果电脑有多个GPU，可以在图、会话中指定要运行的设备</p><ul><li><p><strong>在图中指定运行设备</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">g=tf.Graph()</span><br><span class="line"># 指定计算运行的设备。</span><br><span class="line">with g.device(&apos;/gpu:0&apos;):</span><br><span class="line">    result=tf.add(a,b)</span><br></pre></td></tr></table></figure></li><li><p><strong>在会话中指定运行设备</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">  with tf.device(&quot;/gpu:0&quot;):</span><br><span class="line">    result=tf.add(a,b)</span><br></pre></td></tr></table></figure></li></ul><p>运行的设备用字符串进行标识，目前支持的设备包括：<br>“/cpu:0”: 机器的 CPU<br>“/gpu:0”: 机器的第一个 GPU，如果有的话<br>“/gpu:1”: 机器的第二个 GPU，以此类推</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow学习笔记1：张量与变量</title>
      <link href="/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/"/>
      <url>/2019/08/06/tf%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
      
        <content type="html"><![CDATA[<h1 id="张量的概念"><a href="#张量的概念" class="headerlink" title="张量的概念"></a>张量的概念</h1><p>TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.零阶张量表示标量，一阶张量表示向量，也就是一个以为数组，第n阶张量为矩阵，也就是一个n维数组。<br>但张量在tf中并不是采用数组的形式，只是对TF的运算结果的引用。</p><p>一个张量包含三个属性：名字name ，维度shape，类型 dtype：</p><ul><li>name：张量的唯一标识。命名规范：“node:src_input” 。node 表示图的节点的名称，src_input 表示张量来自节点的第几个输入（从0开始）</li><li>shape：描述张量的维度信息。</li><li>dtype：每一个张量有一个唯一的类型。不同类型计算会报错。</li></ul><a id="more"></a><h2 id="生成张量"><a href="#生成张量" class="headerlink" title="生成张量"></a>生成张量</h2><ul><li><strong>创建固定值张量</strong><br><strong>tf.constant(value, dtype=None, shape=None, name=’Const’)</strong> 创建一个常数张量<br><strong>tf.zeros(shape, dtype=tf.float32, name=None)</strong> 创建所有元素设置为零的张量。此操作返回一个dtype具有形状shape和所有元素设置为零的类型的张量。<br><strong>tf.ones(shape, dtype=tf.float32, name=None)</strong> 创建一个所有元素设置为1的张量。此操作返回一个类型的张量，dtype形状shape和所有元素设置为1。<br><strong>tf.fill(dims, value, name=None)</strong> 创建一个填充了标量值的张量。此操作创建一个张量的形状dims并填充它value。</li><li><p><strong>创建相似形状的张量</strong><br><strong>tf.zeros_like(tensor, dtype=None, name=None)</strong> 给tensor定单张量（），此操作返回tensor与所有元素设置为零相同的类型和形状的张量。<br><strong>tf.ones_like(tensor, dtype=None, name=None)</strong> 给tensor定单张量（），此操作返回tensor与所有元素设置为1 相同的类型和形状的张量。</p></li><li><p><strong>创建随机张量</strong><br><strong>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</strong> 从截断的正态分布中输出随机值，和 tf.random_normal() 一样，但是所有数字都不超过两个标准差<br><strong>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)</strong> 从正态分布中输出随机值，由随机正态分布的数字组成的矩阵<br><strong>randunif_ts=tf.random_uniform(shape,minval=0,maxval=1)</strong> 生成均匀分布的随机数,结果返回从minval（包含）到maxval（不包含）的均匀分布的随机数</p></li></ul><h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1><p>tf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None)</p><ul><li>initial_value:A Tensor或Python对象可转换为a Tensor.变量的初始值.必须具有指定的形状,除非 validate_shape设置为False.</li><li>trainable:如果True，默认值也将该变量添加到图形集合GraphKeys.TRAINABLE_VARIABLES,该集合用作Optimizer类要使用的变量的默认列表</li><li>collections:图表集合键列表,新变量添加到这些集合中.默认为[GraphKeys.VARIABLES]</li><li>validate_shape:如果False允许使用未知形状的值初始化变量,如果True，默认形状initial_value必须提供.</li><li>name:变量的可选名称,默认’Variable’并自动获取</li></ul><h2 id="变量的创建"><a href="#变量的创建" class="headerlink" title="变量的创建"></a>变量的创建</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(5.0,name=&quot;x&quot;)</span><br><span class="line">weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),name=&quot;weights&quot;)</span><br><span class="line">biases = tf.Variable(tf.zeros([200]), name=&quot;biases&quot;)</span><br></pre></td></tr></table></figure><p>调用tf.Variable()向图中添加了几个操作：</p><ul><li>一个variable op保存变量值。</li><li>初始化器op将变量设置为其初始值。这实际上是一个tf.assign操作。</li><li>初始值的ops，例如 示例中biases变量的zeros op 也被添加到图中。</li></ul><h2 id="变量的初始化"><a href="#变量的初始化" class="headerlink" title="变量的初始化"></a>变量的初始化</h2><p>变量的初始化必须在模型的其它操作运行之前先明确地完成。最简单的方法就是添加一个给所有变量初始化的操作，并在使用模型之前首先运行那个操作。最常见的初始化模式是使用便利函数 initialize_all_variables()将Op添加到初始化所有变量的图形中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"> </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">  sess.run(init_op)</span><br></pre></td></tr></table></figure></p><h2 id="通过另一个变量赋值"><a href="#通过另一个变量赋值" class="headerlink" title="通过另一个变量赋值"></a>通过另一个变量赋值</h2><p>你有时候会需要用另一个变量的初始化值给当前变量初始化，由于tf.global_variables_initializer()初始化所有变量，所以需要注意这个方法的使用。<br>就是将已初始化的变量的值赋值给另一个新变量！<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),name=&quot;weights&quot;)</span><br><span class="line"> </span><br><span class="line">w2 = tf.Variable(weights.initialized_value(), name=&quot;w2&quot;)</span><br><span class="line"> </span><br><span class="line">w_twice = tf.Variable(weights.initialized_value() * 0.2, name=&quot;w_twice&quot;)</span><br></pre></td></tr></table></figure></p><h2 id="变量的属性"><a href="#变量的属性" class="headerlink" title="变量的属性"></a>变量的属性</h2><ul><li><p><strong>name</strong><br>返回变量的名字</p></li><li><p><strong>op</strong><br>返回变量所有的的op操作</p></li></ul><h2 id="变量的方法"><a href="#变量的方法" class="headerlink" title="变量的方法"></a>变量的方法</h2><ul><li><p><strong>assign</strong><br>为变量分配一个新值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(5.0,name=&quot;x&quot;)</span><br><span class="line">w.assign(w + 1.0)</span><br></pre></td></tr></table></figure></li><li><p><strong>eval</strong><br>在会话中，计算并返回此变量的值(必须要在初始化后)。这不是一个图形构造方法，它不会向图形添加操作。方便打印结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">v = tf.Variable([1, 2])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"> </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"> </span><br><span class="line">    # 指定会话</span><br><span class="line">    print(v.eval(sess))</span><br><span class="line">    # 使用默认会话</span><br><span class="line">    print(v.eval())</span><br></pre></td></tr></table></figure></li></ul><h2 id="管理图中的变量"><a href="#管理图中的变量" class="headerlink" title="管理图中的变量"></a>管理图中的变量</h2><ul><li><strong>tf.global_variables()</strong><br>返回图中收集的所有变量</li></ul><h1 id="占位符"><a href="#占位符" class="headerlink" title="占位符"></a>占位符</h1><p>占位符和变量是使用TensorFlow计算图的关键工具，两者是有区别的</p><ul><li><strong>变量</strong> 是TensorFlow算法中的参数，通过调整这些变量的状态来优化模型算法；</li><li><strong>占位符</strong> 是TensorFlow对象，用于表示输入输出数据的格式，允许传入指定类型和形状的数据。</li></ul><h2 id="创建占位符"><a href="#创建占位符" class="headerlink" title="创建占位符"></a>创建占位符</h2><p>占位符仅仅是声明数据位置，也即先占个位，后面在会话中通过feed_dict传入具体的数据。示例代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=tf.placeholder(tf.float32,shape=[1,2])</span><br><span class="line">b=tf.placeholder(tf.float32,shape=[1,2])</span><br><span class="line">adder_node=a+b   #这里的“+”是tf.add(a,b)的简洁表达</span><br><span class="line">print(sess.run(adder_node,feed_dict=&#123;a:[2,4],b:[5.2,8]&#125;))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记9：Scale-Aware Trident Networks for Object Detection</title>
      <link href="/2019/08/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B09/"/>
      <url>/2019/08/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B09/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1901.01892" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.01892</a><br>代码地址：<a href="https://github.com/TuSimple/simpledet/tree/master/models/tridentnet" target="_blank" rel="noopener">https://github.com/TuSimple/simpledet/tree/master/models/tridentnet</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>尺度变换是目标检测中的关键挑战。在我们的工作中，我们首先提出了一个控制的实验来研究感受野在检测不同尺寸物体上的影响。基于探索实验的发现，我们提出了一个全新的Trident Net-work (TridentNet)，可以生成具有统一表征能力的特定尺度的feature map，我们构建了一个平行的多分支结构，在每一个分支中分享相同的转换参数，但是具有不同的感受野。然后，我们提出了一种规模感知的训练模式来，通过采样适当比例的对象实例来训练每一个分支。在COCO数据集中，我们的以ResNet-101作为特征提取网络的TridentNet获得了当前最先进的单一模型的结果，48.4的mAP。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>CNN-based方法可以粗略的分为两类：</p><ul><li>one stage method 直接使用前馈神经网络来预测边界框</li><li>two stage method 先生成proposals，然后利用经过CNN提取的区域特征进行更好的refinement</li></ul><p>然而，两种方法的一个中心问题就是怎么处理尺度变换。我们都知道物体的尺寸在很大的范围内变化，这会阻碍检测器的性能，尤其是那些很小或者很大的物体。</p><p>为了补救尺度变化的问题：</p><ul><li>image pyramids<br>从非Deep时代，乃至CV初期就被就被广泛使用的方法叫做image pyramid。在image pyramid中，我们直接对图像进行不同尺度的缩放，然后将这些图像直接输入到detector中去进行检测。虽然这样的方法十分简单，但其效果仍然是最佳，也后续启发了SNIP这一系列的工作。单论性能而言，multi-scale training/testing仍然是一个不可缺少的组件。然而其缺点也是很明显的，测试时间大幅度提高，对于实际使用并不友好。</li><li>feature pyramids<br>另外一大类方法，也是Deep方法所独有的，也就是feature pyramid。最具代表性的工作便是经典的FPN了。这一类方法的思想是直接在feature层面上来近似image pyramid。非Deep时代在检测中便有经典的channel feature这样的方法，这个想法在CNN中其实更加直接，因为本身CNN的feature便是分层次的。从开始的MS-CNN直接在不同downsample层上检测大小不同的物体，再到后续TDM和FPN加入了新的top down分支补充底层的语义信息不足，都是延续类似的想法。然而实际上，这样的近似虽然有效，但是仍然性能和image pyramid有较大差距。</li></ul><p>image pyramids以及feature pyramids的motivation都是一样的，即模型对不同的尺度的object需要具有不同的感受野。尽管并不高效，但image pyramids充分的利用模型的表征能力来平等的转换所有尺寸的objects。而feature pyramids生成多尺度的features因此牺牲了features在不同尺度的一致性。我们的工作的目标就是为所有尺度有效的创建具有统一表征能力的features来获得最好的效果。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/dckUICdChciz.png?imageslim" alt="mark"></p><p>在本文中，我们提出了一个全新的网络结构来适应不同尺寸的网络。特别的，我们通过上图所示的trident blocks，创造了多个尺度特定的feature maps。得益于空洞卷积，trident blocks的不同分支有着同样的网络结构，并且共享同样的权重参数，但是又不同的感受野。另外，为了避免训练具有极端尺寸的objects，我们采用尺度感知的训练模式，使每个分支给定的尺度范围与其感受野相匹配。最终，得益于整个多分支网络之间的权值共享，我们可以在inference的过程中通过一个主要的分支来估计整个TridentNet。这种估计只会带来边际性能的下降(不太懂)。最终，它能够在单尺度的baseline上达到显著地提升，而不需要对inference的速度做出任何妥协。</p><p>总结我们的贡献如下：</p><ul><li>我们提出了我们关于在不同尺度的objects上的感受野的影响。我们是是第一个通过设计控制实验来探索目标检测任务的感受野。</li><li>我们提出了一个全新的Trident Network来处理目标检测上的尺度变换问题。通过多分支的结构以及尺度感知的训练，TridentNet能够通过统一的表征能力生成尺度特定的feature maps。</li><li>得益于我们的权重共享的trident-bloc的设计，我们提出了一种通过主要分支来进行快速的估计的方法。因此可以在inference期间不引入额外的参数以及计算成本。</li><li>我们通过ablation studies在标准的COCO benchmark上验证了我们的方法的高效性。与最先进的方法相比，我们提出的方法获得了显著地性能。</li></ul><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><p><strong>Deep Object Detectors</strong></p></li><li><p><strong>Methods for handling scale variation</strong></p></li><li><p><strong>Dilated convolution</strong></p></li></ul><h1 id="Investigation-of-Receptive-Filed"><a href="#Investigation-of-Receptive-Filed" class="headerlink" title="Investigation of Receptive Filed"></a>Investigation of Receptive Filed</h1><p>可能影响目标检测器的性能的一些关于backbone网络的因素有：下采样率，网络深度以及感受野。一些工作已经讨论了它们的影响。前两个因素的影响是很直观的：网络越深（或叫表示能力更强）结果会越好，下采样次数过多对于小物体有负面影响。据我们所知，先前没有工作单独研究感受野对检测器性能的影响。</p><p>为了研究感受野对检测不同尺度的物体的影响，我们将backbone网络中的一些卷积操作替换为dilated variant。我们使用不同的dilation rates来控制网络的感受野。</p><p>我们在COCO benchmark上使用Faster R-CNN检测器来进行这个试验性实验。我们使用ResNet-50以及ResNet-101作为backbone网络，并且将conv4层的残差模块的3*3卷积操作的dilation rate$d_s$在1和3之间变化。</p><p>下表总结了在不同dilation rates下的检测结果。我们可以发现随着感受野的扩大，检测器对检测小物体的性能持续下降。但是对于大物体，则可以从不断增加的感受中受益。以上的发现可以总结如下：</p><ul><li>对不同尺寸物体的检测的性能受网络感受野的影响。最合适的感受野与objects的尺寸紧密相关。</li><li>尽管ResNet-101有理论上足够大的感受野来覆盖COCO上有着大尺寸的objects，但检测大物体的性能仍然可以通过增加dilation rate来提升。在小物体上也同样如此。我们猜测检测网络高效的感受野需要平衡物体的大小尺寸。</li></ul><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/LJ3wulKNB6bu.png?imageslim" alt="mark"></p><h1 id="Trudent-Network"><a href="#Trudent-Network" class="headerlink" title="Trudent Network"></a>Trudent Network</h1><p>我们提出的TridentNet主要包括权重共享的trident blocks以及尺度感知训练模式。</p><h2 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/0CXwmlgYcB8n.png?imageslim" alt="mark"></p><p>我们的方法以单一尺度的图片作为输入，然后产生尺度特定的feature maps通过平行的分支，即有着相同的参数但是dilation rates不同的卷积。</p><ul><li><p><strong>Multi-branch Block</strong><br>我们通过将原backbone中的一些卷积块替换为trident blocks来构建TridenNets。每一个trident blocks都由多个平行的分支组成。<br>以ResNet为例。对于一个bottleneck样式的残差模块，由三个卷积核大小分别为1*1,3*3,1*1的卷积组成。其对应的trident block由多个平行的有着不同dilation rates的3*3卷积的残差模块构成。堆叠的trident blocks允许我们以一种高效的方式调节不同分支的感受野。通常我们将trident block应用在最后一层的blocks，因为较大的stride会导致需要的感受野有较大的差异。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/6oGwV4st96Kb.jpg?imageslim" alt="mark"></p></li><li><p><strong>Wight sharing among branches</strong><br>TridentNet不同分支之间的权重参数都是共享的，只是dilation rates有所不同。这种设置使得权重的共享更加简单直接。<br>权重共享有三大优势：</p><ol><li>减少了参数的数量,与原始目标检测器相比，并没有额外增加参数数量。</li><li>与我们的motivation相呼应，即不同尺度的物体应该在相同表征力下进行统一的转化。</li><li>转换参数可以针对来自所有分支的更多对象样本进行训练，换句话说就是在不同感受野下对不同尺寸范围训练相同的参数。</li></ol></li></ul><h2 id="Scale-aware-Training-Scheme"><a href="#Scale-aware-Training-Scheme" class="headerlink" title="Scale-aware Training Scheme"></a>Scale-aware Training Scheme</h2><p>我们提出的trident结构根据预定义的ddilation rates生成尺度特定的feature maps。然而在么一个单一分支中仍然存在尺寸的错误匹配。因此我们提出了一种尺度感知训练模式来提升每一个分支的尺度感知能力，避免在不匹配的分支上出现极端尺度的训练对象。</p><p>类似于SNIP，我们为每一个分支$i$定义一个有效区域$[l_i,u_i]$。在训练过程中，我们只选择那些尺寸落在每个分支对应的有效分区的proposals以及真实框。特别的，对于一个长为$h$宽为$w$的Region-of-Interest (ROI)，若它在分支$i$有效，则：</p><script type="math/tex; mode=display">l_i<=\sqrt{wh}<=u_i</script><p>这个尺度感知的训练模式会被应用到RPN以及R-CNN。在我们的尺度感知训练中，在RPN分配anchor标签时，我们选择根据上述公式在该分支有效的ground truth boxes。相似的，我们在R-CNN的训练过程中，在每个分支上采样有效的proposals。</p><h2 id="Inference-and-Approximation"><a href="#Inference-and-Approximation" class="headerlink" title="Inference and Approximation"></a>Inference and Approximation</h2><p>在inference期间，我们在所有分支上生成生成检测结果，然后滤除落在每个分支有效范围外的boxes。我们然后采用NMS或者soft-NMS结合多个分支的的输出来得到最终的结果。</p><h2 id="Fast-Inference-Approximation"><a href="#Fast-Inference-Approximation" class="headerlink" title="Fast Inference Approximation"></a>Fast Inference Approximation</h2><p>为了加速我们的网络的inference速度，在inference过程中，我们可以只使用一个主要的分支来作为对TridentNet估计。特别的，我们将其有效范围设置为$[0,\infty]$来预测所有尺度的objects。对于如上图所示的三分支网络，我们使用中间分支作为我们的主要分支，因为它的有效范围涵盖了大物体以及小物体。在这种方式下，我们的fast TridentNet相比于传统的Faster R-CNN检测器没有额外的时间损耗。令人吃惊的是，我们发现这种估计相比于原来的TridentNet只有轻微的性能下降。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>我们在MXNet上重新实现Faster R-CNN，来作为我们的baseline method。我们将ResNet中的conv4层的输出作为backbone feature map，并且con5阶段作为rcnn head。对于每一张图，我们在每个分支上采样128个ROIs。我们使用三个分支作为我们的默认结构。在三个分支上的dilated rates分别设置为1,2,3。三个分支上的有效范围分别设置为$[0,90]$，$[30,160]$，$[90,\infty]$。</p><h2 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h2><p><strong>Components of TridentNet</strong></p><ol><li>Multi-branch</li><li>Scale-aware</li><li>Weight-sharing</li></ol><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/co4BPG6jmRBG.png?imageslim" alt="mark"></p><p><strong>Number of branches</strong></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/rvodK34irTFf.png?imageslim" alt="mark"></p><p><strong>Stage of Trident blocks</strong></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/fSVqI9XyYkpa.png?imageslim" alt="mark"></p><p><strong>Number of trident blocks</strong></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/ykgrxWGQhqqO.png?imageslim" alt="mark"></p><p><strong>Perfomance of each branch</strong></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/sqBxC3NTgF0v.png?imageslim" alt="mark"></p><h2 id="Comparision-with-State-of-the-Arts"><a href="#Comparision-with-State-of-the-Arts" class="headerlink" title="Comparision with State-of-the-Arts"></a>Comparision with State-of-the-Arts</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/A8tSqPXnVjWT.png?imageslim" alt="mark"></p><p><strong>Fast approximation</strong></p><ul><li>the three-branch methods 42.7/48.4 AP</li><li>the major branch 42.2/47.6 AP</li></ul><p><strong>Compare with FPN</strong></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/Y5lItdKskb6u.png?imageslim" alt="mark"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>在本文中，我们提出了一个简单的目标检测方法TridentNet来通过相同的表征能力在网络中构建尺寸特定的的feature maps。对于我们的多分支结构，我们提出了尺度感知的训练模式为每个分支分配对应的尺度。使用主要分支的fast inference方法使得TridentNet在baseline上得到显著的提升，而不需要额外的参数和计算。我们相信TridentNet有益于当前的目标检测以及其他视觉任务。我们会在未来的工作中探索这个方向。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 注意力机制 </tag>
            
            <tag> 空洞卷积 </tag>
            
            <tag> 目标检测 </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记8：Non-local Neural Networks</title>
      <link href="/2019/08/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B08/"/>
      <url>/2019/08/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B08/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/abs/1711.07971v3" target="_blank" rel="noopener">https://arxiv.org/abs/1711.07971v3</a><br>代码地址：<a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">https://github.com/facebookresearch/video-nonlocal-net</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>不论是卷积网络还是递归网络，它们都是作用在某一块局部区域(local neighborhood)的operations。在本文中，我们提出了non-local operations作为一种通用的神经网络的building blocks来捕捉基于long-range的依赖关系。受到经典的non-local means方法的启发，本文的non-local operation会将某一位置的响应当做是一种从特征图谱所有位置的加权和来计算。该building block可以插入到现在计算机视觉的许多模型当中，进而可以提升分类，检测，分割等视觉任务的性能表现。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>捕获long-range dependencies在深度神经网络中具有重要意义。对于图片数据，long-distance dependencies以深度堆叠的卷积网络形成的更大的感受野为模型。</p><p>不管是卷积操作还是recurrent操作都是在空间或者时间的local neighborhood上进行的；因此long-rangedependencies只能通过重复执行这些操作，通过数据逐步传递信号来得到。重复的local操作有一些限制：</p><ul><li>它的计算很低效</li><li>它导致了需要认真对待的优化困难</li><li>这些challenges make multi-hop dependency modeling，比如较远的两个位置之间的信息前向和反向传播会比较困难(这段没看懂)</li></ul><p>本文中，我们提出了non-local操作，来作为一个在深度神经网络中的高效，简单以及通用的组件老捕捉long-range dependencies。我们提出的non-local操作源自计算机视觉中经典的non-local mean操作。我们的non-local操作计算一个位置的响应作为输入的feature map上的所有位置的加权和来计算。这些位置可以在空间，时间或者时空中，所以我们的操作适用于图片，序列以及视频问题。</p><p>使用non-local的一些优点：</p><ul><li>相比于 CNN 和 RNN 的逐步计算的劣势, non-local 操作 可以直接从任意两点中获取到 long-range dependencies.</li><li>根据实验结果可知, non-local operations 是十分高效的, 并且即使在只有几层网络层时, 也能取得很好的效果.</li><li>最后, 本文的 nocal operaions 会维持输入变量的尺寸大小, 并且可以很容易的与现有的其他 operations 结合使用.</li></ul><p>我们用 video classification 任务来展示 non-local 的有效性. 在视频数据中, long-range interactions 不仅出现在 空间位置上的 distant pixels, 还会出现在时间维度上的 distant pixels. 通过一个单一的 non-local block (basic unit), 以前馈的方式，便可以捕获到这些 spacetime dependencies, 如果将多个 non-local block 组合起来形成 non-local neural networks, 便可以提高 video classification 任务的准确度(不加任何tricks). 另外, non-local 网络要比 3D 卷积网络的计算性价比更高. 为了说明 non-local 的一般性, 我们还在 COCO 数据集上进行了目标检测/分割, 姿态识别等任务的实验, 在基于 MaskRCNN 的网络基础上, 我们的 non-local blocks 可以用较少的计算开销进一步提升模型的精度.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><p><strong>Non-local image processing</strong></p></li><li><p><strong>Graphical models</strong></p></li><li><p><strong>Feedforward  modeling  for  sequences</strong></p></li><li><p><strong>Self-attention</strong></p></li><li><p><strong>Interaction networks</strong></p></li><li><p><strong>Video classification architectures</strong></p></li></ul><h1 id="Non-local-Neural-Network"><a href="#Non-local-Neural-Network" class="headerlink" title="Non-local Neural Network"></a>Non-local Neural Network</h1><p>下面首先给出 non-local operations 的一般性定义, 然后会给出几种特定的变体</p><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><p>遵循non-local mean操作，我们定义了一个在深度神经网络中通用的non-local操作：</p><script type="math/tex; mode=display">y_i=\frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)</script><p>上式中,$i$代表了output中计算响应的位置的index，而$j$枚举了所有可能的position。x是input signal(一般为特征图谱)，y是output signal(与x的size相同)。f会返回一个标量(表示i与j之间的relationship)，g也会返回一个标量，代表在j位置上的input signal。通过系数$C(x)$对响应进行归一化。</p><p>该公式的 non-local 特性主要体现在考虑了所有可能的 position (∀j), 而卷积网络只会考虑 output position 周围位置的像素点。</p><p>non-local操作不同于全连接层。non-local计算的响应是基于不同位置之间的relationship，而全连接层使用的是学习的权重。换句话说，在全连接层中，$x_i$和$x_j$之间的relationship并不是input data的函数。另外，我们的公式支持不同的输入尺寸，同时在输出中保留对应的尺寸。</p><p>non-local是一个非常灵活的模块, 它可以被添加到深度神经网络的浅层网络当中去(不像fc那样处于深层网络), 这使得我们可以构建更加丰富的模型结构来结合non-local和local信息。</p><h2 id="Instantiations"><a href="#Instantiations" class="headerlink" title="Instantiations"></a>Instantiations</h2><p>接下来我们介绍介个不同版本的$f$函数以及$g$函数。我们会在实验中证明我们的non-local模型对这些函数的选择并不敏感。 这意味着non-local的通用性正是提升各个模型在不同任务上性能表现的主要原因。</p><p>出于简洁，我们把$g$函数考虑成线性形式：$g(x_i)=W_g*x_j$，$W_j$是个需要被学习的权重矩阵，在实现时，通常会通过1×1(或 1×1×1)的卷积来实现。</p><p>接下来我们讨论$f$函数的选择：</p><ul><li><p><strong>Gaussian</strong><br>一个最自然的选择就是高斯函数：</p><script type="math/tex; mode=display">f(x_i,x_j)=e^{x_i^Tx_j}</script><p>这里$x_i^Tx_j$为两个向量的点积，会返回一个标量，有时候也可以使用欧几里得距离，不过点积的实现更加容易。归一化参数为$C(x)=\sum_{\forall j}f(x_i,x_j)$。</p></li><li><p><strong>Embedded Gaussian</strong><br>高数函数的一个简单拓展就是在embedding space中计算相似度：</p><script type="math/tex; mode=display">f(x_i,x_j)=e^{\theta(x_i)^T\phi(x_j)}</script><p>其中$\theta(x_i)=W_\theta x_i$，$\phi(x_j)=W_\phi x_j$为两个embeddings。和上述一样，我们设定$C(x)=\sum_{\forall j}f(x_i,x_j)$。<br>我们发现self-attention模块其实就是non-local的embedded Gaussian版本的一种特殊情况。对于给定的$i$，$\frac{f(x_i,x_j)}{C(x)}$就变成了计算所有的$j$的softmax，即$y=softmax(x^T W_\theta^T W_\phi x)g(x)$，这就是self-attention的表达形式。这样我们就将self-attention模型和传统的非局部均值联系在了一起，并且将sequential self-attention network推广到了更具一般性的space/spacetime non-local network，可以在图像、视频识别任务中使用。</p></li><li><p><strong>Dot produt</strong><br>$f$也可以定义为点乘相似度：</p><script type="math/tex; mode=display">f(x_i,x_j)=\theta(x_i)^T \phi(x_j)</script><p>这里我们采用embedded版本。在这种情况中，我们设置归一化参数$C(x)=N$，$N$为在$x$中的所有位置的个数，而不是$f$之和，因为这样能够简化梯度计算。这种形式的归一化是有必要的，因为输入的size是变化的，所以用x的size作为归一化参数有一定道理。<br>dot product和embeded gaussian的版本的主要区别在于是否做softmax，softmax在这里的作用相当于是一个激活函数。</p></li><li><p><strong>Concatenation</strong><br>Concatenation是在Relation Network中使用的pairwise function。于是我们也设计了一个concatenation版本的$f$：</p><script type="math/tex; mode=display">f(x_i,x_j)=ReLU(w_f^T[\theta(x_i),\phi(x_j)])</script><p>这里中括号中的代表concatenation，$w_f$是能够将concatenation向量转化为一个标量的权重向量。这里设置$C(x)=N$。</p></li></ul><p>以上我们定义了多种变种，这说明了我们的non-local操作的灵活性，我们相信也会有别的变种能够提升性能。</p><h2 id="Non-local-Block"><a href="#Non-local-Block" class="headerlink" title="Non-local Block"></a>Non-local Block</h2><p>我们将上面介绍的公式(non-local operation)包装进一个 non-local block 中, 使其可以整合到许多现有的网络结构当中, 我们将 non-local 定义成如下格式:</p><script type="math/tex; mode=display">z_i=W_z y_i+x_i</script><p>$y_i$由上述公式得到，“$+x_i$”代表一个残差连接。残差连接使得我们可以将一个新的non-local block插入一个预先训练好的模型，而不会破坏其原有的结构（如$W_z=0$作为初始化则完全和原始结构一致）。下图演示了一个non-local block。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/n4fEfPnQpfvj.png?imageslim" alt="mark"><br>non-local block的pairwise的计算可以是非常轻量级的，如果它用在高层级，较小的feature map上的话。比如，图2上的典型值是T=4，H=W=14 or 7。通过矩阵运算来计算parwise function的值就和计算一个conv layer的计算量类似。另外我们还通过以下方式使其更高效。</p><ul><li><strong>Implementation of Non-local Blocks</strong><br>我们设置 $W_g$，$W_\theta$，$W_\phi$的通道数为$x$通道数目的一半，这样就形成了一个bottleneck，就能够减少一半的计算量。$W_z$再重新放大到$x$的通道数目，以保证输入输出维度一致。<br>还有个下采样的trick可以进一步采用，就是将non-local的公式改为$y_i=\frac{1}{C(\hat{x})}\sum_{\forall j}f(x_i,\hat{x}_j)g(\hat{x}_j)$，其中$\hat{x}_j$是$x_j$下采样得到的，比如通过pooling。我们将这个方式在空间域上使用，可以减少1/4的pairwise function的计算量。这个trick髌骨会改变non-local的行为，而是使计算变得稀疏了。可以通过在上图中的$\phi$以及$g$后面增加一个max pooling层来实现。</li></ul><h1 id="Video-Classification-Models"><a href="#Video-Classification-Models" class="headerlink" title="Video Classification Models"></a>Video Classification Models</h1><p>没看</p><h1 id="Experiments-on-Video-Classification"><a href="#Experiments-on-Video-Classification" class="headerlink" title="Experiments on Video Classification"></a>Experiments on Video Classification</h1><p>没看</p><h1 id="Extension-Experiments-on-COCO"><a href="#Extension-Experiments-on-COCO" class="headerlink" title="Extension:Experiments on COCO"></a>Extension:Experiments on COCO</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/VEm37JhI3itw.png?imageslim" alt="mark"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种新的神经网络，能够通过non-local操作捕捉 long-range dependencies。我们的non-local blocks能够与任何已存在的结构结合。我们展示了non-local modeling在视频分类，目标检测，语义分割以及姿态估计中的重要性。在所有的任务中，简单的添加一个non-local blocks就可以在baseline上获得稳定的提升。我们希望non-local layers会成为未来网络结构中的一个重要组成部分。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> 注意力机制 </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记7：Libra R-CNN:Towards Balanced Learning for Object Detection</title>
      <link href="/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B07/"/>
      <url>/2019/08/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B07/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>相比模型的结构，关注度较少的训练过程对于检测器的成功检测也是十分重要的。本文发现，检测性能主要受限于训练过程中的三个levels上的不平衡——sample level,feature level,objective level的不平衡问题。为此，提出了Libra R-CNN，用于对平衡目标检测的学习的简单有效的框架。主要包含三个创新点：</p><ul><li>IoU-balanced sampling 用于减少采样过程中的不平衡</li><li>balanced feature pyramid 用于减少feature 的不平衡</li><li>balanced L1 loss 用于减少objective level的不平衡。</li></ul><p>它在MSCOCO上的Average Precision(AP)相比FPN Faster R-CNN以及RetinaNet分别高出了2.5和2个点。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>目标检测的训练过程成功与否主要依靠以下三个关键方面：</p><ul><li>选择的区域样本是否具有代表性</li><li>是否充分利用了提取的视觉特征</li><li><p>设计的目标函数是否是最优的<br>然而，本文发现在典型的训练过程中上述三个方面中存在这严重的不平衡问题。这些不平衡问题会使得网络的结构无法得到完全开发进而影响整体的目标检测性能。如下图所示：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/q3vL1XM9MFRh.png?imageslim" alt="mark"></p></li><li><p><strong>Sample level imbalance</strong><br>当在训练一个检测器时，困难样本是最具有价值的，因为它们更有利于提升目标检测的性能。然而，基于随机采样机制造成的结果一般是使挑选的样本趋向于easy类型,harde mining算法如OHEM可以更多的关注hard samples。然而，这些方法对噪声标签比较敏感，容易造成大量的内存及计算量的消耗。Focal Loss应用于单阶段的效果较好，但是，扩展到大部分样本为简单负样本的双阶段检测模型中，效果一般。</p></li><li><p><strong>Feature level imbalance</strong><br>在backbone中深层具有更多的语义信息，而较低层为更多的内容描述等细节信息。FPN及PANet等通过侧连接来进行特征融合，因此，低层特征与高层特征可以对目标检测进行补充。用来来整合金字塔的特征表示的方法决定着目标检测的性能。本文认为，融合的信息应该平衡地包含每个分辨率的信息。但上述方法使得融合的特征更多关注于相邻的分辨率而不是其他分辨率。在信息传递过程中，每次融合操作，会使得非相邻层级的语义信息变得稀释。</p></li><li><p><strong>Objective level imbalance</strong><br>检测器包含分类及定位两个任务。因此，在训练目标函数中结合两个不同的目标。如果二者不是平衡的，一个目标可能会被限制，进而导致整体不好的效果。此情形同样适用于训练过程中的样本，如果不平衡，由简单样本产生的小梯度值会淹没hard样本产生的较大的梯度值，进而限制了后续的一些强化操作。因此，针对最优收敛，本文平衡了相关的任务及样本。</p></li></ul><p>为解决上述问题，提出了简单有效的Libra R-CNN框架。三点创新如下：</p><ol><li>IoU-balanced sampling:根据assigned ground-truth进行mine hard samples</li><li>balanced feature pyramid:利用相同深度融合的平衡后的语义特征增强多层次的特征。</li><li>balanced L1 loss：增强重要的梯度，进而对分类，粗定位，细定位进行再平衡。</li></ol><p>我们总结了我们的主要贡献：</p><ol><li>我们系统的重新审视了检测器的训练过程。我们的研究揭露了限制检测器性能的三个levels上的不平衡问题。</li><li>我们提出了Libra R-CNN，一个通过结合三个组件来重新平衡检测器的框架。</li><li>我们在MS COCO上测试了我们提出的框架，获得了在当前最先进的检测器(包库单阶段以及二阶段)上的显著提升。</li></ol><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><strong>Model architectures for object detection</strong></li><li><strong>Balanced learning for object detection</strong><ul><li>Sample level imbalance</li><li>Feature level imbalance</li><li>Objective level imbalance</li></ul></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>Libra R-CNN的全貌如下：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/hlFBxlrr83YA.png?imageslim" alt="mark"><br>我们的目标是使用一个整体平衡的设计来减轻在检测器训练过程中存在的不平衡问题，从而尽可能利用模型结构的潜在能力。接下来会详细介绍所有组件：</p><h2 id="IoU-balanced-Sampling"><a href="#IoU-balanced-Sampling" class="headerlink" title="IoU-balanced Sampling"></a>IoU-balanced Sampling</h2><p>首先提出了一个问题：训练样本及对应ground truth的重叠度IoU是否与样本的difficulty(这里类比于easy sample,hard sample)相关。我们进行了实验，结果如下：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/vRBnS7n9jp4x.png?imageslim" alt="mark"><br>从图中可以看到，随机采样超过70%的部分都在 IoU 0到0.05之间。而60%的hard negative样本都在IoU大于0.05的地方，但随机采样只提供了30%的IoU大于0.05的训练样本。由于这种极度的不平衡，许多的hard样本会被数以千计的easy样本埋没掉。<br>于是我们提出了IoU-balanced sampling：一个简单但高效不需要额外消耗的hard mining方法。假设我们需要从M个对应的候选样本中采样N个negative样本。在随机采样下，每一个样本被采样的概率为：</p><script type="math/tex; mode=display">p=\frac{N}{M}</script><p>为了提升选择到hard样本的可能性，我们根据IoU的值将采样区间划分为K个格子。要求negative样本均匀分配到每个格子，然后对其进行均匀采样。得到的采样概率为：</p><script type="math/tex; mode=display">p_k=\frac{N}{K}*\frac{1}{M_k},k\in[0,K)</script><p>$M_k$为对应区间上的候选采样数。在我们的实验中K默认取值为3。<br>通过上图我们可以发现我们的IoU-balanced sampling能够使训练样本的分布近似于hard negative样本的分布。<br>另外，值得注意的是这个方法也适用于hard positive样本。然而在大多数情况下，没有足够的候选样本，对正样本进行扩充，因此，本文选择了一个替代的方法来达到平衡采样的目的，即对每个ground truth采样相同数量的正样本。</p><h2 id="Balanced-Feature-Pyramid"><a href="#Balanced-Feature-Pyramid" class="headerlink" title="Balanced Feature Pyramid"></a>Balanced Feature Pyramid</h2><p>本文不同于以前通过侧连接的方法来结合多个层级特征的方法，本文通过使用相同深度的融合平衡后的语义信息来增强不同层级的特征，如下图所示，主要包含四步：rescaling,integrating,refining,strengthening<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/pHl8soHrBW8h.png?imageslim" alt="mark"></p><ul><li><p><strong>Obtaining balanced semantic features</strong><br>为了融合多层次的特征，同时保留它们的语义层次。我们首先的多层次的特征$\lbrace C_2,C_3,C_4,C_5\rbrace$的尺寸通过插值或者最大池化改变为中等大小。然后，取其均值得到平衡后的语义特征。</p><script type="math/tex; mode=display">C=\frac{1}{L}\sum_{l=l_{min}}^{l_{max}}C_l</script><p>得到的特征，进行rescale然后通过反向操作增强原始每层的特征。在这个过程中每个分辨率从其他分辨率那里获得等同的信息。此过程不包含任何参数，证明了信息传递的高效性。</p></li><li><p><strong>Refining balanced semantic features</strong><br>平衡后的语义特征可以进一步增强增加其分辨性。本文发现卷积和non-local 模型的增强效果都很好，但是non-local模型更加稳健。因此，我们默认使用embeded Gaussian non-local attention。refining这一步可以进一步的丰富特征信息，进而提升结果。使用这种方法来自low-level以及high-level的特征能够同时被融合。融合后得到的特征{P2,P3,P4,P5}用于后续的目标检测中，流程和FPN相同。</p></li></ul><h2 id="Balanced-L1-Losss"><a href="#Balanced-L1-Losss" class="headerlink" title="Balanced L1 Losss"></a>Balanced L1 Losss</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/fmtNaY8jrC0S.png?imageslim" alt="mark"></p><p>从Fast R-CNN开始，分类以及定位的问题在multi-task损失的指导下同时解决。</p><script type="math/tex; mode=display">L_{p,u,t^u,v}=L_{cls}(p,u)+\lambda[u\geq 1]L_{loc}(t^u,v)</script><p>$L_{cls}$和$L_{loc}$分别对应着分类和定位的损失函数，$p,u$分别是$L_{cls}$的预测和目标，$t^u$是对应$u$类的回归结果。$v$是回归目标。$\lambda$用于在多任务学习下调整损失权重。我们把loss大于等于1.0的样本称为outliers，其他的样本称为inliers。</p><p>由于这个损失函数是两个loss的相加，如果分类做得很好地话一样会得到很高的分数，而导致忽略了回归的重要性。平衡上述损失的一个常规方法是，调整两个任务损失的权重，然而，对于没有边界的回归目标，直接增加定位损失的权重将会是模型对outliers的点更加敏感，outliers可以被看作是hard samples，这些hard samples可能会产生过大的损失，不利于训练。inliers可以看作是easy samples，相比outliners对整体的梯度贡献度较低，相比hard sample，平均每个，easy sample对梯度的贡献为hard sample的30%，即较少的outliers贡献了较多的梯度。基于上述分析，提出了balanced L1 Loss记作$L_b$。</p><p>Balanced L1 loss源自conventional smooth L1 loss。在conventional smooth L1 loss中有一个拐点来分离inliners和outliners,并且通过设置一个为1.0的最大值来截断outliners产生的大的梯度。Balanced L1 loss的关键思想是，促进影响较大的回归梯度，（像来自inliers即准确样本的梯度）。进而平衡包含的样本及任务。从而可以在分类，粗定位，及细定位中进行平衡的训练。，基于balanced L1 loss的定位损失如下：</p><script type="math/tex; mode=display">L_{loc}=\sum_{i\in\lbrace x,y,w,h\rbrace}L_b(t_i^u-v_i)</script><p>其对应的梯度公式遵循：</p><script type="math/tex; mode=display">\frac{\partial L_{loc}}{\partial w}\varpropto\frac{\partial L_b}{\partial t_i^u}\varpropto\frac{\partial L_b}{\partial x}</script><p>基于上式，我们设计了一个梯度公式：</p><script type="math/tex; mode=display">\frac{L_b}{x} = \left \{  \begin{array}{lr}    \alpha ln(b|x|+1) & if|x|<1 \\    \gamma            & otherwise  \end{array}\right.</script><p>三个参数的作用：</p><ul><li>$\alpha$ 使得inliers获得更多的梯度(因为原先的easy样本贡献的梯度少)</li><li>$\gamma$ 通过整体放大来陶正回归误差的上限，让更多的outliers参与回归，使得目标函数更好地平衡参与的任务</li><li>$b$ 用来平很参数$\alpha$和$\gamma$，使之满足关系：$\alpha ln(b+1)=\gamma$，使得梯度在误差等于1处连续</li></ul><p>最后得到balanced L1 loss如下:</p><script type="math/tex; mode=display">L_b(x) = \left\{  \begin{array}{lr}    \frac{\alpha}{b}(b|x|+1)ln(b|x|+1)-\alpha|x| & if |x|<1 \\    \gamma|x|+C & otherwise  \end{array}\right.</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/gWwND5mex8zj.png?imageslim" alt="mark"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> R-CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滑动平均(exponential moving average)</title>
      <link href="/2019/08/02/exponential-moving-average/"/>
      <url>/2019/08/02/exponential-moving-average/</url>
      
        <content type="html"><![CDATA[<h1 id="滑动平均"><a href="#滑动平均" class="headerlink" title="滑动平均"></a>滑动平均</h1><h2 id="什么是滑动平均"><a href="#什么是滑动平均" class="headerlink" title="什么是滑动平均"></a>什么是滑动平均</h2><p>滑动平均法是用一组最近的实际数据值来预测未来一期或几期内公司产品的需求量、公司产能等的一种常用方法。滑动平均法适用于即期预测。当产品需求既不快速增长也不快速下降，且不存在季节性因素时，滑动平均法能有效地消除预测中的随机波动，是非常有用的。滑动平均法根据预测时使用的各元素的权重不同</p><p>滑动平均法是一种简单平滑预测技术，它的基本思想是：根据时间序列资料、逐项推移，依次计算包含一定项数的序时平均值，以反映长期趋势的方法。因此，当时间序列的数值由于受周期变动和随机波动的影响，起伏较大，不易显示出事件的发展趋势时，使用滑动平均法可以消除这些因素的影响，显示出事件的发展方向与趋势（即趋势线），然后依趋势线分析预测序列的长期趋势。</p><a id="more"></a><h2 id="用滑动平均估计局部均值"><a href="#用滑动平均估计局部均值" class="headerlink" title="用滑动平均估计局部均值"></a>用滑动平均估计局部均值</h2><p>变量$v$在$t$时刻记为$v_t$，$\theta_t$为变量$v$在$t$时刻的取值，即在不使用滑动平均模型时$v_t=\theta_t$，在使用滑动平均模型后，$v_t$的更新公式如下：</p><script type="math/tex; mode=display">v_t=\beta\cdot v_{t-1}+(1-\beta)\cdot\theta_t</script><p>上式中，$\beta\in[0,1)$。$\beta=0$相当于没有使用滑动平均。<br>Andrew Ng在Course 2 Improving Deep Neural Networks中讲到$t$时刻变量$v$的滑动平均值大致等于过去$\frac{1}{1-\beta}$个时刻$θ$值的平均。这个结论在滑动平均起始时相差比较大，所以有了Bias correction，将$vt$除以$(1−β^t)$修正对均值的估计。<br>加入了Bias correction后，更新公式如下：</p><script type="math/tex; mode=display">v_t=\beta\cdot v_{t-1}+(1-\beta)\cdot\theta_t</script><script type="math/tex; mode=display">v\_{biased}_t=\frac{v_t}{1-\beta^t}</script><p>$t$越大，$1-\beta^t$越接近1，则两者将会越来越接近。<br>当$\beta$越大时，滑动平均得到的值越和$\theta$的历史值相关。若$\beta=0.9$，则大致等于过去10个$\theta$值的平均；若$\beta=0.99$，则大致等于过去100个$\theta$值的平均。</p><h2 id="滑动平均的好处："><a href="#滑动平均的好处：" class="headerlink" title="滑动平均的好处："></a>滑动平均的好处：</h2><p>占内存少，不需要保存过去10个或者100个历史 θ 值，就能够估计其均值。（当然，滑动平均不如将历史值全保存下来计算均值准确，但后者占用更多内存和计算成本更高），同时使用滑动平均法进行预测能平滑掉需求的突然波动对预测结果的影响。</p><h1 id="tf-train-ExponentialMovingAverage"><a href="#tf-train-ExponentialMovingAverage" class="headerlink" title="tf.train.ExponentialMovingAverage"></a>tf.train.ExponentialMovingAverage</h1><h2 id="函数定义"><a href="#函数定义" class="headerlink" title="函数定义"></a>函数定义</h2><p>tensorflow中提供了tf.train.ExponentialMovingAverage来实现滑动平均模型，他使用指数衰减来计算变量的移动平均值。</p><pre><code>tf.train.ExponentialMovingAverage.__init__(self, decay, num_updates=None, zero_debias=False, name=&quot;ExponentialMovingAverage&quot;)</code></pre><p>函数介绍：</p><ul><li>decay 是衰减率在创建ExponentialMovingAverage对象时，需指定衰减率（decay），用于控制模型的更新速度。影子变量的初始值与训练变量的初始值相同。当运行变量更新时，每个影子变量都会更新为：<script type="math/tex; mode=display">shadow\_variable=decay*shadow\_variable+(1-decay)*variable</script>$shadow_variable$就是上式中的$v_t$，$variable$就是上式中的$\theta_t$，$decay$就是上式中的$\beta$。decay 决定了影子变量的更新速度，decay 越大影子变量越趋于稳定。在实际运用中，decay一般会设成非常接近 1 的数（比如0.999或0.9999）。</li><li>num_updates 是ExponentialMovingAverage提供用来动态设置decay的参数，当初始化时提供了参数，即不为none时，每次的衰减率是：<script type="math/tex; mode=display">min\lbrace decay,\frac{1+num\_updates}{10+num\_updates}\rbrace</script>这一点其实和Bias correction很像</li><li>apply() 方法添加了训练变量的影子副本，并保持了其影子副本中训练变量的移动平均值操作。在每次训练之后调用此操作，更新移动平均值。</li><li>average()和average_name() 方法可以获取影子变量及其名称。</li></ul><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># 定义一个变量，初始值为0</span><br><span class="line">v1 = tf.Variable(0, dtype=tf.float32)</span><br><span class="line"># step为迭代轮数变量，控制衰减率</span><br><span class="line">step = tf.Variable(0, trainable=False)</span><br><span class="line"># 初始设定衰减率为0.99</span><br><span class="line">ema = tf.train.ExponentialMovingAverage(0.99, step)</span><br><span class="line"># 更新列表中的变量</span><br><span class="line">maintain_averages_op = ema.apply([v1])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 初始化所有变量</span><br><span class="line">    init_op = tf.initialize_all_variables()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    # 输出初始化后变量v1的值和v1的滑动平均值</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    # 更新v1的值</span><br><span class="line">    sess.run(tf.assign(v1, 5))</span><br><span class="line">    # 更新v1的滑动平均值</span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    # 更新迭代轮转数step</span><br><span class="line">    sess.run(tf.assign(step, 10000))</span><br><span class="line">    sess.run(tf.assign(v1, 10))</span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    # 再次更新滑动平均值，</span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br><span class="line">    # 更新v1的值为15</span><br><span class="line">    sess.run(tf.assign(v1, 15))</span><br><span class="line">    sess.run(maintain_averages_op)</span><br><span class="line">    print(sess.run([v1, ema.average(v1)]))</span><br></pre></td></tr></table></figure><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[0.0, 0.0]</span><br><span class="line">[5.0, 4.5]</span><br><span class="line">[10.0, 4.555]</span><br><span class="line">[10.0, 4.60945]</span><br><span class="line">[15.0, 4.713355]</span><br></pre></td></tr></table></figure></p><p>分析：</p><script type="math/tex; mode=display">decay=min(0.99,\frac{1+0.0}{10+0.0})=0.1,shadow\_variable=0.1*0.0+(1-0.1)*0.0=0.0</script><script type="math/tex; mode=display">decay=min(0.99,\frac{1+0.0}{10+0.0})=0.1,shadow\_variable=0.1*0.0+(1-0.1)*5.0=4.5</script><script type="math/tex; mode=display">decay=min(0.99,\frac{1+1000}{10+1000})=0.99,shadow\_variable=0.99*4.5+(1-0.99)*10=4.555</script><script type="math/tex; mode=display">decay=min(0.99,\frac{1+1000}{10+1000})=0.99,shadow\_variable=0.99*4.555+(1-0.99)*10=4.60945</script><script type="math/tex; mode=display">decay=min(0.99,\frac{1+1000}{10+1000})=0.99,shadow\_variable=0.99*4.60945+(1-0.99)*15=4.713355</script><blockquote><p>参考：<br><a href="https://www.cnblogs.com/wuliytTaotao/p/9479958.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuliytTaotao/p/9479958.html</a><br><a href="https://www.cnblogs.com/cloud-ken/p/7521609.html" target="_blank" rel="noopener">https://www.cnblogs.com/cloud-ken/p/7521609.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记6：Gradient Harmonized Single-stage Detector</title>
      <link href="/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B06/"/>
      <url>/2019/08/01/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B06/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1811.05181.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.05181.pdf</a><br>代码地址：<a href="https://github.com/libuyu/GHM_Detection" target="_blank" rel="noopener">https://github.com/libuyu/GHM_Detection</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>尽管单阶段的检测器速度较快，但在训练时存在以下几点不足，正负样本之间的巨大差距，同样，easy,hard样本的巨大差距。本文从梯度角度出发，指出了上面两个不足带来的影响。然后，作者进一步提出了梯度协调机制(GHM)用于避开上面的不足。GHM的思想可以嵌入到用于分类的交叉熵损失或者用于回归的Smooth-L1损失中，最后，本文修改过的损失函数GHM-C与GHM-R用于平衡anchor分类及回归二者的梯度。提出的`模型在COCO测试集上的mAP达到了41.6，与$Focal Loss(FL)+SL_{1}$相比，超过了0.8。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>但阶段检测器在训练时面临的最具挑战性的问题就是easy和hard样本以及positive和negative样本之间的严重不平衡。巨大数量的easy以及背景样本影响了检测器的训练。归功于proposal-driven机制，这个问题在二阶段检测器中并不存在。后来，出现了基于OHEM的样本挖掘技术，但这种方法舍弃了大部分样本，训练也不是很高效，后来Focal Loss通过修改损失函数来调整不平衡问题。但是Focal loss引入了两个超参数，需要进行大量的实验进行调试，同时，Focal loss是一种静态损失，对数据集的分布不敏感，而在训练过程中，数据集的分布是会发生变化的。<br>本文指出类别不平衡主要归结于难度的不平衡，而难度的不平衡可以归结为，gradient norm分布的不平衡。如果一个正样本很容易被分类，则该样本为easy example同时模型从中得到的信息量较少，比如，通过这个样本，只产生了一点梯度信息。模型应该关注这个被分错的样本无论它属于哪个类别。从整体上来看，负样本易于多为easy examples，而hard examples通常为为正样本。因此，两种不平衡可以归结为属性上的不平衡。<br>此外，上面两种类型的不平衡(hard/easy , positive/negative)可以由gradient norm的分布表示。带有gradient norm的样本密度，被称为梯度密度，如下图左侧所示，由于存在大量的简单负样本，gradient norm较小的样本的密度很大。虽然一个简单样本对整体的梯度贡献很小，但大量的简单样本作用后会占据训练的主导，使训练过程并不是很高效。另外我们发现gradient norm较大的样本(hard examples)的密度也大于中间样本的密度。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/G6cs0JroT6y5.png?imageslim" alt="mark"><br>得益于对gradient norm的分布的分析，我们提出了一个 gradient harmonizing mechanism (GHM)来高效的训练单阶段的目标检测模型，这个机制着重于不同样本的梯度分布的harmony。GHM先统计具有相似梯度密度的样本的数量，对每一个样本根据其密度添加一个harmonizing参数。使用GHM来训练，每种样本的分布会趋于平衡，训练过程会变得更高效和稳定。<br>在实验中，对梯度的改变可以通过对损失函数的重新设计来等效的实现。我们把GHM嵌入分类损失，命名为GHM-C损失，同样我们也在回归分支中使用了GHM，命名为GHM-R损失。<br>我们的最主要贡献：</p><ol><li>我们揭示了单阶段检测器在gradient norm分布方面存在显著不足的基本原理，并且提出了一种新的梯度平衡机制(GHM)来处理这个问题。</li><li>我们提出了GHM-C以及GHM-R，它们纠正了不同样本的梯度分布，并且对超参数具有鲁棒性。</li><li>通过使用GHM，我们可以轻松地训练单阶段检测器，无需任何数据采样策略，并且在COCO基准测试中取得了state-of-the-art的结果。</li></ol><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><strong>Object Detection</strong></li><li><strong>Object Function for Object Detector</strong></li></ul><h1 id="Gradient-Harmonizing-Mechanism"><a href="#Gradient-Harmonizing-Mechanism" class="headerlink" title="Gradient Harmonizing Mechanism"></a>Gradient Harmonizing Mechanism</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>对于一个候选框，它的真实标签为$p^{*}\in\lbrace0,1\rbrace$，预测的值为$p\in[0,1]$，采用二元交叉熵损失：</p><script type="math/tex; mode=display">L_{CE}(p,p^{*}) = \left\{  \begin{array}{lr}    -log(p) & if x < 0\\    -log(1-p) & if x \ge 0  \end{array}\right.</script><p>假设$x$为模型输出，使得$p=sigmoid(x)$，那么上述的交叉熵损失对$x$的导数为：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L_{CE}}{\partial x}& = \left\{  \begin{array}{lr}    p-1 & if x < 0\\    p & if x \ge 0  \end{array}\right. \\& = p-p^{*}\end{aligned}</script><p>那么梯度的模定义为：</p><script type="math/tex; mode=display">g = |p-p^{*}| = \left\{  \begin{array}{lr}    1-p & if p^{*}=1 \\    p   & if p^{*}=0  \end{array}\right.</script><p>$g$代表了这个样本的难易程度以及它对整个梯度的贡献。<br>下图是一个收敛模型的梯度模长的分布，可以看出简单样本的数量很大，使得它对梯度的整个贡献很大，另一个需要的地方是，在梯度模较大的地方仍然存在着一定数量的分布，说明模型很难正确处理这些样本，作者把这类样本归为离群样本，因为他们的梯度模与整体的梯度模的分布差异太大，并且模型很难处理，如果让模型强行去学习这些离群样本，反而会导致整体性能下降。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/r0Gj6X6iRyFj.png?imageslim" alt="mark"></p><h2 id="Gradient-Density"><a href="#Gradient-Density" class="headerlink" title="Gradient Density"></a>Gradient Density</h2><p>训练样本的梯度密度函数为：</p><script type="math/tex; mode=display">GD(g)=\frac{1}{l_{\epsilon}(g)}\sum_{k=1}^N\delta_{\epsilon}(g_k,g)</script><p>其中$g_k$为第$k$个样本的gradient norm。</p><script type="math/tex; mode=display">\delta_{\epsilon}(x,y)= \left\{  \begin{array}{lr}    1 & if y-\frac{\epsilon}{2} <= x < y+\frac{\epsilon}{2} \\    0 & otherwise  \end{array}\right.</script><script type="math/tex; mode=display">l_{\epsilon}(g)=min(g+\frac{\epsilon}{2},1)-max(g-\frac{\epsilon}{2},0)</script><p>g的gradient norm即为在以g为中心，长度为$\epsilon$的区域内的样本数，并且由该区域的有效长度进行归一化。<br>现在我们就可以定义梯度密度harmonizing参数：</p><script type="math/tex; mode=display">\beta_i=\frac{N}{GD(g_i)}</script><p>$N$为样本总数</p><h2 id="GHM-C-Loss"><a href="#GHM-C-Loss" class="headerlink" title="GHM-C Loss"></a>GHM-C Loss</h2><p>根据梯度密度harmonizing参数，就可以得到损失函数的梯度密度harmonized的形式：</p><script type="math/tex; mode=display">\begin{aligned}L_{GHM-C} & =\frac{1}{N}\sum_{i=1}^N\beta_iL_{CE}(p_i,{p_i}^{*}) \\& = \sum_{i=1}^N\frac{L_{CE}(p_i,p_i^*)}{GD(g_i)}\end{aligned}</script><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/a3YO5XCsPIS3.png?imageslim" alt="mark"><br>上图对比了不同损失函数下的gradient norm的分布。可以看出，之前提到的简单样本的权重得到了较大幅度地降低，离群样本也得到了一定程度的降权。使用经过改善之后的损失函数使得训练过程更加高效和鲁棒。</p><h2 id="Unit-Region-Approximation"><a href="#Unit-Region-Approximation" class="headerlink" title="Unit Region Approximation"></a>Unit Region Approximation</h2><h3 id="Complexity-Analysis"><a href="#Complexity-Analysis" class="headerlink" title="Complexity Analysis"></a>Complexity Analysis</h3><p>常规计算所有样本梯度值的算法复杂度为$O(N^2)$，即使使用并行计算，每个计算节点仍有N的计算量。比较好的算法首先会以$O(NlogN)$的复杂度通过梯度正则对样本进行排序，然后使用一个队列来扫描样本，以$O(N)$的方式得到密度。由于单阶段检测中的N为$10^5$或者$10^6$，其梯度计算量仍很大，基于排序的算法比并行计算提升的幅度有限。本文提出了近似的获得样本梯度密度的方法。</p><h3 id="Unit-Region"><a href="#Unit-Region" class="headerlink" title="Unit Region"></a>Unit Region</h3><p>我们把$g$的分布空间分为长度为$\epsilon$的独立unit区域，所以共有$M=\frac{1}{\epsilon}$个区域。让$r_j$表示第$j$个区域，那么$r_j=[(j-1)\epsilon,j\epsilon)$。让$R_j$表示在$r_j$区域里的样本个数。我们定义$ind(g)=t$，表示$g$所在的区域的序号，满足$(t-1)\epsilon&lt;=g&lt;t\epsilon$。<br>然后我们定义近似梯度密度函数：</p><script type="math/tex; mode=display">\hat{GD}(g)=\frac{R_{ind(g)}}{\epsilon}=R_{ind(g)}M</script><p>然后我们就得到了近似梯度密度harmonizing参数：</p><script type="math/tex; mode=display">\hat{\beta}_i=\frac{N}{\hat{GD}(g_i)}</script><p>最后我们得到了重新制定的损失函数：</p><script type="math/tex; mode=display">\begin{aligned}\hat{L}_{GHM-C}& =\frac{1}{N}\sum_{i=1}^N\hat{\beta}_iL_{CE}(p_i,{p_i}^{*}) \\& = \sum_{i=1}^N\frac{L_{CE}(p_i,p_i^*)}{\hat{GD}(g_i)}\end{aligned}</script><h3 id="EMA"><a href="#EMA" class="headerlink" title="EMA"></a>EMA</h3><p>基于Mini-batch统计的方法存在一个问题：当一个mini-batch中存在大量的异常点时，统计结果为噪声，而且使训练变得不稳定。Exponential moving average(EMA)是解决此问题的常用方法，比如带动量的SGD及BN处理。由于梯度密度的近似计算中的样本来自于单元区域，因此可以在每个单元区域应用EMA，进而得到更多稳定的梯度密度。</p><h2 id="GHM-R-Loss"><a href="#GHM-R-Loss" class="headerlink" title="GHM-R Loss"></a>GHM-R Loss</h2><p>Smooth L1损失函数为：</p><script type="math/tex; mode=display">L_{reg}=\sum_{i\in\lbrace x,y,w,h\rbrace}SL_1(t_i-t_i^*)</script><script type="math/tex; mode=display">SL_1(d) = \left\{  \begin{array}{lr}    \frac{d^2}{2\delta} & if |d|<=\delta \\    |d|-\frac{\delta}{2} & otherwise  \end{array}\right.</script><p>由于$d=t_i-t_i^*$,smoothL1 loss关于$t_1$的导数为：</p><script type="math/tex; mode=display">\frac{\partial SL_1}{\partial t_i} = \frac{\partial SL_1}{\partial d} = \left\{  \begin{array}{lr}    \frac{d}{\delta} & if |d|<=\delta \\    sgn(d) & otherwise  \end{array}\right.</script><p>对于所有$|d|&gt;\delta$的样本，都具有相同的gradient norm$|\frac{\partial SL_1}{\partial t_i}|=1$。这就不可能仅仅依靠gradient norm来区分不同属性的样本。一个选择就是直接使用$|d|$作为不同属性的度量。但新的问题是$|d|$理论上可以达到无穷大，单位区域的近似无法实现。<br>为了便捷的在回归loss上应用GHM，我们把传统的$SL_1$loss改变为更优雅的形式：</p><script type="math/tex; mode=display">ASL_1(d)=\sqrt{d^2+{\mu}^2}-\mu</script><p>当$d$很小时，近似为一个方差函数(L2 loss)，当$d$很大时近似为一个线性损失(L1 loss)，称该损失为Authentic Smooth L1损失，具有很好的平滑性，其偏导存在且连续。&amp;ASL_1$的偏导如下：</p><script type="math/tex; mode=display">\frac{\partial ASL1}{\partial d}=\frac{d}{\sqrt{d^2+{\mu}^2}}</script><p>我们定义$gr=|\frac{d}{\sqrt{d^2+{\mu}^2}}|$为$ASL_1$loss的gradient norm，其梯度分布如下图所示，从图中可以看出存在大量的异常点。而回归损失只作用在正样本中，因此，分类与回归的分布是不同的。最后，将GHM应用于回归loss:</p><script type="math/tex; mode=display">\begin{aligned}L_{GHM-R} & =\frac{1}{N}\sum_{i=1}^N\beta_iASL_1(d_i) \\& = \sum_{i=1}^N\frac{ASL_1(d_i)}{GD({gr}_i)}\end{aligned}</script><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/wWnNVGPxv4PO.png?imageslim" alt="mark"></p><p>我们强调，在边框回归中并不是所有的“简单样本”都是不重要的。在分类中一个简单样本通常是一个背景区域，有着很低的预测可能值，会被排除在最终候选框外。因此对这些样本的改进并不会对精度产生任何的影响。但是在边框回归中一个简单样本仍然偏离了真实框位置。对每个样本更好的预测会直接提升最终候选框的质量。而且，高级的数据集更关心定位的精确性<br>我们的GHM-R能够harmonize简单样本以及困难样本对边框回归的贡献，通过对简单样本的重要部分进行升权，以及对离群样本进行降权。实验显示它能比$SL_1$一级$ASL_1$获得更好的性能。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/dqeRz7wD70bm.png?imageslim" alt="mark"><br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/oui6JLhpLsVF.png?imageslim" alt="mark"><br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/9VcFcLujS5rv.png?imageslim" alt="mark"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 损失函数 </tag>
            
            <tag> 目标检测 </tag>
            
            <tag> 论文笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NexT主题阅读全文设置</title>
      <link href="/2019/08/01/hexo%E9%98%85%E8%AF%BB%E5%85%A8%E6%96%87/"/>
      <url>/2019/08/01/hexo%E9%98%85%E8%AF%BB%E5%85%A8%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>Hexo 的 Next 主题默认是首页显示你每篇文章的全文内容。想要在网站首页只显示每篇文章的部分内容，不要全部内容都展示出来。有两个解决方法：</p><ul><li>修改 _config.yml 文件设置</li><li>直接在你的 md 博文中加一句<code>&lt;!--more--&gt;</code></li></ul><a id="more"></a><h1 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h1><p>用文本编辑器打开 themes/next 目录下的 _config.yml 文件，找到这段代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Automatically Excerpt. Not recommend.</span><br><span class="line"># Please use &lt;!-- more --&gt; in the post to control excerpt accurately.</span><br><span class="line">auto_excerpt:</span><br><span class="line">  enable: false</span><br><span class="line">  length: 150</span><br></pre></td></tr></table></figure></p><p>把<code>enable</code>的<code>false</code>改成<code>true</code>就行了，然后<code>length</code>是设定文章预览的文本长度。修改后重启 hexo 就好了。<br>但这种方法的效果是会格式化你文章的样式，直接把文字挤在一起显示，最后会有<code>...</code>。</p><h1 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h1><p>若想保留了样式并且自行选择显示哪些内容来预览。就可以在写md博文时，在想要显示预览的部分后加上<code>&lt;I--more--&gt;</code>，这样这样首页和列表页展示的文章内容就是<code>&lt;!--more--&gt;</code>之前的文字，而之后的就不会显示了，同时也保留文章原样式。</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分组卷积——Group convolution</title>
      <link href="/2019/07/31/group-convolution/"/>
      <url>/2019/07/31/group-convolution/</url>
      
        <content type="html"><![CDATA[<p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/csEm643zcfff.png?imageslim" alt="mark"></p><a id="more"></a><h1 id="常规卷积"><a href="#常规卷积" class="headerlink" title="常规卷积"></a>常规卷积</h1><p>如果输入feature map尺寸为C∗H∗W，卷积核有N个，输出feature map与卷积核的数量相同也是N，每个卷积核的尺寸为C∗K∗K，N个卷积核的总参数量为N∗C∗K∗K，输入map与输出map的连接方式如上图左所示。</p><h1 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h1><p>Group Convolution分组卷积，最早见于AlexNet——2012年Imagenet的冠军方法，Group Convolution被用来切分网络，使其在2个GPU上并行运行，AlexNet网络结构如下<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/TjQDSWrNC4tw.png?imageslim" alt="mark"><br>Group Convolution顾名思义，则是对输入feature map进行分组，然后每组分别卷积。假设输入feature map的尺寸仍为C∗H∗W，输出feature map的数量为N个，如果设定要分成G个groups，则每组的输入feature map数量为$\frac{C}{G}$，每组的输出feature map数量为$\frac{N}{G}$，每个卷积核的尺寸为$\frac{C}{G}∗K∗K$，卷积核的总数仍为N个，每组的卷积核数量为$\frac{N}{G}$，卷积核只与其同组的输入map进行卷积，卷积核的总参数量为$N∗\frac{C}{G}∗K∗K$，可见，总参数量减少为原来的$\frac{1}{G}$，其连接方式如上图右所示，group1输出map数为2，有2个卷积核，每个卷积核的channel数为4，与group1的输入map的channel数相同，卷积核只与同组的输入map卷积，而不与其他组的输入map卷积。</p><h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><ul><li><strong>减少参数量</strong><br>分成G组，则该层的参数量减少为原来的$\frac{1}{G}$</li><li><strong>Structured Sparss</strong><br>每个卷积核的尺寸由$C∗K∗K$变为$\frac{C}{G}∗K∗K$，可以将其余$(C−\frac{C}{G})∗K∗K$的参数视为0，有时甚至可以在减少参数量的同时获得更好的效果（相当于正则）。</li><li><strong>Depthwise Convolution</strong><br>当分组数量等于输入map数量，输出map数量也等于输入map数量，即$G=N=C$、N个卷积核每个尺寸为$1∗K∗K$时，Group Convolution就成了Depthwise Convolution</li><li><strong>Global Depthwise Convolution(GDC)</strong><br>如果分组数$G=N=C$，同时卷积核的尺寸与输入map的尺寸相同，即$K=H=W$，则输出map为$C∗1∗1$即长度为C的向量，此时称之为Global Depthwise Convolution（GDC），可以看成是全局加权池化，与 Global Average Pooling（GAP） 的不同之处在于，GDC 给每个位置赋予了可学习的权重（对于已对齐的图像这很有效，比如人脸，中心位置和边界位置的权重自然应该不同），而GAP每个位置的权重相同，全局取个平均。</li></ul><blockquote><p>参考：<a href="https://www.cnblogs.com/shine-lee/p/10243114.html" target="_blank" rel="noopener">https://www.cnblogs.com/shine-lee/p/10243114.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Squeeze-and-Excitation Networks(SENet)</title>
      <link href="/2019/07/31/SENet/"/>
      <url>/2019/07/31/SENet/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文链接：<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">https://arxiv.org/abs/1709.01507</a><br>代码链接：<a href="https://github.com/hujie-frank/SENet" target="_blank" rel="noopener">https://github.com/hujie-frank/SENet</a> </p></blockquote><h1 id="SENet概述"><a href="#SENet概述" class="headerlink" title="SENet概述"></a>SENet概述</h1><p>卷积核作为CNN的核心，通常都是在局部感受野上将空间（spatial）信息和特征维度（channel-wise）的信息进行聚合最后获取全局信息。卷积神经网络由一系列卷积层、非线性层和下采样层构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述，然而去学到一个性能非常强劲的网络是相当困难的。<br>论文的动机是从特征通道之间的关系入手，希望显式地建模特征通道之间的相互依赖关系。另外，没有引入一个新的空间维度来进行特征通道间的融合，而是采用了一种全新的“特征重标定”策略。具体来说，就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去增强有用的特征并抑制对当前任务用处不大的特征。<br>通俗的来说SENet的核心思想在于通过网络根据loss去学习特征权重，使得有效的feature map权重大，无效或效果小的feature map权重小的方式训练模型达到更好的结果。SE block嵌在原有的一些分类网络中不可避免地增加了一些参数和计算量，但是在效果面前还是可以接受的 。Sequeeze-and-Excitation(SE) block并不是一个完整的网络结构，而是一个子结构，可以嵌到其他分类或检测模型中。</p><a id="more"></a><h1 id="SENet结构"><a href="#SENet结构" class="headerlink" title="SENet结构"></a>SENet结构</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/XWAA6Vvd7iu0.png?imageslim" alt="mark"></p><p>上图为SENet的block单元的示意图，图中的$F_{tr}$为传统的卷积结构，$X\in R^{C^<code>×H^</code>×W^`}$为输入，$U\in$ R^{C×H×W}为输出，这些都是以往结构中已存在的，SKNet增加的是部分是$U$后面的结构，通过三个操作来重新标定前面得到的特征：</p><ul><li><strong>Squeeze操作</strong><br>对$U$做一个Global Average Pooling,将$(H×W×C)$的输入转换为$(1×1×C)$的输出，即将空间上所有点的信息都平均成了一个值。这么做是因为最终的scale是对整个通道作用的，这就得基于通道的整体信息来计算scale。另外作者要利用的是通道间的相关性，而不是空间分布中的相关性，用GAP屏蔽掉空间上的分布信息能让scale的计算更加准确。<script type="math/tex; mode=display">z_c=F_{sq}(U_c)=\frac{1}{W×H}\sum_{i=1}^{W}\sum_{j=1}^{H}u_c(i,j)</script></li><li><strong>Excitation操作</strong><br>它是一个类似于循环神经网络中门的机制。通过参数 w 来为每个特征通道生成权重，其中参数 w 被学习用来显式地建模特征通道间的相关性。用两个全连接来实现，第一个全连接把C个通道压缩成了C/r个通道来降低计算量（后面跟了RELU），第二个全连接再恢复回C个通道（后面跟了Sigmoid），r是指压缩的比例。作者尝试了r在各种取值下的性能 ，最后得出结论r=16时整体性能和计算量最平衡。<script type="math/tex; mode=display">s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))</script></li><li><strong>*Rewight操作</strong><br>将 Excitation 的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。<script type="math/tex; mode=display">\overline{x_c}=F_{scale}=u_c\cdot s_c</script></li></ul><h1 id="SENet在具体网络中应用"><a href="#SENet在具体网络中应用" class="headerlink" title="SENet在具体网络中应用"></a>SENet在具体网络中应用</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/honUhNUdgJEd.png?imageslim" alt="mark"></p><p>上左图是将 SE 模块嵌入到 Inception 结构的一个示例。方框旁边的维度信息代表该层的输出。这里我们使用 global average pooling 作为 Squeeze 操作。紧接着两个 Fully Connected 层组成一个 Bottleneck 结构去建模通道间的相关性，并输出和输入特征同样数目的权重。我们首先将特征维度降低到输入的 1/16，然后经过 ReLu 激活后再通过一个 Fully Connected 层升回到原来的维度。这样做比直接用一个 Fully Connected 层的好处在于：1）具有更多的非线性，可以更好地拟合通道间复杂的相关性；2）极大地减少了参数量和计算量。然后通过一个 Sigmoid 的门获得 0~1 之间归一化的权重，最后通过一个 Scale 的操作来将归一化后的权重加权到每个通道的特征上。<br>除此之外，SE 模块还可以嵌入到含有 skip-connections 的模块中。上右图是将 SE 嵌入到 ResNet 模块中的一个例子，操作过程基本和 SE-Inception 一样，只不过是在 Addition 前对分支上 Residual 的特征进行了特征重标定。如果对 Addition 后主支上的特征进行重标定，由于在主干上存在 0~1 的 scale 操作，在网络较深 BP 优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。<br>目前大多数的主流网络都是基于这两种类似的单元通过 repeat 方式叠加来构造的。由此可见，SE 模块可以嵌入到现在几乎所有的网络结构中。通过在原始网络结构的 building block 单元中嵌入 SE 模块，我们可以获得不同种类的 SENet。如 SE-BN-Inception、SE-ResNet、SE-ReNeXt、SE-Inception-ResNet-v2 等等。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/vKzgpTccYKmN.png?imageslim" alt="mark"></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/0Hjkwl78L9Qf.png?imageslim" alt="mark"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记５：CenterNet:Keypoint Triplets for Object Detection</title>
      <link href="/2019/07/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B05/"/>
      <url>/2019/07/31/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B05/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/abs/1904.08189" target="_blank" rel="noopener">https://arxiv.org/abs/1904.08189</a><br>代码地址：<a href="https://github.com/Duankaiwen/CenterNet" target="_blank" rel="noopener">https://github.com/Duankaiwen/CenterNet</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>目标检测中，基于关键点的方法经常出现大量不正确的边界框，主要是由于缺乏对相关剪裁区域的额外监督造成的。本文提出一种有效的方法，以最小的资源探索剪裁区域的视觉模式。本文提出的CenterNet是一个单阶段的关键点检测模型。CenterNet通过检测每个目标物看作是一个三个关键点，而不是一对关键点，这样做同时提高了准确率及召回率。本文还设计了另外两个模型，cascade corner pooling及center pooling，容易获得从左上角及右下角的丰富信息，同时在中间区域获得更多的识别信息。在ＭＳ-COCO数据集中ＣenterNet的AP达到了47.0%。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文改进了CornerNet，增加了一个观察每个候选区域的视觉模式的功能，进而可以判断每个边界框的正确性。CenterNet通过增加一个关键点来探索proposal中间区域（近似几何中心）的信息，本文创新点在于，如果预测的边界框与ground truth有较高的IoU,则中心关键点预测出相同类别的概率要高，反之亦然。因此，在进行inference时，当通过一组关键点产生了一个边界框，我们继续观察是否具有同类别的一个关键点落入区域的中心，即使用三个点表示目标。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/S6eAnW6ACWt4.png?imageslim" alt="mark"><br>为了更好的检测中心的关键点即角点，提出了两个方法来丰富中心点及角点的信息。</p><ul><li><strong>center pooling</strong>：用于预测中心关键点的分支，有利于中心获得更多目标物的中心区域，进而更易感知proposal的中心区域。通过取中心位置横向与纵向响应值的和的最大值实现此方法。</li><li><strong>cascade corner pooling</strong>：增加原始的corner pooling感知内部信息的功能。结合了feature map中目标物内部及边界方向的响应值和的最大值来预测角点。经试验证实，此方法在存在feature-level 噪声的情况下更加稳定，有助于准确率及召回率的提升。</li></ul><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><strong>Two-stage approaches</strong><br>把目标检测task分为两个stages：提取RoIs,然后对RoIs进行分类和回归。</li><li><strong>One-stage approaches</strong><br>移除RoI提取步骤，直接对候选的anchor boxs进行分类和回归</li></ul><h1 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h1><h2 id="baseline-and-Motivation"><a href="#baseline-and-Motivation" class="headerlink" title="baseline and Motivation"></a>baseline and Motivation</h2><p>本论文的 baseline 为 CornerNet，因此首先讨论 CornerNet 为什么容易产生很多的误检。首先，CornerNet 通过检测角点确定目标，而不是通过初始候选框 anchor 的回归确定目标，由于没有了 anchor 的限制，使得任意两个角点都可以组成一个目标框，这就对判断两个角点是否属于同一物体的算法要求很高，一但准确度差一点，就会产生很多错误目标框。其次，恰恰这个算法有缺陷。因为此算法在判断两个角点是否属于同一物体时，缺乏全局信息的辅助，因此很容易把原本不是同一物体的两个角点看成是一对角点，因此产生了很多错误目标框。最后，角点的特征对边缘比较敏感，这导致很多角点同样对背景的边缘很敏感，因此在背景处也检测到了错误的角点。综上原因，使得 CornerNet 产生了很多误检。</p><p>我们分析出了 CornerNet 的问题后，接下来就是找出解决之道，关键问题在于让网络具备感知物体内部信息的能力。一个较容易想到的方法是把 CornerNet 变成一个 two-stage 的方法，即利用 RoI pooling 或 RoI align 提取预测框的内部信息，从而获得感知能力。但这样做开销很大，因此我们提出了用关键点三元组来检测目标，这样使得我们的方法在 one-stage 的前提下就能获得感知物体内部信息的能力。并且开销较小，因为我们只需关注物体的中心，从而避免了 RoI pooling 或 RoI align 关注物体内部的全部信息。</p><h2 id="Object-Detecton-as-Keypoint-Triplets"><a href="#Object-Detecton-as-Keypoint-Triplets" class="headerlink" title="Object Detecton as Keypoint Triplets"></a>Object Detecton as Keypoint Triplets</h2><p>网络通过 center pooling 和 cascade corner pooling 分别得到 center heatmap 和 corner heatmaps，用来预测关键点的位置。得到角点的位置和类别后，通过 offsets 将角点的位置映射到输入图片的对应位置，然后通过 embedings 判断哪两个角点属于同一个物体，以便组成一个检测框。正如前文所说，组合过程中由于缺乏来自目标区域内部信息的辅助，从而导致大量的误检。为了解决这一问题，CenterNet 不仅预测角点，还预测中心点。我们对每个预测框定义一个中心区域，通过判断每个目标框的中心区域是否含有中心点，若有则保留，并且此时框的 confidence 为中心点，左上角点和右下角点的confidence的平均，若无则去除，使得网络具备感知目标区域内部信息的能力，能够有效除错误的目标框。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/NBiSY1pGPExb.png?imageslim" alt="mark"><br>我们发现中心区域的尺度会影响错误框去除效果。中心区域过小导致很多准确的小尺度的目标也会被去除，而中心区域过大导致很多大尺度的错误目标框无法被去除，因此我们提出了尺度可调节的中心区域定义法。</p><script type="math/tex; mode=display">\left\{ \begin{array}{c}    ctl_{x}=\frac{(n+1)tl_x+(n-1)br_x}{2n} \\     ctl_{y}=\frac{(n+1)tl_y+(n-1)br_y}{2n} \\     cbr_{x}=\frac{(n-1)tl_x+(n+1)br_x}{2n} \\    cbr_{y}=\frac{(n-1)tl_y+(n+1)br_y}{2n}\end{array}\right.</script><p>该方法可以在预测框的尺度较大时定义一个相对较小的中心区域，在预测框的尺度较小时预测一个相对较大的中心区域。如图所示。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/tpzAgOVQ6w05.png?imageslim" alt="mark"><br>$n$决定了中心区域的尺度。在本文中，对于尺度小于或者大于150的边界框，$n$分别取值为3或5。</p><h2 id="Enriching-Center-and-Corner-Information"><a href="#Enriching-Center-and-Corner-Information" class="headerlink" title="Enriching Center and Corner Information"></a>Enriching Center and Corner Information</h2><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/1ntAyvp5mdn5.png?imageslim" alt="mark"></p><ul><li><strong>Center pooling</strong><br>一个物体的中心并不一定含有很强的，易于区分于其他类别的语义信息。例如，一个人的头部含有很强的，易于区分于其他类别的语义信息，但是其中心往往位于人的中部。我们提出了center pooling 来丰富中心点特征。center pooling提取中心点水平方向和垂直方向的最大值并相加，以此给中心点提供所处位置以外的信息。这一操作使中心点有机会获得更易于区分于其他类别的语义信息。Center pooling 可通过不同方向上的 corner pooling 的组合实现。一个水平方向上的取最大值操作可由 left pooling 和 right pooling通过串联实现，同理，一个垂直方向上的取最大值操作可由 top pooling 和 bottom pooling通过串联实现</li><li><p><strong>Cascade corner pooling</strong><br>一般情况下角点位于物体外部，所处位置并不含有关联物体的语义信息，这为角点的检测带来了困难。传统做法 corner pooling，它提取物体边界最大值并相加，该方法只能提供关联物体边缘语义信息，对于更加丰富的物体内部语义信息则很难提取到。cascade corner pooling 首先提取物体边界最大值，然后在边界最大值处继续向内部(图中沿虚线方向)提取提最大值，并与边界最大值相加，以此给角点特征提供更加丰富的关联物体语义信息。Cascade corner pooling 也可通过不同方向上的 corner pooling 的组合实现。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/UgBB4Ir7T0n2.png?imageslim" alt="mark"></p><h2 id="Training-and-Inference"><a href="#Training-and-Inference" class="headerlink" title="Training and Inference"></a>Training and Inference</h2></li><li><p><strong>Training</strong> 输入图像的分辨率为512×512，heatmaps的大小为128×128，我们使用数据增强来训练一个鲁棒的模型。用Adam来优化训练的损失：</p><script type="math/tex; mode=display">L=L_{det}^{co}+L_{det}^{ce}+\alpha L_{pull}^{co}+\beta L_{push}^{co}+\gamma(L_{off}^{co}+L_{off}^{ce})</script></li></ul><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><ul><li><strong>Comparisons with State-of-the-art Detectors</strong><br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/m19Pji4uD5je.png?imageslim" alt="mark"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> anchor-free </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 关键点检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记4：CornerNet:Detecting Objects as Paired Keypoints</title>
      <link href="/2019/07/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04/"/>
      <url>/2019/07/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B04/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="noopener">https://arxiv.org/abs/1808.01244</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="noopener">https://github.com/umich-vl/CornerNet</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了一种新的目标检测方法，使用单个卷积神经网络将目标边界框检测为一对关键点（即边界框的左上角和右下角）。通过将目标检测为成对关键点，我们消除了现有的one stage检测器设计中对一组anchors的需要。除了上述新颖的构想，文章还引入了corner pooling，这是一种新型的池化层，可以帮助网络更好地定位边界框的角点。CornerNet在MS COCO上实现了42.1％的AP，优于所有现有的one stage检测器。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>基于卷积神经网络的对象检测器（ConvNets）已经在各种具有挑战性的基准测试中取得了最新成果。现有技术方法的一个共同组成部分是anchor boxes，它们是包含各种尺寸和宽高比的矩形框，是用作检测的候选框。anchor boxes广泛用于one stage检测器，它可以获得与two stage检测器高度相当的结果，同时效率更高。 one stage检测器将anchor boxes密集地分布在图像上，通过对anchor boxes进行评分，并通过回归来改进其坐标来生成最终的边界框预测。</p><p>但anchor boxes的使用有两个缺点。 首先，我们通常需要一组非常大的anchor boxes，例如： 在DSSD中超过4万，在RetinaNet中超过10万， 这是因为训练器被训练以分类每个anchor boxes是否与ground truth充分重叠，并且需要大量anchor boxes以确保与大多数ground truth充分重叠。 结果，只有一小部分anchor boxes与ground truth重叠; 这在正负样本之间造成了巨大的不平衡，减慢了训练速度。</p><p>其次，anchor boxes的使用引入了许多超参数和设计选择。 这些包括多少个box，大小和宽高比。 这些选择主要是通过ad-hoc启发式方法进行的，并且当与多尺度架构相结合时可能会变得更加复杂，其中单个网络在多个分辨率下进行单独预测，每个尺度使用不同的特征和它自己的一组anchor boxes。</p><p>在本文中，我们介绍了CornerNet，这是一种新的one stage目标检测方法，可以消除anchor boxes。 我们将一个目标物体检测为一对关键点——边界框的左上角和右下角。 我们使用单个卷积网络来预测同一物体类别的所有实例的左上角的热图，所有右下角的热图，以及每个检测到的角点的嵌入向量。 嵌入用于对属于同一目标的一对角点进行分组——训练网络以预测它们的类似嵌入。 我们的方法极大地简化了网络的输出，并且无需设计anchor boxes。我们的方法受到Newell等人在多人姿态估计上下文中关联嵌入的启发。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/8oWSGbF7LwCN.png?imageslim" alt="mark"></p><p>CornerNet的另一个新颖组件是corner pooling，这是一种新型的池化层，可帮助卷积网络更好地定位边界框的角点。 边界框的一角通常在目标之外，参考圆形的情况以及图2中的例子。在这种情况下，角点不能根据当前的信息进行定位。相反，为了确定像素位置是否有左上角，我们需要水平地向右看目标的最上面边界，垂直地向底部看物体的最左边边界。 这激发了我们的corner pooling layer：它包含两个特征图; 在每个像素位置，它最大池化从第一个特征映射到右侧的所有特征向量，最大池化从第二个特征映射下面的所有特征向量，然后将两个池化结果一起添加</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/dJdvIuRl3DVd.png?imageslim" alt="mark"></p><p>我们假设了两个原因，为什么检测角点会比检测边界框中心或proposals更好。首先，盒子的中心可能更难以定位，因为它取决于目标的所有4个边，而定位角取决于2边，因此更容易，甚至更多的corner pooling，它编码一些明确的关于角点定义的先验信息。 其次，角点提供了一种更有效的方式来密集地离散边界框的空间：我们只需要用$O(wh)$角点来表示$O(w^2h^2)$可能的anchor boxes。</p><h1 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h1><h2 id="Two-stage-object-detectors"><a href="#Two-stage-object-detectors" class="headerlink" title="Two-stage object detectors"></a>Two-stage object detectors</h2><p>Two-stage目标检测由R-CNN首次引入并推广。Two-stage检测器生成一组稀疏的感兴趣区域(RoIs)，并通过网络对每个区域进行分类。R-CNN使用低层次视觉算法生成(RoIs)。然后从图像中提取每个区域，由ConvNet独立处理，这将导致大量计算冗余。后来，SPP-Net和Fast R-CNN改进了R-CNN，设计了一个特殊的池化层(金字塔池化)，将每个区域从feature map中池化。然而，两者仍然依赖于单独的proposals算法，不能进行端到端训练。Faster-RCNN通过引入区域生成网络(RPN)来去除低层次的proposals算法，RPN从一组预先确定的候选框(通常称为anchor boxes)中生成proposals。这不仅使检测器更加高效，通过RPN与检测网络的联合训练，可实现端到端训练。R-FCN将全连接子检测网络替换为完全卷积子检测网络，进一步提高了Faster R-CNN的检测效率。其他的工作主要集中在结合子类别信息，用更多的上下文信息在多个尺度上生成目标的proposals，选择更好的特征，提高速度，并行处理和更好的训练过程。</p><h2 id="One-stage-object-detectors"><a href="#One-stage-object-detectors" class="headerlink" title="One-stage object detectors"></a>One-stage object detectors</h2><p>另一方面，YOLO和SSD推广了one-stage方法，该方法消除了RoI池化步骤，并在单个网络中检测目标。One-stage检测器通常比two-stage检测器计算效率更高，同时在不同的具有挑战性的基准上保持着具有竞争性的性能。<br>SSD算法将anchor boxes密集地放置在多个尺度的feature maps之上，直接对每个anchor boxes进行分类和细化。YOLO直接从图像中预测边界框坐标，后来在YOLO9000中，通过使用anchor boxes进行了改进。DSSD和RON采用了类似沙漏的网络，使它们能够通过跳跃连接将低级和高级特性结合起来，从而更准确地预测边界框。然而，在RetinaNet出现之前，这些one-stage检测器的检测精度仍然落后于two-stage检测器。在RetinaNet中，作者认为密集的anchor boxes在训练中造成了正样本和负样本之间的巨大不平衡。这种不平衡导致训练效率低下，从而导致结果不佳。他们提出了一种新的loss，Focal Loss，来动态调整每个anchor boxes的权重，并说明了他们的one-stage检测器检测性能优于two-stage检测器。RefineDet建议对anchor boxes进行过滤，以减少负样本的数量，并对anchor boxes进行粗略的调整。</p><p>我们的方法受到Newell等人在多人姿态估计上下文中关联嵌入的启发。Newell等人提出了一种在单个网络中检测和分组人类关节的方法。在他们的方法中，每个检测到的人类关节都有一个嵌入向量。这些关节是根据它们嵌入的距离来分组的。本文是第一个将目标检测任务定义为同时检测和分组角点的任务。我们的另一个新颖之处在于corner pooling layer，它有助于更好定位角点。我们还对沙漏结构进行了显著地修改，并添加了新的focal loss的变体，以帮助更好地训练网络。</p><h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>在CornerNet中，我们将物体边界框检测为一对关键点（即边界框的左上角和右下角）。卷积网络通过预测两组热图来表示不同物体类别的角的位置，一组用于左上角，另一组用于右下角。 网络还预测每个检测到的角的嵌入向量，使得来自同一目标的两个角的嵌入之间的距离很小。 为了产生更紧密的边界框，网络还预测偏移以稍微调整角的位置。 通过预测的热图，嵌入和偏移，我们应用一个简单的后处理算法来获得最终的边界框。<br>我们使用沙漏网络作为CornerNet的骨干网络。 沙漏网络之后是两个预测模块。 一个模块用于左上角，而另一个模块用于右下角。 每个模块都有自己的corner pooling模块，在预测热图、嵌入和偏移之前，池化来自沙漏网络的特征。 与许多其他物体探测器不同，我们不使用不同尺度的特征来检测不同大小的物体。 我们只将两个模块应用于沙漏网络的输出。</p><h2 id="Detecting-Corners"><a href="#Detecting-Corners" class="headerlink" title="Detecting Corners"></a>Detecting Corners</h2><p>我们预测两组热图，一组用于左上角，另一组用于右下角。 每组热图具有C个通道，其中C是分类的数量，并且大小为H×W。 没有背景通道。 每个通道都是一个二进制掩码，用于表示该类的角点位置。<br>对于每个角点，有一个ground-truth正位置，其他所有的位置都是负值。 在训练期间，我们没有同等地惩罚负位置，而是减少对正位置半径内的负位置给予的惩罚。 这是因为如果一对假角点检测器靠近它们各自的ground-truth位置，它仍然可以产生一个与ground-truth充分重叠的边界框（图5）。我们通过确保半径内的一对点生成的边界框与ground-truth的IoU ≥ t（我们在所有实验中将t设置为0.7）来确定物体的大小，从而确定半径。 给定半径，惩罚的减少量由非标准化的2D高斯$e^{-\frac{x^2+y^2}{2\sigma^{2}}}$给出，其中心位于正位置，其$\sigma$是半径的1/3。<br>$p_{cij}$为预测热图中$c$类位置$(i,j)$的得分，$y_{cij}$为用非标准化高斯增强的“ground-truth”热图。我们设计了一个局部损失(focal loss)的变体:</p><script type="math/tex; mode=display">L_{det}=\frac{-1}{N}\sum_{c-1}^{C}\sum_{i=1}^{H}\sum_{j=1}^{W}\left\{  \begin{array}{lr}    (1-p_{cij})^{\alpha}log(p_{cij}) & if 　y_{cij} = 1\\    (1-y_{cij})^{\beta}(p_{cij})^{\alpha}lpg(1-p_{cij}) & otherwise  \end{array}\right.</script><p>其中N是图像中目标的数量,$\alpha$和$\beta$是控制每个点的贡献的超参数（在所有实验中我们将$\alpha$设置为2, $\beta$设置为4）。利用$y_{cij}$中编码的高斯凸点，$(1-y_{cij})$项减少了ground-truth周围的惩罚。</p><p>许多网络涉及下采样层以收集全局信息和减少内存使用。当它们完全卷积应用于图像时，输出的尺寸通常小于图像。因此，图像中的位置$(x,y)$被映射到热图中的位置$(\lfloor\frac{x}{n}\rfloor,[\lfloor\frac{y}{n}\rfloor)$，其中n是下采样因子。当我们将热图中的位置重新映射到输入图像时，可能会丢失一些精度，这会极大地影响小边界框与ground-truth之间的IoU。 为了解决这个问题，我们预测位置偏移，以稍微调整角位置，然后再将它们重新映射到输入分辨率。</p><script type="math/tex; mode=display">o_{k}=(\frac{x_{k}}{n}-\lfloor\frac{x_{k}}{n}\rfloor,\frac{y_{k}}{n}-\lfloor\frac{y_{k}}{n}\rfloor)</script><p>其中$o_{k}$是偏移量，$x_{k}$和$y_{k}$是角点$k$的x和y坐标。特别是，我们预测所有类别的左上角共享一组偏移，另一组由右下角共享。对于训练，我们在ground-truth角点位置应用平滑的L1损失:</p><script type="math/tex; mode=display">L_{off}=\frac{1}{N}\sum_{k=1}^{N}SmoothL1Loss(o_{k},\hat{o}_{k}</script><h2 id="Grouping-Corners"><a href="#Grouping-Corners" class="headerlink" title="Grouping Corners"></a>Grouping Corners</h2><p>图像中可能出现多个目标，因此可能检测到多个左上角和右下角。我们需要确定左上角和右下角的一对角点是否来自同一个目标边界框。我们的方法受到Newell等人提出的用于多人姿态估计任务的关联嵌入方法的启发。Newell等人检测所有人类关节，并为每个检测到的关节生成嵌入。他们根据嵌入之间的距离将节点进行分组。</p><p>关联嵌入的思想也适用于我们的任务。 网络预测每个检测到的角点的嵌入向量，使得如果左上角和右下角属于同一个边界框，则它们的嵌入之间的距离应该小。 然后，我们可以根据左上角和右下角嵌入之间的距离对角点进行分组。 嵌入的实际值并不重要。 仅使用嵌入之间的距离来对角点进行分组。</p><p>我们关注Newell等人并使用1维嵌入。$e_{tk}$成为对象k的左上的嵌入，$e_{bk}$为右下角的的嵌入。如中所述，我们使用“pull”损失来训练网络对角点进行分组，并且用“push”损失来分离角点：</p><script type="math/tex; mode=display">L_{pull}=\frac{1}{N}\sum_{k=1}^{N}[(e_{tk}-e_{k})^2+(e_{bk}-e_{k})^2]</script><script type="math/tex; mode=display">L_{push}=\frac{1}{N-1}\sum_{k=1}^{N}\sum_{j=1,j\neq k}^{N}max(0,\Delta-|e_{k}-e_{j}|)</script><p>其中$e_{k}$是$e_{tk}$和$e_{bk}$的平均值，我们在所有实验中将$\Delta$设为1。与偏移损失类似，我们仅在ground-truth角点位置应用损失。</p><h2 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h2><p>如图2所示，通常没有局部视觉证据表明存在角点。要确定像素是否为左上角，我们需要水平地向右看目标的最上面边界，垂直地向底部看物体的最左边边界。因此，我们提出corner Pooling通过编码显式先验知识来更好地定位角点。</p><p>假设我们要确定$(i,j)$位置是左上角。设$f_t$和$f_l$为左上角池化层的输入特征映射，$f_{t_{ij}}$和$f_{l_{ij}}$分别为$(i,j)$位置中$f_t$和$f_lf$的向量$H×W$的特征映射，corner pooling层首先最大池化$f_t$中在$(i,j)$与$(i,H)$之间所有的特征向量，使之成为特征向量$t_{ij}$，还有，最大池化$f_l$中在$(i,j)$与$(W,j)$之间所有的特征向量，使之成为特征向量$l_{ij}$。最后，把$t_{ij}$和$l_{ij}$加在一起。这个计算可以用以下公式表示:</p><script type="math/tex; mode=display">t_{ij} = \left\{  \begin{array}{lr}    max({f_t}_{ij},t_{(i+1)j}) & if　i < H\\    {f_t}_{Hj} & otherwise  \end{array}\right.</script><script type="math/tex; mode=display">l_{ij} = \left\{  \begin{array}{lr}    max({f_l}_{ij},l_{i(j+1)}) & if　j < H\\    {f_l}_{iW} & otherwise  \end{array}\right.</script><p>在这里，我们应用了一个elementwise最大化操作。动态规划可以有效地计算$t_{ij}$和$l_{ij}$。我们以类似的方式定义右下角池化层。最大池化$(0,j)$与$(i,j)$之间所有的特征向量、$(i,0)$与$(i,j)$之间所有的特征向量，然后将池化结果相加。Corner pooling层用于预测模块，用于预测热图、嵌入和偏移量。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/E04bx45Kh5V4.png?imageslim" alt="mark"></p><p>预测模块的结构如图所示。模块的第一部分是残差模块的修改版本。在这个修改后的残差模块中，我们将第一个3×3的卷积模块替换为一个corner pooling模块。这个残差模块，首先通过具有128个通道的2个3×3卷积模块的主干网络处理特征，然后应用一个corner pooling层。残差模块之后，我们将池化特征输入具有256个通道的3×3的Conv-BN层，然后加上反向projection shortcut。修改后的残块，后跟一个具有256个通道的3×3的卷积模块，和256个通道的3个Conv-ReLU-Conv层来产生热图，嵌入和偏移量。</p><h2 id="Hourglass-Network"><a href="#Hourglass-Network" class="headerlink" title="Hourglass Network"></a>Hourglass Network</h2><p>CornerNet使用沙漏网络作为其骨干网络。沙漏网络首次被提到是用于人体姿态估计任务。它是一个完全卷积神经网络，由一个或多个沙漏模块组成。沙漏模块首先通过一系列卷积层和最大池化层对输入特性进行下采样。然后通过一系列的上采样和卷积层将特征上采样回原来的分辨率。由于细节在最大池化层中丢失，因此添加了跳过层用来将细节带回到上采样的特征。沙漏模块在一个统一的结构中捕获全局和局部特征。当多个沙漏模块堆积在网络中时，沙漏模块可以重新处理特征以获取更高级别的信息。这些特性使沙漏网络成为目标检测的理想选择。事实上，许多现有的检测器已经采用了类似沙漏网络的网络。</p><p>我们的沙漏网络由两个沙漏组成，我们对沙漏模块的结构做了一些修改。我们不使用最大池，而是使用步长2来降低特征分辨率。我们减少了5倍的特征分辨率，并增加了特征通道的数量(256,384,384,384,512)。当我们对特征进行上采样时，我们应用了两个残差模块，然后是一个最近的相邻上采样。每个跳跃连接还包含两个残差模块。沙漏模块中间有4个512通道的残差模块。在沙漏模块之前，我们使用128个通道7×7的卷积模块，步长为2，4倍减少的图像分辨率，后跟一个256个通道，步长为2的残差块。</p><p>在基础上，我们还在训练时增加了中间监督。但是，我们没有向网络中添加反向中间预测，因为我们发现这会损害网络的性能。我们在第一个沙漏模块的输入和输出，应用了一个3×3的Conv-BN模块。然后，我们通过元素级的加法合并它们，后跟一个ReLU和一个具有256个通道的残差块，然后将其用作第二个沙漏模块的输入。沙漏网络的深度为104。与许多其他最先进的检测器不同，我们只使用整个网络最后一层的特征来进行预测。</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h2><p>我们在PyTorch中实现了CornerNet。网络是在默认的PyTorch设置下随机初始化的，没有在任何外部数据集上进行预训练。当我们应用focal loss时，我们遵循来设置卷积层中的偏差来预测角点热图。在训练期间，我们设置了网络的输入分辨率511×511,导致输出分辨率为128×128。为了减少过拟合，我们采用了标准的数据增强技术，包括随机水平翻转、随机缩放、随机裁剪和随机色彩抖动，其中包括调整图像的亮度，饱和度和对比度。 最后，我们将PCA应用于输入图像。<br>我们使用Adam来优化完整的训练损失：</p><script type="math/tex; mode=display">L=L_{det}+\alpha L_{pull}+\beta L_{push}+\gamma L_{off}</script><p>其中$\alpha$，$\beta$和$\gamma$分别是pull，push和offset的权重。我们将$\alpha$和$\beta$都设置为0.1，将$\gamma$设置为1。我们发现，1和更大的$\alpha$和$\beta$值会导致性能不佳。我们使用49的batch size，并在10个Titan X（PASCAL）GPU上训练网络（主GPU 4个图像，其余GPU每个GPU 5个图像）。 为了节省GPU资源，在我们的 ablation experiments（即模型简化测试，去掉该结构的网络与加上该结构的网络所得到的结果进行对比）中，我们训练网络，进行250k次迭代，学习率为$2.5×10^{-4}$。当我们将我们的结果与其他检测器进行比较时，我们额外训练网络，进行250 k次迭代，并到最后50 k次迭代时，将学习速率降低到$2.5×10^{-5}$。</p><h2 id="Testing-Details"><a href="#Testing-Details" class="headerlink" title="Testing Details"></a>Testing Details</h2><p>在测试期间，我们使用简单的后处理算法从热图，嵌入和偏移生成边界框。我们首先通过在角点热图上使用3×3最大池化层来应用非极大值抑制(NMS)。然后我们从热图中选择前100个左上角和前100个右下角。角点位置由相应的偏移调整。我们计算左上角和右下角嵌入之间的L1距离。距离大于0.5或包含不同类别的角点对将被剔除。左上角和右下角的平均得分用作检测分数。</p><p>我们不是将图像大小调整为固定大小，而是保持图像的原始分辨率，并在将其输入给CornerNet之前用0填充。原始图像和翻转图像都用于测试。我们将原始图像和翻转图像的检测结合起来，并应用soft-max来抑制冗余检测。仅记录前100个检测项。Titan X（PASCAL）GPU上的每个图像的平均检测时间为244ms。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> anchor-free </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 关键点检测 </tag>
            
            <tag> Heatmap </tag>
            
            <tag> Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记3：Selective Kernel Networks</title>
      <link href="/2019/07/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/"/>
      <url>/2019/07/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B03/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1903.06586.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1903.06586.pdf</a><br>论文代码：<a href="https://github.com/implus/SKNet" target="_blank" rel="noopener">https://github.com/implus/SKNet</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>在标准的卷积网络中，每层网络中神经元的感受野的大小都是相同的。在神经学中，视觉神经元感受野的大小是由刺激机制构建的，而在卷积网络中却很少考虑这个因素。本文提出的方法可以使神经元对于不同尺寸的输入信息进行自适应的调整其感受野的大小。building block为Selective Kernel单元。其存在多个分支，每个分支的卷积核的尺寸都不同。不同尺寸的卷积核最后通过softmax进行融合。分支中不同注意力在fusion layer产生不同的有效感受野。多个SK单元进行堆叠构成Selective  Kernel  Networks  (SKNets)。详细分析显示SKNet里的神经元可以根据输入通过适应性的调整感受野的大小来捕捉不同尺度的目标object。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在上世纪猫的前视觉皮层神经元的局部感受野激发了卷积网络的产生。在视觉皮层中，相同区域的神经元的局部感受野的大小是不同的，从而可以在相同的处理阶段中获得不同尺寸的空间信息。该思想被Inception系列网络完美的应用，在其building block中，3x3,5x5,7x7的卷积通过简单的拼接来得到多尺寸信息。</p><p>然而，在设计卷积网络时，神经元感受野的其他属性并没有被考虑到。比如感受野尺寸的自适应调整。大量实验证明，视觉皮层的神经元的感受野尺寸并不是固定的，而是受激励调制的。像Inception这种具有多个分支的网络其内部存在一种潜在的机制可以在下一个卷积层根据输入的内容调整神经元感受野的大小，这是因为下一个卷积层通过线性组合将不同分支的特征进行融合。但是线性组合的方法不足于提供网络强大的调整能力。</p><p>本文提出了一种非线性的方法聚合来自不同核的特征进而实现感受野不同尺寸的调整。引入了”Selective Kernel”卷积，其包含了三个操作：Split,Fuse,Select：</p><ul><li>Split操作产生多个不同核尺寸的通道对应于神经元的不同感受野尺寸。</li><li>Fuse操作组合融合来自多通道的信息从而获得一个全局及可理解性的表示用于进行权重选择。</li><li>Select操作根据挑选得到的权重对不同核尺寸的feature map进行融合。</li></ul><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul><li>Multi-batch convolutional network<ul><li>Highway network</li><li>ResNet</li><li>FractalNets and Multilevel ResNets</li><li>TnceptionNets</li></ul></li><li>Grouped/depthwise/dilated convolutions<ul><li>Grouped convolution 减少了参数个数以及计算成本，首次应用于AlexNet。使用grouped convolution，ResNeXts也能够提升准确率。</li><li>Depthwise convolution grouped convolution的特殊情况，groups的个数等于通道的个数。</li><li>Dilated convolutions 可以指数式扩大感受野，而且不会loss of coverage。<br>在SK convolution中，大尺寸的卷积核被设计成grouped/depthwise/dilated convolutions的集合，来避免沉重的开销。</li></ul></li><li>Attention mechanisms<br>设计一系列的注意力分配系数，也就是一系列权重参数，可以用来强调或选择目标处理对象的重要信息，并且抑制一些无关的细节信息。</li><li>Dynamic convolutions<ul><li>Dynamic Filter 只能适应性的改变filters的参数而不是卷积核大小的调整</li><li>Active Convo-lution 使用偏移来增强卷积中的位置采样</li><li>De-formable Convolutional Networks 动态地产生位置偏移</li></ul></li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Selective-Kernel-Convolution"><a href="#Selective-Kernel-Convolution" class="headerlink" title="Selective Kernel Convolution"></a>Selective Kernel Convolution</h2><p>SK convolution 包含三个操作——Split,Fuse以及Select，如下图所示：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/rBQK3LMJSQBa.png?imageslim" alt="mark"><br>在这个例子中由两个分支，因此只有两个不同的卷积核，单它很容易拓展成多个分支。</p><ul><li><p>Split：给定feature map $X\in R^{H^{‘}×W^{‘}×C^{‘}}$,我们先执行两个卷积核大小分别为3和5的变换$\overline{T}:X\rightarrow \overline{U}\in R^{R×W×C}$以及$\hat{T}:X\rightarrow \hat{U}\in R^{R×W×C}$两种变换都是由一系列高效的grouped/depthwise convolution，Batch Normalization以及ReLU函数组成。为了更加高效，5<em>5的卷积核由3</em>3空洞为2的空洞卷积代替。</p></li><li><p>Fuse：基本的想法是使用gates来控制来自多个分支的带着不同尺度信息的information流入下一层的神经元。为了实现这个目标，gates需要聚合来自所有分支的information。我们首先通过一个逐元素的summation：</p><script type="math/tex; mode=display">U=\overline{U}+\hat{U}</script><p>然后我们编码这个全局信息通过全局平均池化来生成逐通道的统计量$s\in R^{C}$s中的第c个元素通过在U的HxW维度上进行压缩计算得到：</p><script type="math/tex; mode=display">s_{c}=F_{gp}(U_{c})=\frac{1}{H×W}\sum_{i=1}^{H}\sum_{j=1}^{W}U_{c}(i,j)</script><p>然后产生一个用于指导精确性以及适应性选择的紧凑的feature$z\in R^{d×1}$。通过一个全连接层得到，同时，进行了降维处理：</p><script type="math/tex; mode=display">z=F_{fc}(s)=\delta(B(Ws))</script><p>$\delta$为ReLU函数，B为Batch Normalization。为了验证W中d的作用，引入了一个衰减率r，如下，其中C代表通道数，L为d的最小值。</p><script type="math/tex; mode=display">d=max(C/r,L)</script></li><li><p>Select：通道间的soft attention可以选择不同尺寸的信息，其被紧凑的特征信息Z引导。在channel-wise应用softmax操作。</p><script type="math/tex; mode=display">a_{c}=\frac{e^{A_{c}z}}{e^{A_{c}z+e^{B_{c}z}}} \quad b_{c}=\frac{e^{B_{c}z}}{e^{A_{c}z+e^{B_{c}z}}}</script><p>$A,B\in R^{C×d}$，个人理解矩阵A和B就是两个全连接的权值矩阵，这里就存在一个疑问，那么生成向量z的意义是什么，可以直接在向量s上做全连接。向量a和b分别为$\overline{U}$以及$\hat{U}$的soft attention vector。$A_{c}$为矩阵A的第c行，$a_{c}$为向量a的第c个元素。在两个分支的情况中，矩阵B是多余的，因为$a_{c}+b_{c}=1$。最后的feature map$V$通过不同卷积核的注意力权重得到：</p><script type="math/tex; mode=display">V_{c}=a_{c}\cdot \overline{U}+b_{c}\cdot \hat{U},\quad a_{c}+b_{c}=1</script><p>$V=[V_{1},V_{2},…,V_{C}],\quad V_{c}\in R^{H×W}$</p></li></ul><h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h2><p>和ResNeXt类似，SKNet也主要由一堆名为“SK unit”的repeated bottleneck blocks组成。每一个SK unit由1<em>1的卷积，SK卷积以及1</em>1卷积组成。ResNeXt中所有具有较大尺寸的卷积核都替换为SK卷积，从而可以使网络选择合适的感受野大小。<br>在SK单元中，存在三个重要的超参数，来决定SK卷积的最终设置：</p><ul><li>路径的数量M，决定了聚合多少种不同尺寸的卷积核。</li><li>组数G，控制每一条路径的基数</li><li>衰减率r，控制在fuse操作中参数的个数</li></ul><p>网络结构如下：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/Pf4spQPdb7Fm.png?imageslim" alt="mark"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/JMgYO8YaevDQ.png?imageslim" alt="mark"></p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/U2Y13ykzBOS0.png?imageslim" alt="mark"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积网络 </tag>
            
            <tag> 注意力机制 </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 感受野 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python——argparse模块</title>
      <link href="/2019/07/29/python-argparse/"/>
      <url>/2019/07/29/python-argparse/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：<a href="http://vra.github.io/2017/12/02/argparse-usage/" target="_blank" rel="noopener">http://vra.github.io/2017/12/02/argparse-usage/</a></p></blockquote><p>argparse 是python自带的命令行参数解析包，可以用来方便地读取命令行参数，当你的代码需要频繁地修改参数的时候，使用这个工具可以将参数和代码分离开来，让你的代码更简洁，适用范围更广。</p><a id="more"></a><h1 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h1><p>下面代码可以使用argparser从命令行获取用户名，然后打印’Hello ‘+ 用户名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># file-name:print_name.py</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def get_parser():</span><br><span class="line">    parser = argparse.ArgumentParser(description=&quot;Demo of argparse&quot;)</span><br><span class="line">    parser.add_argument(&apos;--name&apos;, default=&apos;Great&apos;)</span><br><span class="line">    </span><br><span class="line">    return parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = get_parser()</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    name = args.name</span><br><span class="line">    print(&apos;Hello &#123;&#125;&apos;.format(name))</span><br></pre></td></tr></table></figure></p><p>执行代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python print_name.py --name Zheng</span><br><span class="line">Hello Zheng</span><br></pre></td></tr></table></figure></p><p>上面的代码段中，我们显示引入了argparser包，然后通过argparser.ArgumentParser函数生成argparser对象，其中这个函数的description函数表示在命令行显示帮助信息的时候，这个程序的描述信息。之后我们通过对象的add_argument函数来增加参数。这里我们只增加了一个—name的参数，然后后面的default参数表示如果没提供参数，我们默认采用的值。最后我们通过argpaser对象的parser_args函数来获取所有参数args，然后通过args.name的方式得到我们设置的—name参数的值，可以看到这里argparse默认的参数名就是—name形式里面—后面的字符串。整个流程就是这样。</p><h1 id="default-没有设置值情况下的默认参数"><a href="#default-没有设置值情况下的默认参数" class="headerlink" title="default:没有设置值情况下的默认参数"></a><code>default</code>:没有设置值情况下的默认参数</h1><p>如同上例中展示的，default表示命令行没有设置该参数的时候，程序中用什么值来代替。</p><h1 id="required-表示这个参数是否一定需要设置"><a href="#required-表示这个参数是否一定需要设置" class="headerlink" title="required:表示这个参数是否一定需要设置"></a><code>required</code>:表示这个参数是否一定需要设置</h1><p>若设置了<code>required=true</code>,则在实际运行时不设置该参数将报错</p><pre><code>parser.add_argument(&#39;-name&#39;, required=True)</code></pre><p>则运行下面的命令会报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python print_name.py</span><br><span class="line">usage: print_name.py [-h] --name NAME</span><br><span class="line">print_name.py: error: argument --name is required</span><br></pre></td></tr></table></figure></p><h1 id="type：参数类型"><a href="#type：参数类型" class="headerlink" title="type：参数类型"></a><code>type</code>：参数类型</h1><p>默认参数类型为<code>str</code>类型，若需要一个整数或者布尔参数类型，需要设置<code>type=int</code>或<code>type=bool</code>。</p><h1 id="choices：参数值只能从几个选项里面选择"><a href="#choices：参数值只能从几个选项里面选择" class="headerlink" title="choices：参数值只能从几个选项里面选择"></a><code>choices</code>：参数值只能从几个选项里面选择</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># file-name: choices.py</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def get_parser():</span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=&apos;choices demo&apos;)</span><br><span class="line">    parser.add_argument(&apos;-arch&apos;, required=True, choices=[&apos;alexnet&apos;, &apos;vgg&apos;])</span><br><span class="line"></span><br><span class="line">    return parser</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = get_parser()</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    print(&apos;the arch of CNN is &apos;.format(args.arch))</span><br></pre></td></tr></table></figure><p>如果像下面这样执行会报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python choices.py -arch resnet</span><br><span class="line">usage: choices.py [-h] -arch &#123;alexnet,vgg&#125;</span><br><span class="line">choices.py: error: argument -arch: invalid choice: &apos;resnet&apos; (choose from &apos;alexnet&apos;, &apos;vgg&apos;)</span><br></pre></td></tr></table></figure></p><h1 id="help：指定参数的说明信息"><a href="#help：指定参数的说明信息" class="headerlink" title="help：指定参数的说明信息"></a><code>help</code>：指定参数的说明信息</h1><p>在现实帮助信息的时候，help参数的值可以给使用工具的人提供该参数是用来设置什么的说明，对于大型的项目，help参数和很有必要的，不然使用者不太明白每个参数的含义，增大了使用难度。下面是个例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># file-name: help.py</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def get_parser():</span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=&apos;help demo&apos;)</span><br><span class="line">    parser.add_argument(&apos;-arch&apos;, required=True, choices=[&apos;alexnet&apos;, &apos;vgg&apos;],</span><br><span class="line">        help=&apos;the architecture of CNN, at this time we only support alexnet and vgg.&apos;)</span><br><span class="line"></span><br><span class="line">    return parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = get_parser()</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    print(&apos;the arch of CNN is &apos;.format(args.arch))</span><br></pre></td></tr></table></figure></p><p>在命令行加-h或—help参数运行该命令，获取帮助信息的时候，结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ python help.py -h</span><br><span class="line">usage: help.py [-h] -arch &#123;alexnet,vgg&#125;</span><br><span class="line"></span><br><span class="line">choices demo</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --help           show this help message and exit</span><br><span class="line">  -arch &#123;alexnet,vgg&#125;  the architecture of CNN, at this time we only support</span><br><span class="line">                       alexnet and vgg.</span><br></pre></td></tr></table></figure></p><h1 id="dest：设置参数在代码中的变量名"><a href="#dest：设置参数在代码中的变量名" class="headerlink" title="dest：设置参数在代码中的变量名"></a><code>dest</code>：设置参数在代码中的变量名</h1><p>argparse默认的变量名是<code>--</code>或<code>-</code>后面的字符串，但是你也可以通过<code>dest=xxx</code>来设置参数的变量名，然后在代码中用<code>args.xxx</code>来获取参数的值。</p><h1 id="nargs：-设置参数在使用可以提供的个数"><a href="#nargs：-设置参数在使用可以提供的个数" class="headerlink" title="nargs： 设置参数在使用可以提供的个数"></a><code>nargs</code>： 设置参数在使用可以提供的个数</h1><p>使用方式如下：</p><pre><code>parser.add_argument(&#39;-name&#39;, nargs=x)</code></pre><p>x的选值和含义：</p><div class="table-container"><table><thead><tr><th>值</th><th>含义</th></tr></thead><tbody><tr><td>N</td><td>参数的绝对个数</td></tr><tr><td>‘?’</td><td>0或1一个参数</td></tr><tr><td>‘*’</td><td>0或所有参数</td></tr><tr><td>‘+’</td><td>所有，并且至少一个参数</td></tr></tbody></table></div><p>如下例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># file-name: nargs.py</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">def get_parser():</span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=&apos;nargs demo&apos;)</span><br><span class="line">    parser.add_argument(&apos;-name&apos;, required=True, nargs=&apos;+&apos;)</span><br><span class="line"></span><br><span class="line">    return parser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    parser = get_parser()</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    names = &apos;, &apos;.join(args.name)</span><br><span class="line">    print(&apos;Hello to &#123;&#125;&apos;.format(names))</span><br></pre></td></tr></table></figure></p><p>执行命令和结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python nargs.py -name A B C</span><br><span class="line">Hello to A, B, C</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python——with语句和contextlib模块</title>
      <link href="/2019/07/29/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86/"/>
      <url>/2019/07/29/python%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="with语句"><a href="#with语句" class="headerlink" title="with语句"></a>with语句</h1><p>创建上下文管理实际就是创建一个类，添加<strong>enter</strong>和<strong>exit</strong>方法。下面我们来实现open的上下文管理功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class OpenContext(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, filename, mode):</span><br><span class="line">        self.fp = open(filename, mode)</span><br><span class="line"></span><br><span class="line">    def __enter__(self):</span><br><span class="line">        return self.fp</span><br><span class="line"></span><br><span class="line">    def __exit__(self, exc_type, exc_val, exc_tb):</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">with OpenContext(&apos;/tmp/a.txt&apos;, &apos;a&apos;) as file_obj:</span><br><span class="line">    file_obj.write(&quot;hello 6666&quot;)</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="contextlib模块"><a href="#contextlib模块" class="headerlink" title="contextlib模块"></a>contextlib模块</h1><p>上面我们自定义上下文管理器确实很方便，但是Python标准库还提供了更加易用的上下文管理器工具模块contextlib，它是通过生成器实现的，我们不需要再创建类以及<strong>enter</strong>和<strong>exit</strong>这两个特俗的方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from contextlib import contextmanager</span><br><span class="line"></span><br><span class="line">@contextmanager</span><br><span class="line">def make_open_context(filename, mode):</span><br><span class="line">    fp = open(filename, mode)</span><br><span class="line">    try:</span><br><span class="line">        yield fp</span><br><span class="line">    finally:</span><br><span class="line">        fp.close()</span><br><span class="line"></span><br><span class="line">with make_open_context(&apos;/tmp/a.txt&apos;, &apos;a&apos;) as file_obj:</span><br><span class="line">    file_obj.write(&quot;hello 666&quot;)</span><br></pre></td></tr></table></figure><p>在上文中，yield关键词把上下文分割成两部分：yield之前就是<strong>init</strong>中的代码块；yield之后其实就是<strong>exit</strong>中的代码块，yield生成的值会绑定到with语句as子句中的变量，例如在上面的例子中，yield生成的值是文件句柄对象fp，在下面的with语句中，会将fp和file_obj绑定到一起，也就是说file_obj此时就是一个文件句柄对象，那么它就可以操作文件了，因此就可以调用file_obj.write(“hello 666”)，另外要注意的是如果yield没有生成值，那么在with语句中就不需要写as子句了</p><h1 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># _*_ coding:utf-8 _*_</span><br><span class="line"></span><br><span class="line">from contextlib import contextmanager</span><br><span class="line"></span><br><span class="line">@contextmanager</span><br><span class="line">def book_mark():</span><br><span class="line">    print(&apos;《&apos;, end=&quot;&quot;)</span><br><span class="line">    yield</span><br><span class="line">    print(&apos;》&apos;, end=&quot;&quot;)</span><br><span class="line"></span><br><span class="line">with book_mark():</span><br><span class="line">    # 核心代码</span><br><span class="line">    print(&apos;且将生活一饮而尽&apos;, end=&quot;&quot;)</span><br></pre></td></tr></table></figure><p>这样在打印书名时，通过上下文管理器，会自动加上书名号</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo使用数学公式</title>
      <link href="/2019/07/29/hexo%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2019/07/29/hexo%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：<a href="https://www.jianshu.com/p/7ab21c7f0674" target="_blank" rel="noopener">https://www.jianshu.com/p/7ab21c7f0674</a></p></blockquote><h1 id="在Hexo中渲染MathJax数学公式"><a href="#在Hexo中渲染MathJax数学公式" class="headerlink" title="在Hexo中渲染MathJax数学公式"></a>在Hexo中渲染MathJax数学公式</h1><p>在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。</p><a id="more"></a><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线’_’代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。<br>因为类Latex格式书写的数学公式下划线 ‘_’ 表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如，$x_i$在开始被渲染的时候，处理为$x<code>&lt;em&gt;</code>i<code>&lt;/em&gt;</code>$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。<br>类似的语义冲突的符号还包括’*’, ‘{‘, ‘}’, ‘\’等。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>更换Hexo的markdown渲染引擎，<code>hexo-renderer-kramed</code>引擎是在默认的渲染引擎<code>hexo-renderer-marked</code>的基础上修改了一些bug，两者比较接近，也比较轻量级。</p><pre><code>npm uninstall hexo-renderer-marked --save #卸载原引擎npm install hexo-renderer-kramed --save #安装新引擎</code></pre><p>安装后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改：</p><pre><code>escape: /^\\([`*\[\]()#$+\-.!_&gt;])/</code></pre><p>这一步是在原基础上取消了对\,{,}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。</p><pre><code>em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</code></pre><h2 id="开启mathjax"><a href="#开启mathjax" class="headerlink" title="开启mathjax"></a>开启mathjax</h2><p>进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Math Formulas Render Support</span></span><br><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Default (true) will load mathjax / katex script on demand.</span></span><br><span class="line">  <span class="comment"># That is it only render those page which has `mathjax: true` in Front-matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.</span></span><br><span class="line"><span class="attr">  mathjax:</span></span><br><span class="line"><span class="attr">    enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># See: https://mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line"><span class="attr">    mhchem:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>还需要在文章的Front-matter里打开mathjax开关:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">--</span><br></pre></td></tr></table></figure></p><p>这样就只有在用到公式的页面才加载 Mathjax，不需要渲染数学公式的页面的访问速度就不会受到影响了。</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo上传图片</title>
      <link href="/2019/07/29/hexo%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/"/>
      <url>/2019/07/29/hexo%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo插入本地图片"><a href="#Hexo插入本地图片" class="headerlink" title="Hexo插入本地图片"></a>Hexo插入本地图片</h1><p>1.打开博客配置文件_config.yml，将post_asset_folder这个选项设置为true</p><pre><code>post_asset_floder: true</code></pre><p>2.cd到博客文件根目录，输入命令行</p><pre><code>npm install hexo-asset-image --save</code></pre><p>下载安装一个可以上传本地图片的插件</p><a id="more"></a><p>3.在运行<code>hexo new xxx</code>来生成md格式博文后，在/source/_posts文件夹下除了会生成xxx.md文件，还会生成一个同名文件夹</p><p>4.在xxx.md中想引入本地图片时，先把图片复制到同名文件夹下，然后在需要引用图片的地方按照markdown的格式输入</p><pre><code>![图片标题](xxx/图片名.jpg)</code></pre><h1 id="上传图片没有显示"><a href="#上传图片没有显示" class="headerlink" title="上传图片没有显示"></a>上传图片没有显示</h1><p>完成以上操作后你可能发现打开网页图片没有显示，可能的原因就是hexo-asset-image的版本不对，我们默认下载的是1.0.0版本。在网上查阅资料后得知0.0.1版本可行。于是打开博客根目录下的package.ison文件，将kexo-asset-image的版本号改成0.0.1，然后在该根目录下执行</p><pre><code>npm install</code></pre><p>然后执行hexo clean,hexo g,hexo s等一系列命令，即可看到网页上上传了本地图片</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记2：Objects as Points</title>
      <link href="/2019/07/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/"/>
      <url>/2019/07/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B02/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1904.07850.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.07850.pdf</a><br>论文代码：<a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>目标检测识别往往在图像上将目标以轴对称的框形式框出。大多成功的目标检测器都先穷举出潜在目标位置，然后对该位置进行分类，这种做法浪费时间，低效，还需要额外的后处理。本文中，我们采用不同的方法，构建模型时将目标作为一个点——即目标BBox的中心点。我们的检测器采用关键点估计来找到中心点，并回归到其他目标属性，例如尺寸，3D位置，方向，甚至姿态。我们基于中心点的方法，称为：CenterNet，相比较于基于BBox的检测器，我们的模型是端到端可微的，更简单，更快，更精确。我们的模型实现了速度和精确的最好权衡，以下是其性能：<br>MS COCO dataset, with 28:1% AP at 142 FPS, 37:4% AP at 52 FPS, and 45:1% AP with multi-scale testing at 1.4 FPS.</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>当前的目标检测用一个紧紧包含目标的轴对称的边界框来表示每一个目标。对于每一个目标，使用分类器来确定每个框中是否为特定的目标还是背景。</p><ul><li><p><strong>One-stage detectors</strong> 在图像上滑动复杂排列的可能bbox（即锚点）,然后直接对框进行分类，而不会指定框中内容。</p></li><li><p><strong>Two-stage detectors</strong> 对每个潜在框重新计算图像特征，然后将那些特征进行分类。</p></li><li><p><strong>Post-processing</strong> 也即是non-maxima suppression(非极大值抑制)，通过计算IoU来移除对同一个instance的重复检测。这种后处理很难区分和训练，因此现有大多检测器都不是端到端可训练的。</p></li></ul><p>本文提出了一种简便和高效的方法。使用目标边界框中心的一个点来代表目标。然后在中心点位置回归出目标的一些属性，例如：size, dimension, 3D extent, orientation, pose。 而目标检测问题变成了一个标准的关键点估计问题。我们仅仅将图像传入全卷积网络，得到一个heatmap，heatmap峰值点即中心点，每个特征图的峰值点位置预测了目标的宽高信息。不需要NMS做后处理。</p><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><ul><li><strong>Object  detection  by  region  classification</strong> RCNN Fast-RCNN</li><li><strong>Object detection with implicit anchors</strong> Faster-RCNN通过检测网络生成region proposal。在低分辨率图像网格周围采样固定形状的边界框(anchors)，对每个anchor进行是否为前景的分类。然后对每一个前景样本进行多类别分类，得到最终的结果。<br>我们的方法类似于anchor-based单阶段的方法，一个中心点可以看做一个形状未知的anchor。然而，存在一些重要的不同点：<ul><li>我们分配的锚点仅仅是放在位置上，没有尺寸框。没有手动设置的阈值做前后景分类。（像Faster RCNN会将与GT IOU &gt;0.7的作为前景，&lt;0.3的作为背景，其他不管）</li><li>每个目标仅仅有一个正的锚点，因此不会用到NMS，我们提取关键点特征图上局部峰值点（local peaks）</li><li>CenterNet 相比较传统目标检测而言（缩放16倍尺度），使用更大分辨率的输出特征图（缩放了4倍），因此无需用到多重特征图锚点</li></ul></li><li><strong>Object detection by keypoint estimation</strong> CornerNet检测边界框的两个角点作为关键点，ExtremeNet检测所有物体的上下左右以及中心店来检测目标。然而它们都需要在关键点检测后加上一个grouping阶段，这会显著拖慢它们的算法。而CenterNet只是对每个目标简单的提取一个中心店，因此不需要grouping以及post-processing。</li></ul><h1 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h1><p>用$I\in R^{W×H×3}$表示长为H宽为W的输入图像。我们的目标是产生一个关键点heat map $Y\in[0,1]^{\frac{W}{R}×\frac{H}{R}×C}$,R为输出步长，C为关键点的类别数量。$\hat{Y}_{xyz}=1$表示检测到的关键点，$\hat{Y}_{xyz}=0$表示背景。我们采用了几个不同的全卷积编码-解码网络来预测图像 I 得到的$\hat{Y}$：</p><ul><li>stacked hourglass network</li><li>upconvolutional residual networks (ResNet)</li><li>deep layer aggregation (DLA)</li></ul><p>对于类别为c的真实框的关键点$p\in R^2$，我们计算它在低分辨率上对应的关键点为$\overline{p}=\lfloor\frac{p}{R}\rfloor$。我们将GT关键点通过高斯核$Y_{xyz}=exp{(-\frac{(x-\overline{p}_{x})^2+(y-\overline{p}_{y})^2}{2\sigma_{p}^2})}$分散到heat map  $Y\in[0,1]^{\frac{W}{R}×\frac{H}{R}×C}$上，其中$\sigma_{p}$是目标尺度自适应的标准方差。如果对于同个类 c （同个关键点或是目标类别）有两个高斯函数发生重叠，我们选择元素级最大的。训练目标函数如下，像素级逻辑回归的focal loss：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/GLEG226YnzAL.png?imageslim" alt="mark"></p><p>由于图像下采样时，GT的关键点会因数据是离散的而产生偏差，我们对每个中心点附加预测了个局部偏移$\hat{O}\in R^{\frac{W}{R}×\frac{H}{R}×2}$,所有类别 c 共享同个偏移预测，这个偏移同个 L1 loss来训练：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/UL4RQSnlxtm8.png?imageslim" alt="mark"><br>只会在关键点$\overline{p}$位置 做监督操作，其他位置无视。<br>关于这个损失函数补充一点解释。这个损失函数与focal loss类似，对于easy example的中心店，适当减少其训练比重也就是loss值。</p><ul><li>当$Y_{xyc}=1$时，$(1-\hat{Y}_{xyc})^\alpha$就充当了矫正的作用。<ul><li>当$\hat{Y}_{xyc}$接近于1时，说明是容易检测出来的点，这个值就会相应比较低。</li><li>当$\hat{Y}_{xyc}$接近于0时，说明这个中心点还没有学习到，那么就要增大其训练比重，因此这个值就会比较大。</li></ul></li><li>当otherwise时，$(1-Y_{xyc})^\beta$和$(\hat{Y}_{xyc})^\alpha$协同进行矫正。<ul><li>当$\hat{Y}_{xyc}$越接近1时，说明样本越容易被误检，需要增加对其的惩罚，通过$(\hat{Y}_{xyc})^\alpha$来实现。</li><li>由于正负样本的数量很不平衡，我们需要抑制负样本的训练比重。由于检测到的负样本在正样本即中心点周围的可能性比较高，所以它们的$(1-Y_{xyc})$接近于1。因此可以通过$(1-Y_{xyc})^\beta$来实现。</li></ul></li></ul><h1 id="Objects-as-Points"><a href="#Objects-as-Points" class="headerlink" title="Objects as Points"></a>Objects as Points</h1><p>用$(x_{1}^{(k)},y_{1}^{(k)},x_{2}^{(k)},y_{2}^{(k)})$来表示类别为$c_{k}$的object$k$的边界框。那么它的中心点即为$(\frac{x_{1}^{(k)}+x_{2}^{(k)}}{2},\frac{y_{1}^{(k)}+y_{2}^{(k)}}{2})$。我们用关键点估计$\hat{Y}$来得到所有的中心点。另外，我们为每个目标 k 回归出目标的尺寸$s_{k}=(x_{2}^{(k)}-x_{1}^{(k)},y_{2}^{(k)}-y_{1}^{(k)})$。为了减少计算负担，我们为每个目标种类使用单一的尺寸预测$\hat{S}\in R^{\frac{W}{R}×\frac{H}{R}×2}$,我们在中心点位置添加了 L1 loss:</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/dFbxUqWF31Xr.png?imageslim" alt="mark"></p><p>我们不将scale进行归一化，直接使用原始像素坐标。为了调节该loss的影响，将其乘了个系数，整个训练的目标loss函数为：</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/wOOp7nG5FUAw.png?imageslim" alt="mark"></p><p>整个网络预测会在每个位置输出 C+4个值(即关键点类别C, 偏移量的x,y，尺寸的w,h)，所有输出共享一个全卷积的backbone。</p><p><strong>From points to bounding boxes</strong></p><p>在推理的时候，我们分别提取热力图上每个类别的峰值点。如何得到这些峰值点呢？做法是将热力图上的所有响应点与其连接的8个临近点进行比较，如果该点响应值大于或等于其八个临近点值则保留，最后我们保留所有满足之前要求的前100个峰值点。</p><ul><li>关键点检测结果 $\hat{P}=\lbrace(\hat{x}_{i},\hat{y}_{i})\rbrace_{i=1}^{n}$</li><li>偏移检测结果 $\hat{O}_{\hat{x}_{i},\hat{y}_{i}}=(\delta\hat{x}_{i},\delta\hat{y}_{i})$</li><li>尺度检测结果 $\hat{S}_{\hat{x}_{i},\hat{y}_{i}}=(\hat{w}_{i},\hat{h}_{i})$</li></ul><p>根据以上结果得到在$(\hat{x}_{i},\hat{y}_{i})$的边界框：</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/jiA2m9tPNenL.png?imageslim" alt="mark"></p><p>所有的输出都直接从关键点估计得到，无需基于IOU的NMS或者其他后处理。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> anchor-free </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 关键点检测 </tag>
            
            <tag> Focal Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>next主题个性化设置</title>
      <link href="/2019/07/26/next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E8%AE%BE%E7%BD%AE/"/>
      <url>/2019/07/26/next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="修改网站描述"><a href="#修改网站描述" class="headerlink" title="修改网站描述"></a>修改网站描述</h1><pre><code># Sitetitle: 浅笑の博客subtitle: 我们的征途是星辰大海author: Zheng Yujiedescription: Pytho/C++/深度学习language: zh-CNtimezone: Asia/Shanghai</code></pre><ul><li>“title”：博客的名称。</li><li>“subtitle”：根据主题的不同，有的会显示有的不会显示。</li><li>“description”：主要用于SEO，告诉搜索引擎一个关+ 于站点的简单描述，通常建议在其中包含网站的关键词。</li><li>“author”：作者名称，用于主题显示文章的作者。</li><li>“language”：语言会对应的解析正在应用的主题中的+ languages文件夹下的不同语言文件。所以这里的名称要和languages文件夹下的语言文件名称一致。</li><li>“timezone”：可不填写。</li></ul><a id="more"></a><h1 id="设置网站图标"><a href="#设置网站图标" class="headerlink" title="设置网站图标"></a>设置网站图标</h1><p>主题配置文件中第一行代码就是网站icon设置，这里你只需要找到你喜欢的logo把它制作成ico格式然后改名favicon.ico，放到/themes/next/source/images下面即可。</p><pre><code># Put your favicon.ico into `hexo-site/source/` directory.favicon: /favicon.ico # 网站logo</code></pre><h1 id="更换头像"><a href="#更换头像" class="headerlink" title="更换头像"></a>更换头像</h1><p>修改主题配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Sidebar Avatar</span><br><span class="line">avatar:</span><br><span class="line">  # In theme directory (source/images): /images/avatar.gif</span><br><span class="line">  # In site directory (source/uploads): /uploads/avatar.gif</span><br><span class="line">  # You can also use other linking images.</span><br><span class="line">  url: /images/1.jpeg</span><br><span class="line">  # If true, the avatar would be dispalyed in circle.</span><br><span class="line">  rounded: false</span><br><span class="line">  # If true, the avatar would be rotated with the cursor.</span><br><span class="line">  rotated: false</span><br></pre></td></tr></table></figure></p><h1 id="添加侧边栏社交链接"><a href="#添加侧边栏社交链接" class="headerlink" title="添加侧边栏社交链接"></a>添加侧边栏社交链接</h1><p>主要修改主题配置文件的社交链接和对应图标</p><pre><code># Social Links# Usage: `Key: permalink || icon`# Key is the link label showing to end users.# Value before `||` delimiter is the target permalink.# Value after `||` delimiter is the name of Font Awesome icon. If icon (with or without delimiter) is not specified, globe icon will be loaded.social:GitHub: https://github.com/zhengyujie || github#E-Mail: mailto:yourname@gmail.com || envelope#Google: https://plus.google.com/yourname || google#Twitter: https://twitter.com/yourname || twitter#FB Page: https://www.facebook.com/yourname || facebook#VK Group: https://vk.com/yourname || vk#StackOverflow: https://stackoverflow.com/yourname || stack-overflow#YouTube: https://youtube.com/yourname || youtube#Instagram: https://instagram.com/yourname || instagram#Skype: skype:yourname?call|chat || skypeMusic: https://music.163.com/#/playlist?id=327924141 || headphonesGoogle: https://www.google.com || googleBaidu: https://baidu.com || firefox</code></pre><h1 id="添加友情链接"><a href="#添加友情链接" class="headerlink" title="添加友情链接"></a>添加友情链接</h1><p>修改主题配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Blog rolls</span><br><span class="line">links_icon: heart</span><br><span class="line">links_title: FRIENDS</span><br><span class="line">links_layout: block</span><br><span class="line">#links_layout: inline</span><br><span class="line">links:</span><br><span class="line">  #Title: http://example.com</span><br></pre></td></tr></table></figure></p><h1 id="全文阅读"><a href="#全文阅读" class="headerlink" title="全文阅读"></a>全文阅读</h1><p>在写文章时添加<code>&lt;!--more--&gt;</code></p><ul><li><code>&lt;!--more--&gt;</code>上方的正常显示</li><li><code>&lt;!--more--&gt;</code>下方的隐藏</li></ul><h1 id="添加follow-me-on-GitHub"><a href="#添加follow-me-on-GitHub" class="headerlink" title="添加follow me on GitHub"></a>添加follow me on GitHub</h1><p>修改主题配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># `Follow me on GitHub` banner in the top-right corner.</span><br><span class="line">github_banner:</span><br><span class="line">  enable: true</span><br><span class="line">  permalink: https://github.com/zhengyujie</span><br><span class="line">  title: Follow me on GitHub</span><br></pre></td></tr></table></figure></p><h1 id="添加看板娘"><a href="#添加看板娘" class="headerlink" title="添加看板娘"></a>添加看板娘</h1><p>安装插件</p><pre><code>npm install --save hexo-helper-live2d</code></pre><p>在博客配置文件中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  model:</span><br><span class="line">    scale: 1</span><br><span class="line">    hHeadPos: 0.5</span><br><span class="line">    vHeadPos: 0.618</span><br><span class="line">  display:</span><br><span class="line">    superSample: 2</span><br><span class="line">    width: 200</span><br><span class="line">    height: 400</span><br><span class="line">    position: right</span><br><span class="line">    hOffset: 0</span><br><span class="line">    vOffset: -20</span><br><span class="line">  mobile:</span><br><span class="line">    show: false</span><br><span class="line">  react:</span><br><span class="line">    opacity: 0.7</span><br></pre></td></tr></table></figure></p><h1 id="字数统计和阅读时长"><a href="#字数统计和阅读时长" class="headerlink" title="字数统计和阅读时长"></a>字数统计和阅读时长</h1><p>安装插件</p><pre><code>npm install hexo-symbols-count-time --save</code></pre><p>修改站点配置文件</p><pre><code>symbols_count_time:    #文章内是否显示    symbols: true    time: true    # 网页底部是否显示    total_symbols: true    total_time: true</code></pre><p>修改主题配置文件</p><pre><code># Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time:    separated_meta: true    item_text_post: true    item_text_total: false    awl: 4    wpm: 275    suffix: mins.</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>next主题安装和配置</title>
      <link href="/2019/07/26/next%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
      <url>/2019/07/26/next%E4%B8%BB%E9%A2%98%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a>安装主题</h1><p>在博客根目录下输入以下命令</p><pre><code> git clone https://github.com/theme-next/hexo-theme-next themes/next</code></pre><p>就可以将next主题文件从<code>https://github.com/iissnan/hexo-theme-next</code>下载到当前目录下的themes里面的next文件夹中。</p><h1 id="启用主题"><a href="#启用主题" class="headerlink" title="启用主题"></a>启用主题</h1><p>修改站点配置文件</p><pre><code># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next</code></pre><a id="more"></a><h1 id="主题设定"><a href="#主题设定" class="headerlink" title="主题设定"></a>主题设定</h1><p>目前next主题支持四种Scheme：</p><ul><li>Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白</li><li>Mist - Muse 的紧凑版本，整洁有序的单栏外观</li><li>Pisces - 双栏 Scheme，小家碧玉似的清新</li><li>Gemini - 左侧网站信息及目录，块+片段结构布局</li></ul><p>Scheme的切换通过更改主题配置文件,搜索scheme关键字。你会看到有四行scheme的配置，将你需用启用的scheme前面注释#去除即可。</p><h1 id="设定语言"><a href="#设定语言" class="headerlink" title="设定语言"></a>设定语言</h1><p>明确设定你所需要的语言</p><pre><code>language： zh-CN</code></pre><h1 id="设置标签和分类等界面"><a href="#设置标签和分类等界面" class="headerlink" title="设置标签和分类等界面"></a>设置标签和分类等界面</h1><p>修改主题配置文件</p><pre><code># ---------------------------------------------------------------# Menu Settings# ---------------------------------------------------------------# When running the site in a subdirectory (e.g. domain.tld/blog), remove the leading slash (/archives -&gt; archives)menu:home: /archives: /archives/tags: /tags/categories: /categories/about: /about/#sitemap: /sitemap.xml#commonweal: /404/</code></pre><p>去掉相应的<code>#</code>即可<br>然后新建页面</p><pre><code>hexo new page &#39;tags&#39; #创建tags子目录hexo new page &#39;categories&#39; #创建categories子目录</code></pre><p>分别修改对应的<code>index.me</code>文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2019-07-26 13:47:40</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p><p>以及<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章分类</span><br><span class="line">date: 2019-07-26 13:47:40</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> NexT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo的简介和安装</title>
      <link href="/2019/07/26/hexo%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/"/>
      <url>/2019/07/26/hexo%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo简介"><a href="#hexo简介" class="headerlink" title="hexo简介"></a>hexo简介</h1><p>主页： <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">https://hexo.io/zh-cn/</a><br>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><ul><li>hexo 可以理解为是基于node.js制作的一个博客工具，不是我们理解的一个开源的博客系统。其中的差别，有点意思。</li><li>hexo 正常来说，不需要部署到我们的服务器上，我们的服务器上保存的，其实是基于在hexo通过markdown编写的文章，然后hexo帮我们生成静态的html页面，然后，将生成的html上传到我们的服务器。简而言之：hexo是个静态页面生成、上传的工具</li></ul><a id="more"></a><h1 id="hexo搭建"><a href="#hexo搭建" class="headerlink" title="hexo搭建"></a>hexo搭建</h1><h2 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h2><pre><code>sudo apt-get install git-core</code></pre><p>安装好后，使用<code>git --Version</code>来查看一下版本。</p><h2 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h2><p>先安装nvm，即是Node Version Manager(Node版本管理器)</p><pre><code>wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash</code></pre><p>激活nvm</p><pre><code>source ~/.nvm/nvm.sh</code></pre><p>安装node</p><pre><code>nvm install node</code></pre><p>安装好后</p><pre><code>node -vnpm -v</code></pre><p>来检查有没有安装成功</p><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><pre><code>npm install -g hexo-cil</code></pre><p>仍然使用<code>hexo -v</code>来检查有没有安装成功<br>创建一个文件夹，然后<code>cd</code>到这个文件夹下</p><pre><code>hexo init</code></pre><p>初始化后就能在文件夹下看到一系列文件，这里做点解释</p><ul><li>node_modules：是依赖包</li><li>public：存放的是生成的页面</li><li>scaffolds：命令生成文章等的模板</li><li>source：用命令创建的各种文章</li><li>themes：主题</li><li>_config.yml：整个博客的配置</li><li>db.json：source解析所得到的</li><li>package.json：项目所需模块项目的配置信息</li></ul><p>接着输入命令行</p><pre><code>hexo ghexo server</code></pre><p>随后在浏览器输入<code>localhost:4000</code>就可以看到生成的微博了。</p><h2 id="GitHub创建个人仓库"><a href="#GitHub创建个人仓库" class="headerlink" title="GitHub创建个人仓库"></a>GitHub创建个人仓库</h2><p>首先注册一个GitHub账号<br>然后创建一个和你用户名相同的仓库，在后面加上<code>.github.io</code>只有这样，将来要部署到GitHub page的时候，才会被识别，也就是<code>xxxx.github.io</code>，其中xxx就是你注册GitHub的用户名。</p><h2 id="将hexo部署到GitHub"><a href="#将hexo部署到GitHub" class="headerlink" title="将hexo部署到GitHub"></a>将hexo部署到GitHub</h2><p>修改站点配置文件<code>_config.yml</code>的最后部分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https://github.com/YourgithubName/YourgithubName.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>要先安装deploy-git,才能用命令部署到GitHub</p><pre><code>npm install hexo-deployer-git --save</code></pre><p>然后</p><pre><code>hexo clean #清除之前生成的东西hexo generate  #生成静态文章，缩写hexo ghexo deploy  #部署文章，缩写hexo d</code></pre><p>过一会儿就可以在<code>http://yourname.github.io</code>这个网站看到你的博客了</p>]]></content>
      
      
      <categories>
          
          <category> Hexo框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读笔记1：RepPoints:Point Set Representation for Object Detection</title>
      <link href="/2019/07/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01/"/>
      <url>/2019/07/25/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B01/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/abs/1904.11490" target="_blank" rel="noopener">https://arxiv.org/abs/1904.11490</a><br>论文代码：未公布</p></blockquote><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>当前主流的目标检测网络，对于bounding box的依赖很严重，无论是one-stage还是two-stage的检测器，都需要bounding box对目标区域进行提取，划分，然后进行分类和回归。但是这些bounding box都是规则且相对固定的候选框，只能对目标提供一个较为粗糙的定位，这就导致bounding box提取出来的特征也是粗糙的。本文提出了RepPoints（representive points），通过一组有代表性的点，实现对目标的更精细表示，对于后续的分类和定位都有帮助。在训练过程中，给定Ground Truth的位置和识别目标，RepPoints学会自动排列代表性点，限定目标的空间范围，并表示语义上重要的局部区域。整个过程都不需要anchor进行bounding box的采样，是名副其实的anchor-free。实验表明，基于RepPoints的网络结构，在没采用多尺度训练的情况下，在COCO数据集上，AP指标上达到了42.8%。</p><a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>目标检测一直是计算机视觉中的基础且热门领域，为诸如分割，跟踪等任务提供辅助。随着深度学习的迅速发展，目标检测网络取得了迅速的发展进步，代表性网络如Faster R-CNN, YOLO，Ssd等都有很好的表现。纵观所有的目标检测网络，都绕不开bounding box的存在，bounding box包围图像的目标区域，作为整个目标检测中的基本处理单元。基于提取的bounding box，进一步进行分类和位置回归处理。bounding box的应用很大程度源于网络结构的评价指标，即预估框和GT框的重叠覆盖程度。还有一个原因是因为bounding box这种规则的形状对特征提取和后续的池化处理提供了便利。虽然如此，但是bounding box只是对目标的粗糙提取，这种提取可能并不符合目标的姿态和形状，因此提取出来的内容势必会包括一些背景信息等无关信息，这可能会产生较低质量的特征，从而影响目标检测的分类性能。</p><p>本文提出了一种全新的方法RepPoints，通过一组点集提供更细粒度的位置表示和便于分类的信息。如下图所示：RepPoints是一系列点组成的集合，这些点分布在目标的空间范围和具有重要语义信息的位置。RepPoints的训练由目标定位和识别共同驱动，RepPoints与GT bounding box紧密结合，引导检测器正确分类目标。这种方法摆脱了bounding box的限制，更没有了anchor的烦恼。</p><p>为了证明ReoPoints的强大，本文利用一个可变形卷积实现了目标检测网络，该网络在保持特征提取方便的同时，提供了适合于指导自适应采样的识别反馈。接下来会有详细介绍。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/YN1n83VRQuLV.png?imageslim" alt="mark"></p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><ul><li><p><strong>Bounding boxes for the object detection problem</strong><br>Bounding box流行的主要原因有：</p><ul><li>Bounding box可以很便捷的进行注释而且几乎没有歧义，同时在最后的认知阶段可以提供高效的进准定位。</li><li>不管是过去，还是在深度学习领域，几乎所有的图片特征提取都是以网格的形式作为输入。因此，应用bounding box可以很方便的促进特征提取</li></ul></li><li><p><strong>Bounding boxes in modern object detectors</strong><br>bounding box表示出现在多阶段识别模式的几乎所有阶段：1）作为anchors。2）作为object proposals。3）作为最终的localization targets。</p></li></ul><ul><li><p><strong>Other representations for object detection</strong><br>最近的目标检测Bottom-up approaches：</p><ul><li>CornerNet 预测左上以及右下的角点，应用专门的grouping方法来获得object的bounding box</li><li>ExtremeNet 定位object在x和y方向上的极点来预测bounding box</li></ul></li><li><p><strong>Deformation  modeling  in  object  recognition</strong></p></li></ul><h1 id="The-RepPoints-Representation"><a href="#The-RepPoints-Representation" class="headerlink" title="The RepPoints Representation"></a>The RepPoints Representation</h1><h2 id="Bounding-Box-Representation"><a href="#Bounding-Box-Representation" class="headerlink" title="Bounding Box Representation"></a>Bounding Box Representation</h2><ul><li><strong>Review of Multi-Stage Object Detectors</strong></li></ul><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/7urfHbbDBTBK.png?imageslim" alt="mark"></p><ul><li><strong>Bounding Box Regression</strong><br>边界框回归的过程不再赘述</li></ul><h2 id="RepPoints"><a href="#RepPoints" class="headerlink" title="RepPoints"></a>RepPoints</h2><p>正如前面讨论的，4-d的bounding box是目标位置的粗糙表达，边界框只考虑目标的矩形空间范围，不考虑形状、姿态和语义上重要的局部区域的位置，但恰恰是这些区域，可用于更好的定位和更好的对象特征提取。为了克服这些局限，RepPoints建立一组自适应的特征点集$R\in\lbrace(x_{k},y_{k})\rbrace_{k=1}^{n}$其中n是构成点集的点数量，在本文中，n被设置为9，也就是抽样9个点</p><ul><li><p><strong>RepPoints refinement</strong><br>对R中的每一个点预测一组偏移$\lbrace(\Delta x_{k},\Delta y_{k})\rbrace_{k=1}^{n}$。因此refinement的过程就可以简单的表示为：$R_{r}=\lbrace(x_{k}+\Delta x_{k},y_{k}+\Delta y_{k})\rbrace$</p></li><li><p><strong>Converting RepPoints to bounding box</strong><br>得到一组点后，接下来就把这些点转化为bounding box ,我们定义一个转换函数T:$R_{p}\rightarrow B_{p}$,<br>$R_{p}$为object $P$的RepPoints,$B_{p}$表示一个pseudo box，转换函数有三种形式：</p><ul><li>T=T1：Min-max function. 在x,y两个坐标轴方向上，寻找点集中的极值点来表示bouning box，一般来说，有两个点就能构成一个bounding box</li><li>T = T2: Partial min-max function. 先从点集中选取相应的子集，将选取出的点利用（1）式方法，构成bounding box</li><li>T = T3: Moment-based function.计算点集中所有点的均值作为bounding box的中心点坐标，二阶矩作为bounding box的宽和高。“矩”是一组点组成的模型的特定的数量测度，关于二阶矩，可以简单理解为随机变量的方差。</li></ul></li><li><p><strong>Learning RepPoints</strong><br>RepPoints的学习是目标定位损失以及目标认知损失共同驱动的。关于目标定位的损失，需要我们利用上文中提到的转换函数T生成一个pseudo box，然后利用bounding box和GT bounding box进行运算。在本文中，作者在左上角和右下角之间的距离运用smooth L1，进行位置回归。</p></li></ul><h1 id="RPDet-an-Anchor-Free-Detector"><a href="#RPDet-an-Anchor-Free-Detector" class="headerlink" title="RPDet: an Anchor Free Detector"></a>RPDet: an Anchor Free Detector</h1><p>为了验证利用RepPoints代替bounding box候选的可行和强大，设计了一个anchor-free的网络结构，整个网络是由两个基于可变形卷积的识别阶段组成。作者将RepPoints和可变形卷积很好的结合起来。<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/snvnCe1X2Yrl.png?imageslim" alt="mark"></p><p>目标表示的演化过程：<br><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/ilphoMobOhuc.png?imageslim" alt="mark"></p><ul><li><p><strong>Center point based initial object representation</strong><br>我们和YOLO和DenseBox一样使用中心点作为ojects的初始表示，使之成为一个anchor-free的目标检测器。中心点表示的一个重要好处就是它的假设空间相比于anchor-based更tight。</p></li><li><p><strong>Utilization  of  RepPoints</strong><br>the first set of RepPoints通过一个常规的3*3卷积得到，这些RepPoints的学习由两个objectives驱动：</p><ul><li>pseudo box和ground-truth bbox之间的左上以及右下角点的距离loss</li><li>随后阶段的认知loss<br>the second set of RepPoints表示最终的object定位，只由点之间的距离loss驱动。</li></ul></li><li><p><strong>Other details</strong><br>不再叙述</p></li></ul><h1 id="Deformable-Convolution"><a href="#Deformable-Convolution" class="headerlink" title="Deformable Convolution"></a>Deformable Convolution</h1><p>研究发现，标准卷积中的规则采样格点采样是导致网络难以适应几何形变的“罪魁祸首”，为了削弱这个限制，对卷积核中每个采样点为位置都增加一个偏移量，可以实现在当前位置附近随意采样而不局限于规则的格点，如下图是常见的采样点和可变形卷积采样点的对比。其中（a）是规则的采样点，(b)(c)(d)是在规则采样点的基础上，加上一个偏移量，使得采样点的位置发生变化</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/CjChbYWDvgGU.png?imageslim" alt="mark"></p><p>可变形卷积的目的是提升形变的建模能力。通过可变形卷积（deformable conv）和可变形感兴趣区域池化（deformable ROI Pooling）这两个模块，基于一个平行网络学习一个偏移量（offset），使得卷积核在feature map上的采样点发生偏移，集中于我们感兴趣的区域。我们可以用下列网络结构来看：以一个feature map作为输入，常规的采样点如绿框所示，是规则且局限的，而可变形卷积的做法是，增加一路网络，经过卷积之后，输出一个维度为2N的map，其中N是采样点数量，2N是说明学习x,y两个方向的偏移，这样，对于每一个原始的采样点，我们都学习到了它在x,y两个方向上的偏移，即下图offsets map，指示了原始的采样点在融合x,y两个方向的偏移量后，最终的偏移方向。然后，将原始采样点与offset对应融合，得到最终采样位置，经过卷积运算，得到输出feature map上的结果。</p><p><img src="http://pwbhioup3.bkt.clouddn.com/blog/20190816/yfz1L28KuvKm.png?imageslim" alt="mark"></p><p>非常需要注意的一个点是，可变形卷积的可变形体现在采样点不是局限规则的，而不是卷积核是可变形的</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
            <tag> anchor-free </tag>
            
            <tag> Deformable Convolution </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 关键点检测 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
